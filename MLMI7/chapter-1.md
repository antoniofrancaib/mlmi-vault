# Reinforcement Learning (RL)

Reinforcement learning (RL) is a paradigm of machine learning concerned with how an agent can learn to make decisions through interaction with an environment in order to achieve a goal. In this introductory chapter, we will explore the foundational concepts of reinforcement learning in a comprehensive yet intuitive manner. We contrast RL with other learning frameworks (supervised and unsupervised learning), discuss the idea of learning from interaction, introduce the exploration–exploitation dilemma, survey real-world applications of RL, and delve into the key formal concepts: the agent–environment interface, the Markov decision process (MDP), value functions, and the Bellman equations. Throughout, we emphasize practical understanding with mathematical rigor, setting the stage for deeper exploration in subsequent chapters.

## 1. Introduction to Learning Frameworks

Machine learning comprises several frameworks by which agents or models improve their performance: supervised learning, unsupervised learning, and reinforcement learning are three primary paradigms. Each framework addresses a different type of learning problem:

- **Supervised learning**: In supervised learning, the model learns from labeled data – for each input example, a correct output (label) is provided. The goal is to generalize from these examples so that the model can predict the correct output for new, unseen inputs. Learning is typically formulated as minimizing a prediction error or loss function on the training data. In other words, the model is "supervised" by the given correct answers. For example, a classifier might be trained on images labeled as "cat" or "dog" and must learn to label new images correctly. Formally, supervised learning can be seen as learning a function $f:X\to Y$ mapping inputs $X$ to outputs $Y$ based on example pairs $(x, y)$. The model’s performance is evaluated by how well it predicts the correct outputs on test data. Key to supervised learning is the assumption that a fixed dataset of input-output pairs is available and the learning algorithm does not influence which data it sees – data is given and static.

- **Unsupervised learning**: In unsupervised learning, no explicit labels or targets are provided. Instead, the algorithm must find patterns, structure, or relationships in the data on its own. Common unsupervised tasks include clustering (discovering groups of similar examples) and dimensionality reduction (discovering informative low-dimensional representations of data). For instance, given a collection of news articles with no labels, an unsupervised algorithm might cluster articles by topic. The goal is not to predict an external label, but to model the underlying structure of the data. Unsupervised learning is often exploratory; because there is no single objective right answer (no labels), success is measured by the meaningfulness of the patterns found (which can be subjective). The learning algorithm processes the input data and organizes or compresses it in some way to reveal insights, such as grouping similar data points or detecting anomalies, all without direct feedback about what is “correct.”

- **Reinforcement learning**: Reinforcement learning differs fundamentally from the above two. In RL, an agent learns from interaction with an environment, using a system of rewards and penalties rather than direct labels. There is no supervisor providing correct actions; instead, the agent discovers which actions yield the most reward by trying them. Learning is driven by the consequences of actions: the agent receives feedback in the form of scalar rewards, which it attempts to maximize over time. Unlike supervised learning, where the correct output for each input is given, in RL the agent often only knows if an outcome was good or bad after the fact. And unlike unsupervised learning, RL is goal-directed: there is a notion of cumulative reward that the agent tries to optimize. Another key distinction is that the data the agent uses to learn (its experiences) are generated by the agent’s own actions. This means the learning process is interactive and dynamic – the choices an RL agent makes will influence the subsequent data it receives (future states and rewards). This self-directed learning from trial and error underlies the unique challenges of RL.

In summary, supervised learning is about learning from provided examples of correct behavior (minimizing prediction error), unsupervised learning is about finding hidden structure in unlabeled data, and reinforcement learning is about learning how to act in an environment to maximize cumulative reward. RL is often considered more complex because it involves sequential decision making, feedback that may be delayed, and the necessity for the agent to actively gather information. It occupies a special place in machine learning, drawing connections to psychology (trial-and-error learning), control theory (sequential decision processes), and game theory (sequential strategic interaction).

## 2. Learning from Interaction

Reinforcement learning can be described succinctly as learning from interaction. An RL agent improves its behavior by interacting with its environment and observing the outcomes of these interactions. At each step, the agent perceives the current state of the environment, chooses an action, and as a result of this action the environment transitions to a new state and provides the agent with a reward (a numerical feedback signal). This cycle of state → action → reward → next state continues over a sequence of time steps, as illustrated in the typical reinforcement learning loop (Figure 1).

**Figure 1**: The reinforcement learning agent–environment interaction loop. At each time step $t$, the agent receives the current state ($S_t$) from the environment and chooses an action ($A_t$). The environment responds by giving the agent a reward ($R_{t+1}$) and transitioning to a new state ($S_{t+1}$). The agent’s goal is to learn a policy for selecting actions that maximizes the cumulative reward it receives over time.

Learning from interaction has several important implications:

- **Trial and error**: The agent explores different actions and gradually learns which actions lead to good outcomes. Early in learning, the agent may perform suboptimal or random actions; over time, it uses the feedback (rewards) to improve its decisions. This trial-and-error process is fundamental to RL – the agent was not pre-programmed with correct actions, it discovers them through experience.

- **Delayed consequences**: Actions can have consequences that are not immediately evident. A current action might yield a small immediate reward but set the stage for a much larger reward (or penalty) later. For example, consider a chess move that sacrifices a pawn (immediate negative reward) but leads to a checkmate in a dozen moves (large positive reward). Reinforcement learning algorithms must contend with this delayed reward problem, also known as the temporal credit assignment problem: how to attribute rewards or penalties to earlier actions that partly caused those outcomes.

- **Stateful, sequential data**: The data an RL agent trains on is a sequence of states, actions, and rewards arising from its own behavior, rather than an i.i.d. (independent and identically distributed) dataset. Crucially, the agent’s actions influence the subsequent states it will see. This is very different from supervised learning, where the training data distribution is fixed and outside the model’s control. In RL, by choosing different actions, the agent might visit different states – it actively samples the part of the environment it experiences.

- **Goal of maximizing reward**: The agent’s objective is usually defined as maximizing the cumulative reward it receives over time (for instance, the total reward in an episode, or the discounted sum of future rewards if we use a discount factor as discussed later). This gives a clear goal but not a direct recipe for how to achieve it; the agent must learn a strategy (or policy) that yields high rewards through interaction.

Because the agent’s actions affect its future inputs, the problem of learning from interaction is often more complex than static supervised learning. The agent faces a moving target (its policy changes, the state distribution changes) and must handle the exploration of new actions versus exploitation of known rewarding actions (more on this soon). The environment’s dynamics may also be stochastic – the same action in the same state may not always lead to the exact same next state or reward, introducing uncertainty that the agent must learn to handle by reasoning in terms of expectations and probabilities. It’s also worth noting that in practical scenarios, finding an exact optimal solution to an RL problem (especially when the state space is very large) can be intractable. Therefore, much of reinforcement learning focuses on approximate solutions. Instead of analytically computing the perfect way to act (which might be impossible for complex environments), RL methods often rely on simulation, sampling, and function approximation to estimate the best behavior. In fact, many reinforcement learning algorithms can be understood as approximately solving certain fundamental equations (like the Bellman optimality equation, introduced later) for the given problem. This allows RL to scale to large problems where classical dynamic programming methods (which require a complete model and exhaustive computation) would fail. We will touch more on these computational methods in later chapters, but even at the conceptual level, it’s useful to remember that RL typically learns incrementally from experience, updating its estimates and policy as it gathers more data through interaction.

## 3. The Exploration–Exploitation Dilemma

A central challenge in reinforcement learning is the exploration–exploitation dilemma. The agent must exploit what it has learned to gain high rewards (choose the best-known action), but it also must explore actions that it is uncertain about, to discover if they might lead to even higher rewards. Balancing these two conflicting needs is crucial for successful learning:

- **Exploitation** refers to using the current knowledge to maximize immediate reward. If an agent knows (or believes) that a particular action in a given state yields a high reward, exploitation means choosing that action. For example, if a robot has learned that turning left leads to a charging station (providing a positive reward), it will exploit that knowledge whenever it needs power by consistently turning left.

- **Exploration** refers to trying out new or less certain actions to gather more information about their outcomes. This might temporarily sacrifice reward in the short term in hopes of gaining knowledge that improves long-term performance. For instance, the same robot might try turning right (even if it’s unsure what will happen) to see if perhaps there’s an even closer charging station or a shortcut, even if turning right has lower expected reward based on current knowledge.

The dilemma arises because too much exploitation can be as bad as too much exploration. If the agent exploits too greedily, it may never discover better strategies. It can get stuck in a suboptimal behavior pattern (a local optimum) because it never tries the alternatives that appear worse in the short term but could be better in the long term. On the other hand, if the agent explores too much, it will waste time and incur unnecessary negative rewards by frequently trying random actions instead of leveraging what it has already learned. In extreme cases, pure exploration would ignore the accumulated knowledge and act randomly, never reliably accumulating reward. A classic example of this dilemma is the multi-armed bandit problem, where an agent must choose among several slot machines ("one-armed bandits") each with an unknown payout rate. The agent has to allocate pulls among the machines to both discover their payout rates (exploration) and earn money from the best machine (exploitation). Various strategies (like $\epsilon$-greedy, which with probability $\epsilon$ explores a random action and with probability $1-\epsilon$ exploits the current best action) are designed to address this balance. In full-fledged reinforcement learning problems (which involve many states, not just a single bandit state), the exploration–exploitation tradeoff is even more complex. The agent might need to explore new states as well as new actions. The long-term consequences of exploration must be weighed: exploring a new state could lead to discovering an entirely new region of the state space with high rewards, or it could lead to a dead-end with many negative rewards. Effective exploration is directed (not purely random) in many algorithms, meaning the agent tries to evaluate uncertainty and directs exploration where it estimates the potential for gain is highest. It’s important to note that in most theoretical treatments of RL, especially when deriving optimal policies, one assumes that eventually the agent will explore sufficiently so that it has an accurate estimate of the value of all relevant actions. In practice, managing exploration is a design choice. Many practical RL implementations include heuristics or dedicated exploration bonuses (intrinsic rewards for exploring novel states) to encourage the agent to explore. Overall, balancing exploitation and exploration is a core theme in reinforcement learning and a major factor that distinguishes RL from supervised learning (which doesn’t have this aspect since the data is usually given). We will see algorithms and approaches that handle this dilemma in later chapters, but always remember: an RL agent must learn what to do while simultaneously learning about the environment, which is a tricky dual objective.

## 4. Examples of RL Applications

Reinforcement learning might sound abstract so far, but it has been applied successfully to a wide range of domains. Below are some illustrative examples of RL applications, demonstrating its flexibility and power:

- **Robotics**: RL is used to train robots to perform complex control tasks. This includes teaching robotic arms how to grasp objects, making legged robots learn to walk and balance, or enabling drones to perform agile maneuvers. In these cases, the state could be the robot’s sensor readings (joint angles, velocities, camera inputs), and the actions are motor commands. The reward is designed to encourage the desired behavior (e.g. a higher reward when the robot achieves the task, such as moving an object to a target location, or maintaining forward motion without falling). Over time, the robot can learn a control policy that might be difficult to hand-engineer. A notable example is robotic manipulation: researchers have used RL to make a robot hand solve a Rubik’s Cube, and to enable robots to adapt their grip in real-time while handling objects. In industrial robotics, RL can optimize efficiency – for instance, robotic arms learning to minimize time or energy for assembly tasks through trial and error. The challenges in robotics often include continuous state/action spaces and safety (the robot must avoid catastrophic exploration), but RL provides a framework for continuous improvement from experience.

- **Game playing**: Some of the most publicized successes of reinforcement learning have been in games. RL algorithms have achieved superhuman performance in classic Atari video games and even in the ancient game of Go. For example, AlphaGo, developed by DeepMind, famously defeated a world champion Go player by combining RL with deep neural networks. The AlphaGo system learned from both human games and millions of self-play games, using RL to continually refine its strategies. Likewise, algorithms like DQN (Deep Q-Network) learned to play Atari 2600 video games directly from pixel inputs, outperforming humans in many of them by learning effective policies from the reward signal (game score). Beyond board and video games, RL has also been applied to chess and shogi (AlphaZero learned these games via self-play RL), and even to game theory settings or simulated economic games. Games provide a controlled environment to test and showcase RL – they have clear rules, a reward (win/lose or game score) and allow millions of rapid simulations. These successes have demonstrated that RL agents can learn very complex behaviors that rival or exceed human expertise.

- **Dialogue systems**: In natural language processing, RL is used to train agents that hold conversations or interact with humans. For example, a dialogue system (like a chatbot for customer service or personal assistants) can use RL to learn how to respond to user inputs in a way that maximizes some notion of long-term user satisfaction or task success. The state might include the dialogue history, the action is the next reply to the user, and the reward could be immediate user feedback or a delayed metric like successful completion of a task (booking a ticket, answering a question). Dialogue systems face the challenge that user feedback is often implicit or delayed (the user might only say "thanks, that helped" at the end). Researchers use RL to fine-tune dialogue policies, often after initializing them with supervised learning, to optimize for qualities like helpfulness, relevance, or user engagement. A specific example is training a negotiation bot via self-play: two agents converse (starting from some initial strategy) and receive a reward based on the outcome of the negotiation; over many iterations, they can learn more effective negotiation tactics. While dialogue systems present additional complexities (like understanding natural language), RL provides a way to go beyond next-word prediction and move towards goal-directed conversational behavior.

- **Online advertising and recommendations**: Many online systems face the problem of sequential decision making with user interaction, which is a natural fit for RL. In online advertising, for instance, when deciding which ad to show to a user, the system can be seen as an agent taking an action (choosing an ad) in a state (user’s context or profile) to receive a reward (whether the user clicks or converts). The system must balance exploring new ads or strategies (to learn which ads might appeal to the user) with exploiting known preferences to maximize immediate revenue. This can be modeled as a bandit problem or a full RL problem if the sequence of ads shown has an effect on user behavior (for example, showing too many repetitive ads might reduce engagement over a session). Companies use variations of RL to optimize click-through rates and user engagement metrics over long user sessions. Similarly, recommender systems (e.g. video or music recommendation) can employ RL to personalize content selection for users over time, aiming to maximize watch time or satisfaction. The reward in these scenarios is often a proxy metric like a click, a purchase, or time spent – something that reflects user satisfaction or business value. By continually updating its policy based on user interactions, an RL-based recommendation system can adapt to changing user preferences or new content.

- **Self-driving cars**: Autonomous driving involves making a sequence of decisions in a complex, dynamic environment – a natural playground for RL. Consider a self-driving car that must decide how to steer, accelerate, or brake. The state is the car’s perception of the environment (from sensors like cameras, LiDAR, etc., including positions of other cars, pedestrians, etc.), the actions are the continuous control inputs (steering angle, throttle, brake), and the reward must capture the goals of driving safely and efficiently. For example, a reward function could give positive rewards for progress towards the destination and negative rewards for collisions, sudden braking, or leaving the lane. Various research efforts use RL for sub-tasks of driving: lane keeping, adaptive cruising, or parking. One example is training an agent to perform automatic parking by reward feedback – successful parking yields a high reward, collisions a large negative reward, and the agent practices in simulation until it learns a robust parking policy. Another example is using RL for motion planning: selecting trajectories that avoid obstacles and follow rules; the agent learns by repeated trial (often in simulation) to navigate complex intersections. Companies have also used simulation environments (like the open-source OpenAI Gym or CARLA simulator) to train driving policies with deep RL. While real-world deployment of fully RL-driven cars is still an open challenge (due to safety and reliability requirements), the approach holds promise for improving specific driving skills and for automatically handling edge cases by learning from experience rather than relying purely on hand-coded rules.

- **Healthcare**: Decision-making in healthcare can benefit from RL, as many medical problems involve sequential decisions with patient-specific states. For example, RL has been explored for treatment planning: an agent recommends treatments or dosages over time for a patient, aiming to maximize health outcomes. The state could include the patient’s current health metrics and history, actions are treatment choices (which drug or therapy to administer, what dosage, etc.), and the reward is related to patient improvement (positive reward for recovery, negative for adverse events or deterioration). A concrete application is in dosing strategies for chronic conditions, where an RL agent learns an optimal policy for adjusting medication based on patient response. Another is in clinical trial design – adaptively adjusting treatments for participants based on intermediate outcomes. In healthcare settings, RL’s ability to handle delayed rewards is crucial, since a treatment’s benefit might only become apparent after some time. Additionally, RL can incorporate patient-specific personalization, leading to policies tailored to individual patients. While applying RL in healthcare must be done carefully (due to the high stakes and need for safety), it offers a way to derive treatment policies from data, complementing medical expertise. For instance, researchers have used RL to suggest optimal sequences of chemotherapy and radiotherapy for cancer patients, or to manage sepsis treatment in ICU by learning from retrospective patient data. A study of RL in healthcare noted that RL can find optimal policies using past experiences without requiring a complete mathematical model of the patient’s biology. This is valuable in medicine where the underlying system (human body) is extremely complex. As electronic health record data and medical sensors proliferate, there is increasing potential for RL to assist in making data-driven clinical decisions, always under appropriate supervision and validation by healthcare professionals.

These examples show how reinforcement learning provides a general framework for goal-directed learning from interaction that can be adapted to many domains. Whether it’s a game, a robot, a conversation, an ad, a car, or a patient treatment, we can identify states, actions, and rewards to formulate the problem for an RL agent. The successes in games and robotics have inspired new research in other fields, and as computational power and algorithms improve (especially with function approximation via deep learning), RL’s range of real-world applications is rapidly expanding. By studying the core principles in this book, you will be equipped to understand these applications and even contribute to advancing RL in these exciting domains.

## 5. Key Properties of Reinforcement Learning

Having discussed what RL is and examples of its use, let’s summarize some key properties that characterize reinforcement learning and distinguish it from other types of learning:

- **Learning from Interaction**: RL is inherently interactive. An RL agent learns by interacting with an environment over time. This interaction is sequential and often state-dependent. The agent’s experiences are generated by its own actions. This property means that unlike in supervised learning, data collection and learning are entwined – the agent is both a learner and an experimenter in its world.

- **Goal-directed Learning**: The agent has a purpose encapsulated by the reward signal. It is not just trying to model the environment or predict something abstract; it is trying to achieve a goal (maximize cumulative reward). This makes RL evaluative (actions are judged by outcomes) rather than instructive. The notion of a long-term goal leads to planning and foresight: the agent must consider not just immediate rewards, but how its actions serve the ultimate objective.

- **Sequential Decision Making (Planning)**: Reinforcement learning problems involve a chain of decisions, not isolated one-shot predictions. The agent often must plan several steps ahead (implicitly or explicitly) because current actions can influence future states and opportunities. Even model-free RL methods, which don't explicitly plan using a model, must learn value estimates that encode the expected future rewards. In essence, RL involves solving sequential decision problems, much like those studied in the field of optimal control or planning (e.g. path planning, scheduling tasks). However, in RL this planning happens through learning and trial-and-error rather than a known model.

- **Delayed Rewards and Credit Assignment**: Rewards may not be immediately received after an action – they can be delayed. The agent must integrate information over time to determine which actions were beneficial. The credit assignment problem (deciding which actions in the past led to which outcomes) is a key challenge. This property forces the agent to maintain some memory or state representation of past decisions or to propagate reward information backwards in time (which is exactly what algorithms like Temporal-Difference learning and Backpropagation Through Time do, as we'll see later).

- **Uncertainty and Stochasticity**: RL typically deals with uncertain outcomes. The environment’s response to an action may be probabilistic (for example, a robot’s motors might slip unpredictably, or an opponent in a game might respond in various ways). The agent usually cannot fully predict what will happen – it must learn a policy that works on average or with high probability. This means the agent often learns value functions that estimate expected returns (expectations over stochastic outcomes). Handling uncertainty is a core part of RL, linking it to fields like statistics and probability theory.

- **No Supervisor (Evaluation vs Instruction)**: In RL there is no supervisor telling the agent the correct action to take. The only feedback is the reward, which is often sparse and delayed. The agent might perform a long sequence of actions with zero reward and then get a positive reward at the end if it succeeds. This is different from supervised learning where each training example comes with an immediate correct label or target. Thus, the agent must self-discover good behavior. The reward signal is an evaluation of how well the agent is doing, not a full instruction on what to do.

- **Experience-Based Adaptation**: RL relies on the agent’s ability to improve its behavior with experience. Initially, the agent might behave poorly (like a novice), but as it gathers more experience, it updates its policy and value estimates to perform better. This learning can be online (during the task) or through re-playing experience (off-line learning from a replay buffer or past log of interactions). Over time, the agent adapts to the environment’s dynamics and reward structure. If the environment changes, a well-designed RL agent can continue to learn and adapt to the new conditions, exhibiting a form of continuous learning.

- **Use of Function Approximation (when needed)**: In many practical RL applications, the state or action space is very large or continuous (e.g., continuous sensor readings). A key property of modern RL is the use of function approximation (such as neural networks) to generalize from seen states to unseen states. While this is more about the implementation of RL algorithms than the fundamental problem, it’s a noteworthy aspect: RL algorithms often approximate the value function or policy rather than computing them exactly, due to the enormity of possible states. This connects RL to supervised learning (since function approximation uses supervised signals derived from reward feedback). The result is that an RL agent can make reasonable decisions in states it has never encountered before, by generalizing from its past experiences.

In summary, reinforcement learning is characterized by active learning through trial-and-error, goal-oriented behavior with long-term consequences, and the lack of explicit instruction. It requires dealing with temporal aspects (state transitions, delayed effects) and uncertainty. These properties make RL both powerful (it can solve very complex, even ill-defined goal-seeking tasks) and challenging (it often requires more data or careful tuning to learn effectively, and the theory is more complex due to the sequential nature). Understanding these properties helps one appreciate why RL algorithms are designed the way they are. As we proceed, keep these traits in mind, as they will reappear when we discuss specific algorithms and how they address the challenges inherent in reinforcement learning.

## 6. The Agent–Environment Interface

The formal framework for reinforcement learning centers on the interaction between an agent and its environment. We define everything from the perspective of this agent–environment interface. At each discrete time step $t$, the agent perceives the state $S_t$ of the environment and chooses an action $A_t$ to execute. The environment then responds by transitioning to a new state $S_{t+1}$ and emitting a reward $R_{t+1}$ to the agent. This interaction loop was illustrated in Figure 1 above. Let’s break down the key components and definitions in this interface:

- **Agent**: The agent is the learner or decision-maker. In software terms, this is the algorithm or model we are training. It receives information (states, rewards) and outputs actions. The agent contains the policy (its decision-making rule) and any internal mechanisms for learning (like updating value estimates or adjusting policy parameters). The agent is embedded in the loop and its goal is to improve its policy to maximize cumulative reward.

- **Environment**: The environment is everything outside the agent that the agent interacts with. It’s the world that responds to the agent’s actions. The environment defines the dynamics (state transitions in response to actions) and provides rewards. From a software modeling perspective, the environment can be a simulator or the real world itself. The agent–environment boundary is chosen such that everything the agent cannot control is considered the environment, including possibly other agents if we’re in a multi-agent scenario. For example, in a chess game, the environment includes the opponent (since our learning agent cannot directly control the opponent’s moves).

- **State ($S_t$)**: A state is a representation of the environment’s configuration at time $t$. It should contain all the information needed to determine what might happen next (more formally, states in many RL formulations satisfy the Markov property, discussed in the next section). The state is fed to the agent. States can be low-dimensional (like coordinates of a robot) or high-dimensional (like an image frame from a game). We often distinguish between the true state of the environment and the agent’s observations – in this introductory context we assume the agent fully observes the true state (the fully observable case). If the agent has incomplete information (partial observability), the observations may not capture the full state of the environment, leading to a Partially Observable Markov Decision Process (POMDP) – but we won’t delve into POMDPs here.

- **Action ($A_t$)**: An action is a choice made by the agent that can influence the state of the environment. Each state $S_t$ has a set of possible actions (the action space available in that state). Actions could be discrete (e.g. {up, down, left, right} moves in a grid, or a choice of a discrete accelerations) or continuous (e.g. any real-valued steering angle). The agent’s policy will specify how actions are chosen. After the agent selects $A_t$, the environment will receive that input and react.

- **Reward ($R_{t+1}$)**: The reward is a crucial scalar feedback signal. At time $t+1$, after the agent’s action, the environment gives a number $R_{t+1}$ to the agent. This number indicates the immediate utility of the state transition that just happened. In essence, reward measures how good or bad the outcome was in that moment. The agent’s objective is to maximize the cumulative reward it receives over time. The design of the reward function is an important aspect of modeling an RL problem – it encodes the task we want the agent to accomplish. For example, in a game, winning might give a reward of +1, losing -1, and all other steps 0. In a robot scenario, one could give a small negative reward at each time step to encourage faster task completion, and a big positive reward when the task is done. Rewards can be stochastic or deterministic functions of the state and action. Often we denote by $r(s,a)$ the expected immediate reward when taking action $a$ in state $s$.

- **Policy ($\pi$)**: The policy is the agent’s behavior function. It can be thought of as a mapping from states to a probability distribution over actions. We denote it as $\pi(a|s)$ = P($A_t=a \mid S_t=s$), the probability that the agent takes action $a$ in state $s$. If the policy is deterministic, we can write it as $a = \pi(s)$, the action chosen in state $s$. The policy is the thing we ultimately want to learn or improve – it defines the agent’s approach to the task. A good policy is one that results in lots of reward over time. Policies can be fixed (non-learning agent) or adaptive (improving via learning). In RL algorithms, the policy might be explicitly stored and updated (policy-based methods) or it might be implicit in the value function (value-based methods where the agent picks the action with highest estimated value).

- **Trajectory/Episode**: As the agent and environment interact, they produce a sequence (or trajectory) of states, actions, and rewards: $S_0, A_0, R_1, S_1, A_1, R_2, \dots$. In episodic tasks, this sequence will eventually end when a terminal state is reached (for example, game over in a game, or task completion). Each sequence from a start to a terminal state is called an episode. In continuing tasks (no natural end), the sequence could, in theory, go on indefinitely; in such cases we often consider a discount factor to keep total rewards bounded.

- **Return ($G_t$)**: Although not an entity in the interface per se, it’s useful to define the return at time $t$, denoted $G_t$, as the total discounted reward from time $t$ onward. If we have rewards $R_{t+1}, R_{t+2}, \dots, R_T$ until the end of an episode (or to infinity for continuing tasks), the return might be defined as $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$, where $0 \le \gamma \le 1$ is the discount factor. The discount factor $\gamma$ determines how much future rewards are worth relative to immediate rewards – a $\gamma$ of 0 makes the agent short-sighted (only cares about immediate reward), while a $\gamma$ near 1 makes it far-sighted (valuing future rewards almost as much as immediate ones). We introduce $\gamma$ here for completeness; it is a key part of the mathematical formulation of cumulative reward.

- **Model (of the environment)**: A model in RL context refers to the agent’s representation of the environment’s dynamics, i.e., a function or process that predicts the next state and reward given a state and action. If the agent has a model, it can simulate the environment internally, which enables planning. Not all RL methods use models – many successful approaches are model-free, meaning they don’t explicitly estimate the transition probabilities or reward function, they just learn from observations. However, conceptually, the environment has some underlying rules: often we assume there are state transition probabilities $\mathbb{P}(s'|s,a)$ (the probability of next state $s'$ given current state $s$ and action $a$) and a reward function $r(s,a)$ or $r(s,a,s')$. Together, those define the model of the environment. Some RL algorithms will try to learn these (model learning) and then use them for planning (these are called model-based methods).

The agent–environment interface can be diagrammed as in Figure 1, and formally described as a Markov Decision Process, which we will define shortly. At this interface, the agent is completely characterized by its policy (and possibly its value estimates and internal learning process), while the environment is characterized by the state set, action set, transition dynamics, and reward function. It's important to clearly delineate what the agent can observe and affect versus what is outside its control. This formulation is very general. By choosing different definitions for states, actions, and rewards, you can cast many sequential decision problems into this agent-environment framework. For example:

- In a chess game: State = arrangement of pieces on the board (plus whose turn it is); Action = a legal chess move for the current player; Reward = 0 for most moves until the end, +1 for winning, -1 for losing (one could also give small penalties for losing pieces, etc., but let's keep it simple). The episode is one chess game from start to finish.

- For a robot path planning task: State = robot’s location and maybe velocity; Action = direction or motion command; Reward = -1 per time step (to encourage shorter paths) plus a big +100 reward if the robot reaches the goal, and maybe a -100 if it crashes. The episode ends when the robot reaches the goal or crashes or times out.

- In an investment portfolio management scenario: State = current portfolio holdings and market indicators; Action = rebalance portfolio (how much to invest in each asset); Reward = change in portfolio value (profit or loss) over the next interval. This might be a continuing task where we use a discount factor.

In each of these, we identify the agent, environment, state, action, reward, etc. With these identified, we are ready to describe the problem formally as a Markov Decision Process and then discuss how an agent can learn a good policy. But before that, a quick note on the Markov assumption, which underpins the mathematics of state transitions in most RL formulations.

## 7. Mathematical Foundations of RL

To analyze and design reinforcement learning algorithms rigorously, we typically assume the underlying problem can be described as a Markov Decision Process (MDP). MDPs provide the mathematical foundation for sequential decision making with randomness and rewards.

### The Markov Assumption

An MDP relies on the Markov property for state transitions. Informally, the Markov property means memorylessness: the future is independent of the past given the present. In the context of an agent’s interaction, this assumption is that the state encapsulates all relevant information from the history. More precisely, a state $S_t$ is Markov if:

$$
P(S_{t+1} = s' \mid S_t = s, A_t = a, S_{t-1} = s_{t-1}, A_{t-1} = a_{t-1}, \dots) = P(S_{t+1} = s' \mid S_t = s, A_t = a).
$$

This equation says that the probability distribution of the next state $s'$ depends only on the current state $s$ and action $a$, and not on any earlier states or actions. In other words, once you know the current state, the history doesn't provide any additional predictive power for the next state. The state is a sufficient statistic for the future. This assumption is powerful because it simplifies the dynamics: we can describe the environment by one-step transition probabilities and rewards without worrying about long-term history. It does put a burden on how we define the state – we must ensure the state representation includes all information necessary to be predictive of what happens next. If important information is left out of the state representation, the process might not be Markov. For example, if the environment actually has some hidden variable influencing transitions, and our state doesn’t include it, then the Markov property is violated from the agent’s perspective (leading to a POMDP). In practice, part of the art of applying RL is choosing a state representation that is “rich enough” to be approximately Markov. For now, we will assume the problem satisfies the Markov property by construction.

### Markov Decision Processes (MDPs)

A Markov Decision Process is a formal tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ that provides a mathematical model of the agent-environment interaction. Let’s define each element:

- $\mathcal{S}$: The set of states. Each state $s \in \mathcal{S}$ is a configuration the environment (and agent) can be in. $\mathcal{S}$ can be finite or infinite.

- $\mathcal{A}$: The set of actions. When in state $s$, the agent can choose any action $a$ from the set of actions $\mathcal{A}(s)$ available in that state (sometimes $\mathcal{A}$ is independent of $s$, sometimes not, but usually we assume a fixed action set for simplicity).

- $P$: The state transition probabilities, defining the dynamics of the environment. For any states $s, s' \in \mathcal{S}$ and action $a \in \mathcal{A}$,

$$
P(s' \mid s, a) = \Pr\{S_{t+1} = s' \mid S_t = s, A_t = a\},
$$

is the probability that the next state is $s'$ given that the current state is $s$ and action $a$ is taken. This is sometimes also denoted by $P_{s,a}(s')$ or $P_{sa}^{s'}$. The collection of these probabilities for all $s, a, s'$ essentially defines the dynamics model of the environment. If the environment is deterministic, then for each $s, a$ there is a single $s'$ with $P(s'|s,a)=1$.

- $R$: The reward function. It defines the immediate reward distribution for state-action (and next state) pairs. Formally, $R(s,a,s')$ is the expected value of the reward $R_{t+1}$ given $S_t=s, A_t=a, S_{t+1}=s'$. Often an MDP is described with a simplified reward function $R(s,a)$ which is the expected reward for taking action $a$ in state $s$ (this assumes the reward depends only on the current state and action, not explicitly on the resulting state, which is a common formulation). In any case, the agent when taking action $a$ in state $s$ will on average receive reward $R(s,a)$ and find itself in a new state according to $P(\cdot|s,a)$. Note: In some texts, $R(s)$ or $R(s,s')$ might be used if reward only depends on state or on the transition. The Markov property also applies to rewards – we assume the reward for the transition is dependent on $(s,a)$ (and possibly $s'$) and not on earlier history. We sometimes write $r_t$ or $R_{t+1}$ for the actual scalar reward received at a particular time step, distinguishing it from the expected values given by $R(s,a)$.

- $\gamma$: The discount factor $\gamma \in [0,1]$. This is not strictly part of the definition of the environment, but it is usually included in the definition of an MDP when we talk about return and value (particularly for infinite-horizon or continuing tasks). $\gamma$ determines how future rewards are weighted relative to immediate rewards. If $\gamma = 0$, the agent only cares about immediate reward $R_{t+1}$. If $\gamma = 1$, the agent is undiscounted and cares about the sum of all future rewards (for episodic tasks that eventually end – for a continuing task, $\gamma$ must be < 1 to avoid infinite returns unless we use other means). Typically $\gamma$ is something like 0.9 or 0.99 in many problems, meaning each time step into the future, rewards are worth 90% or 99% as much as they would be if they were immediate. Discounting formalizes the idea that "sooner rewards are better" and also mathematically ensures the return $G_t$ is finite if an episode does not end.

When we specify an MDP, we often also specify a starting state distribution (possible starting states for episodes), and a set of terminal states (states which, when reached, end the episode). But the core of the MDP is the quintuple $(\mathcal{S},\mathcal{A},P,R,\gamma)$. To reiterate, an MDP is a fully specified environment for RL problems under the Markov assumption. If the agent knows the MDP (knows $P$ and $R$), then the reinforcement learning problem reduces to a planning problem: the agent could, in principle, compute an optimal policy by solving the MDP using dynamic programming (because with $P$ and $R$ known, it becomes a classic planning problem for which algorithms like value iteration and policy iteration can be used). However, in most RL scenarios, the agent does not initially know $P$ or $R$. It might know $\mathcal{S}$ and $\mathcal{A}$ (what states and actions exist), or it might not even know that. The agent must learn from experience to behave well, without having a complete model of the MDP. This difference is often described as learning (RL) versus planning (classical dynamic programming). Nonetheless, MDPs provide the theoretical underpinning: we measure an RL algorithm’s performance in terms of how well it does in the underlying MDP. MDPs also allow us to define what it means to be optimal. Specifically, we can define the concept of a value function on states (and state-action pairs) and derive the Bellman equations, which characterize optimal behavior. That’s our next topic.

## 8. Value Functions and Bellman Equations

Much of reinforcement learning theory and algorithms revolve around the concept of value functions. A value function tells us how good it is to be in a state (or to take a certain action in a state) in terms of expected future rewards. Intuitively, the value of a state is the total amount of reward an agent can expect to accumulate starting from that state (now and in the future), if it behaves optimally or according to a particular policy. Value functions are useful because they allow the agent to plan ahead in a sense: by knowing the value of subsequent states, the agent can select actions that lead to states of high value, thus obtaining high reward. Let's define two key types of value functions:

- **State-Value Function ($V$)**: Given a policy $\pi$ (a way of behaving), the state-value function $V^\pi(s)$ is defined as the expected return (cumulative discounted reward) when starting in state $s$ and following policy $\pi$ thereafter. Formally,

$$
V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s\right],
$$

for all states $s \in \mathcal{S}$. Here $G_t$ denotes the return from $t$ onward as defined earlier, and the expectation $\mathbb{E}_\pi[\cdot]$ indicates that actions are taken according to policy $\pi$ inside the expectation. This definition means $V^\pi(s)$ is answering: “If we start in state $s$ and always choose actions as dictated by $\pi$, what reward total can we expect to accumulate (discounted into the future)?” The value function essentially assigns a numerical score to each state, reflecting the desirability of that state under policy $\pi$. A high value means that state is a "good place to be" because a lot of reward can be obtained from there if one acts according to $\pi$. A low (or negative) value means the state is bad under $\pi$ (likely leads to poor outcomes or loss of rewards). Note that if $\pi$ is the optimal policy, $V^\pi(s)$ will be the optimal value of state $s$ (more on optimal value in a moment). For now, $V^\pi$ is tied to a policy.

- **Action-Value Function ($Q$)**: Similarly, the action-value function (or Q-value) $Q^\pi(s,a)$ is defined for state-action pairs. $Q^\pi(s,a)$ is the expected return if the agent starts in state $s$, takes action $a$, and thereafter follows policy $\pi$. Formally,

$$
Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s, A_t = a\right].
$$

This quantity $Q^\pi(s,a)$ can be thought of as: "if I’m in state $s$ and I try action $a$, how good is that, considering I will follow $\pi$ afterwards?" It breaks the value into two parts: the immediate reward from doing action $a$ in $s$, and the value of whatever state results, following $\pi$. State-value and action-value are closely related: In fact, given $Q^\pi$, you can get $V^\pi$ by taking the expected value over the policy's action choice:

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^\pi(s,a).
$$

This just says: in state $s$, policy $\pi$ chooses action $a$ with probability $\pi(a|s)$. The value of $s$ under $\pi$ is the weighted average of the Q-values of $(s,a)$ for actions $a$ chosen by $\pi$. For a deterministic policy, $V^\pi(s) = Q^\pi(s, a=\pi(s))$. Now, why are these value functions so useful? Two reasons:

1. **Relation to optimal policy**: If you somehow knew the true values $Q^*(s,a)$ for all state-action pairs – where $Q^*$ denotes the action-value function for the optimal policy – then acting greedily with respect to these values would give you an optimal policy. Specifically, an optimal policy $\pi^*$ would choose in each state $s$ an action $a$ that maximizes $Q^*(s,a)$. Because $Q^*(s,a)$ already accounts for the future, picking the action with highest $Q^*$ is the best immediate decision. This means the Q-value function alone can dictate optimal decisions. Similarly, $V^*(s)$ (the optimal state-value) is the expected return from $s$ if you behave optimally; and $V^*(s) = \max_a Q^*(s,a)$.

2. **Recursive relationships (Bellman equations)**: Value functions satisfy important recursive equations due to the Markov property and the definition of return. These are the Bellman equations. They allow us to relate the value of a state to the values of its successor states, which forms the basis for many solution methods (both analytical and learning algorithms).

Let’s derive the Bellman equation for $V^\pi$ (the state-value function for a given policy $\pi$). Consider a particular state $s$. Under policy $\pi$, the agent will take some action $a$ according to $\pi(a|s)$. Then it will receive an immediate reward $R_{t+1}$ (with expectation $r(s,a)$) and transition to a next state $s'$ with probability $P(s'|s,a)$. From that state $s'$, the value is $V^\pi(s')$ (because from $s'$ onward the agent still follows $\pi$). So the expected return from $s$ can be broken down into two parts: the one-step reward plus the discounted value of the next state. Taking expectation over both the random action (as per $\pi$) and the random next state (as per $P$), we get:

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right].
$$

This is the Bellman expectation equation for the policy $\pi$. In words, it says: the value of state $s$ under policy $\pi$ equals the expected immediate reward plus the expected discounted value of the next state, where the expectation is over the action choice of $\pi$ and the environment’s stochastic transition. It’s a self-consistency condition – $V^\pi$ must satisfy this for all states $s$. Written more succinctly using expectation notation:

$$
V^\pi(s) = \mathbb{E}_\pi\left[R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s\right].
$$

The above is an example of a Bellman equation. It is essentially a linear equation (or system of equations, one per state) for the values $V^\pi(s)$. If the MDP is finite and $\pi$ is fixed, this set of equations can be solved in principle (e.g., by iterative methods or matrix inversion) to find $V^\pi$. Many RL algorithms like policy evaluation (part of policy iteration) are based on iteratively applying this Bellman update to converge to $V^\pi$. There is a similar Bellman equation for the action-value function $Q^\pi$. It can be derived by considering the first step: from $(s,a)$, you get reward $R_{t+1}$ and next state $s'$, then follow policy $\pi$. So:

$$
Q^\pi(s,a) = \mathbb{E}\left[R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a\right] = r(s,a) + \gamma \sum_{s'} P(s' \mid s, a) V^\pi(s').
$$

And if we substitute the expression for $V^\pi(s')$ (expanding it one step further), we could also express $Q^\pi$ in terms of $Q^\pi$ of next state-action pairs (this gets into Bellman operator concepts, but let's keep it simple). Now, the more exciting part is the Bellman optimality equations. These equations characterize the value function $V^*$ of an optimal policy $\pi^*$, without needing to know $\pi^*$ upfront. They are non-linear (because of a max operator), but they are the cornerstone of understanding optimal solutions. For optimal state-value $V^*(s)$, we can reason as follows: if you are in state $s$, you will choose whatever action maximizes your expected return. If that action $a$ leads to a next state $s'$, you will then get optimal return from $s'$, which is $V^*(s')$. So the Bellman optimality equation for $V^*$ is:

$$
V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma V^*(s')\right].
$$

This equation says: the optimal value of state $s$ equals the maximum (over actions) of the expected immediate reward plus discounted value of the next state, assuming optimal play onward. Notice this is a kind of self-consistency: if $V^*$ is the true optimal value function, plugging it in on the right, the equation should hold. Conversely, any $V$ that satisfies this Bellman optimality equation must be the optimal value function. This equation is non-linear due to the max, but there are standard algorithms (like value iteration) that can solve it iteratively by turning it into an update:

$$
V_{\text{new}}(s) \leftarrow \max_{a} \sum_{s'} P(s' \mid s, a) \left[R(s, a, s') + \gamma V_{\text{old}}(s')\right].
$$

Repeatedly applying that update will converge to $V^*$. Once you have $V^*$, you can extract an optimal policy $\pi^*$ by choosing for each state $s$ any action $a$ that achieves the argmax in the above equation. There is also a Bellman optimality equation for the optimal action-value function $Q^*(s,a)$:

$$
Q^*(s,a) = \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left[R(s, a, s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s', a')\right].
$$

This is a one-step lookahead: the Q-value of $(s,a)$ under optimal behavior equals the expected reward plus discounted value of the next state, assuming optimal action $a'$ is taken next. Compare this to the non-optimal $Q^\pi$ equation we gave earlier – the difference is we replace "follow policy $\pi$" with "take the best possible next action". The Bellman optimality equations are at the heart of RL because they give us a way to compute or approximate optimal values and hence optimal policies. Many RL algorithms can be viewed as trying to estimate $Q^*$ or $V^*$ without knowing $P$ and $R$ explicitly, by sampling transitions through interaction. For example, Q-learning (a famous RL algorithm) is essentially an iterative update to approximate the Bellman optimality equation for $Q^*$, using actual experienced transitions (samples of $s, a, r, s'$). Another class of algorithms, policy iteration, uses the Bellman expectation equation for a given policy (policy evaluation) and then improves the policy by acting greedily w.r.t. the value function (policy improvement); it can be shown that this converges to satisfy the Bellman optimality equation. It’s worth noting why we use discounting ($\gamma < 1$) in many cases: it ensures that the infinite sum of rewards converges and places progressively less weight on rewards further in the future (which often aligns with practical considerations like uncertainty in long-term predictions). In episodic problems that always terminate, you can set $\gamma = 1$ and consider undiscounted total reward (the episode will end so the sum is finite). The theory still holds, but sometimes even in episodic tasks a discount less than 1 is used to encourage focusing on nearer-term rewards (or to approximate a situation where there is a small probability the episode might continue indefinitely). Finally, a side concept: optimality and uniqueness – for finite MDPs, there is at least one optimal policy $\pi^*$. All optimal policies share the same value function $V^*$ (and $Q^*$). The Bellman optimality equation typically has a unique solution for $V^*$ (assuming some mild conditions), so solving it yields the value of states under any optimal policy. However, there could be multiple policies that achieve those values (multiple ways to be optimal). As an example, in some states there may be two or more actions that are equally good and lead to equally good outcomes; any of those actions could be chosen by an optimal policy. To recap this section: value functions ($V$ and $Q$) are predictions of future reward. They satisfy Bellman equations that relate the value of a state to the values of successor states. The Bellman optimality equation characterizes the value function of the optimal policy. These equations provide both conceptual insight (they tell us what optimality means in a recursive way) and practical algorithms (many RL methods work by trying to satisfy these equations from data). We’ve kept the explanations intuitive, but mathematically, you can see that RL algorithms often involve solving these equations approximately, since we usually don't know $P$ and $R$ to solve them exactly. In Chapter 2, we will dive into specific algorithms (like dynamic programming, Monte Carlo, Temporal-Difference learning) that use these principles to compute or estimate value functions and ultimately find optimal policies.

## 9. Conclusion and Summary

In this chapter, we introduced the fundamental concepts of reinforcement learning as a learning paradigm. Let’s summarize the key points:

- **Reinforcement Learning vs Other Paradigms**: RL is one of the three basic machine learning frameworks. Unlike supervised learning (which learns from labeled examples) or unsupervised learning (which finds patterns in unlabeled data), RL is about an agent learning from direct interaction with an environment, guided by a reward signal rather than explicit correct answers. The agent’s goal is to maximize cumulative reward, which often requires strategizing over long horizons and coping with uncertain outcomes. This makes RL especially suited to problems of sequential decision-making and control.

- **Learning from Interaction**: An RL agent learns through a continuous feedback loop: state → action → reward → next state. The agent’s actions influence its future data (the next states and rewards), creating a dependency on its own behavior. The agent must explore to discover profitable actions and exploit its knowledge to accumulate reward, balancing the two via the exploration–exploitation tradeoff. This trial-and-error learning process enables agents to improve their performance over time, even in initially unknown environments.

- **Exploration–Exploitation Dilemma**: We discussed the importance of exploration in RL – an agent cannot succeed by greedily sticking to what it already knows, because it may miss out on better behaviors. Conversely, it can’t just explore forever either, because it would fail to gather rewards. We highlighted strategies conceptually (like $\epsilon$-greedy) to manage this balance. This dilemma is a recurring theme as we design and analyze RL algorithms.

- **Applications**: RL has been applied to many domains: robotics (for learning motor skills and control policies), games (where it has achieved superhuman performance in complex games like Go), conversational AI (optimizing dialogue policies), recommender systems and ad placement (making sequential content suggestions or decisions to maximize user engagement or revenue), autonomous vehicles (decision making for driving), and even healthcare (treatment planning). These examples illustrate that whenever we have a sequential decision problem with a clear goal, we can potentially formulate it as an RL problem. Each domain brings its own challenges (safety in healthcare, sample efficiency in the real world, etc.), but the core RL ideas remain applicable.

- **Key Properties of RL**: We summarized key characteristics that define RL problems: the need for active interaction, goal-directed behavior with cumulative rewards, handling delayed effects and credit assignment, dealing with uncertainty and stochastic outcomes, and learning policies based on evaluative feedback rather than supervised signals. Recognizing these properties helps to choose appropriate solution methods and to set expectations (e.g., RL often needs more samples to learn than supervised learning because it must explore and learn the consequences of actions).

- **Agent–Environment Interface**: We formalized the RL problem as an agent interacting with an environment in discrete time steps. The agent selects actions; the environment responds with next states and rewards. We defined states, actions, rewards, policy, etc., and assumed the Markov property for the state dynamics. This led to the formulation of RL tasks as Markov Decision Processes, which provide the standard language and mathematical framework for analyzing RL problems.

- **Markov Decision Processes**: We described MDPs in terms of states $\mathcal{S}$, actions $\mathcal{A}$, transition probabilities $P(s'|s,a)$, reward function $R(s,a)$, and optional discount factor $\gamma$. MDPs assume the Markov property (the future depends only on the current state and action). They provide a way to compute optimal policies in principle (using dynamic programming if the model is known), and they underpin the theoretical guarantees for many RL algorithms. Even when the model is not known, most RL algorithms seek to approximate what could be computed if the MDP were known.

- **Value Functions and Bellman Equations**: We introduced the concept of state-value function $V^\pi(s)$ and action-value function $Q^\pi(s,a)$ as measures of future expected reward. We discussed how these can be used to evaluate how good states or actions are, and how an optimal policy can be inferred by looking at what actions maximize value. Crucially, we covered the Bellman equations – the recursive relations that tie together values of states. The Bellman expectation equation for a given policy describes consistency of $V^\pi$ with one-step lookahead of that policy. The Bellman optimality equation characterizes $V^*$ (and $Q^*$) by taking a max over actions, capturing the best possible step from each state. These equations form the foundation for solving MDPs and for RL algorithms that estimate values from experience.

At this point, you should have a high-level but solid understanding of what reinforcement learning entails and the fundamental concepts that any RL method builds upon. We have not yet discussed how to compute the optimal policies or how specific algorithms (like Q-learning, policy gradients, etc.) work – that will come in subsequent chapters. But you now understand the problem setting and goal of reinforcement learning: an agent in a possibly unknown environment, learning to make decisions to maximize reward over time, operating under the Markov assumption with the dynamics possibly unknown, and leveraging the mathematical toolset of MDPs and Bellman equations. Reinforcement learning is a rich field that combines ideas from dynamic programming, probability, optimization, and even psychology and neuroscience (where similar trial-and-error reward-driven learning is studied in animals and humans). Its importance is growing as we seek to build AI systems that can make complex decisions autonomously in unstructured environments – from robots in homes to autonomous cars on roads to trading agents in markets. Understanding the basics covered in this chapter is crucial before moving on to algorithmic and advanced topics. In the next chapter, we will delve into algorithms for solving reinforcement learning problems, starting with dynamic programming approaches that assume a known model, and then progressing to model-free learning methods that can learn $V^\pi$, $Q^\pi$, or directly $\pi$ from interaction data. We will see how the Bellman equations introduced here can be turned into practical update rules, and how an agent can learn efficiently even when it doesn’t know the environment’s dynamics upfront. By combining the foundational concepts you’ve learned here with concrete algorithms, you will be well on your way to understanding and implementing reinforcement learning solutions to a variety of problems.