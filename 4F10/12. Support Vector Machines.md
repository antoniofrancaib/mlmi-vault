
# 1. The Binary Classification Framework

## Problem Statement

**Dataset:**  
Consider a dataset  

$$D=\{(\mathbf{x}_n,t_n)\}_{n=1}^N,$$  

where each input $\mathbf{x}_n \in \mathbb{R}^d$ is a feature vector and each target $t_n$ is a label in $\{-1,+1\}$.

**Objective:**  
Learn a classifier $y(\mathbf{x})$ so that:  

- $y(\mathbf{x}) \geq 0$ for $t=+1$, and  
- $y(\mathbf{x}) < 0$ for $t=-1$.  

**Decision Boundary:**  
The decision boundary is defined by the set of input points where  

$$y(\mathbf{x}) = 0.$$  

A new input $\mathbf{x}'$ is correctly classified if  

$$t' \cdot y(\mathbf{x}') > 0.$$  
## Intuition   
The goal of classification is to choose a function that correctly "separates" the two classes. In the simplest case, the classifier is linear, and its sign determines the predicted class.

# 2. Linear Classifiers and Linear Separability  

## The Linear Classifier  

**Function Form:**  A linear classifier can be expressed as:  

$$y(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b,$$  
where:  
- $\mathbf{w}$ is a weight vector,  
- $b$ is a bias (intercept).  

**Geometric Interpretation:**  The vector $\mathbf{w}$ is orthogonal to the decision boundaryâ€”a hyperplane where $y(\mathbf{x}) = 0$. Changing $b$ shifts the hyperplane without changing its orientation.  

## Linear Separability  

**Definition:**  A classification problem is linearly separable if there exists at least one hyperplane (i.e., a choice of $\mathbf{w}$ and $b$) such that all training points are classified correctly (i.e., no errors on the training set).  

**Multiple Solutions:**  When the data is linearly separable, many hyperplanes can achieve zero training error. The challenge is to select the hyperplane that is expected to generalize best to new data.  

# 3. Maximum Margin Classifiers  

## The Margin Concept  

**Margin Definition:**  The margin is defined as the distance from the decision boundary (the hyperplane) to the closest data point in any class. A larger margin is often associated with better generalization.  

**Scaling Invariance:**  Note that if we scale the classifier by any $c>0$, i.e.,  

$$y'(\mathbf{x}) = c \left[ \mathbf{w}^T \mathbf{x} + b \right],$$  
the decision boundary (where $y(\mathbf{x}) = 0$) remains unchanged.  

## Normalizing the Scale  

**Support Vectors and Normalization:**  To uniquely define the classifier, we choose the scaling such that for any support vector (the point that lies closest to the hyperplane):  

- For a positive support vector $\mathbf{x}_+$, set  
$$\mathbf{w}^T \mathbf{x}_+ + b = +1.$$  
- For a negative support vector $\mathbf{x}_-$, set  
$$\mathbf{w}^T \mathbf{x}_- + b = -1.$$  
With this normalization, the margin can be shown to be 

$$\text{Margin} = \frac{\mathbf{w}^T (\mathbf{x}_+ - \mathbf{x}_-)}{2 \|\mathbf{w}\|} = \frac{1}{\|\mathbf{w}\|}$$

## Choosing the Optimal Hyperplane  

**Maximization Objective:**  The idea is to choose the hyperplane that maximizes this margin. Intuitively, a larger margin separates the classes more robustly and is thought to improve the classifier's ability to generalize.  

**Implication:**  Because the classifier is completely determined by its support vectors (the points exactly on the margin), these are the only points that directly influence the position of the decision boundary.  

# 4. Deriving the Max-Margin Classifier  

## Primal Formulation  

Given the normalization (with support vectors satisfying the above equations), the minimum (absolute) value of the classifier outputâ€”attained on the support vectorsâ€”is $1$. Therefore, we want to maximize the margin:  

$$\text{Margin} = \frac{1}{\|\mathbf{w}\|}.$$  

Maximizing $\frac{1}{\|\mathbf{w}\|}$ is equivalent to minimizing $\|\mathbf{w}\|$ (or, for mathematical convenience, minimizing $\frac{1}{2} \|\mathbf{w}\|^2$). The optimization problem becomes:  

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{subject to } t_n (\mathbf{w}^T \mathbf{x}_n + b) \geq 1, \quad n=1,\dots,N.
$$  
## Optimization with Inequality Constraints  

### Constrained Optimization Background  

**Generic Formulation:**  
Consider the problem  

$$
\max_{x,y} f(x,y) \\
\text{subject to } g(x,y) \geq 0.
$$  

**Lagrange Multipliers:**  
One forms the Lagrangian:  

$$
\mathcal{L}(x,y,\lambda) = f(x,y) + \lambda g(x,y),
$$  

where $\lambda$ is the Lagrange multiplier. For an equality constraint $g(x,y) = 0$, the stationarity condition reads:  

$$
\nabla_{x,y} f(x,y) = -\lambda \nabla_{x,y} g(x,y).
$$  

**Two Cases:**  

1. **Inactive Constraint:** If at the optimum $g(x^*,y^*) > 0$ (i.e., the constraint does not "bind"), then the optimum satisfies $\lambda = 0$ and $\nabla_{x,y} f(x,y) = 0$.  
2. **Active Constraint:** If $g(x^*,y^*) = 0$ (i.e., the constraint is tight), then $\lambda > 0$ and the stationarity condition $\nabla_{x,y} f(x,y) = -\lambda \nabla_{x,y} g(x,y)$ applies.  

### Karush-Kuhn-Tucker (KKT) Conditions  

For inequality constraints, the KKT conditions formalize the solution requirements:  

1. **Primal feasibility:**  
   $$g(x,y) \geq 0.$$  
2. **Dual feasibility:**  
   $$\lambda \geq 0.$$  
3. **Complementary Slackness:**  
   $$\lambda g(x,y) = 0.$$  
4. **Stationarity:**  
   $$\nabla_{x,y} f(x,y) = -\lambda \nabla_{x,y} g(x,y).$$  

Applied to the SVM optimization problem, these conditions ensure that only support vectors (points for which $t_n (\mathbf{w}^T \mathbf{x}_n + b) = 1$) have nonzero multipliers and contribute to defining the optimal hyperplane.  

# 5. The Dual Formulation of the Hard-Margin SVM  

## Formulating the Lagrangian  

For the constrained minimization problem  

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2, \\
\text{subject to } t_n (\mathbf{w}^T \mathbf{x}_n + b) \geq 1,
$$  
introduce Lagrange multipliers $a_n \geq 0$ (one per constraint) to construct:  

$$
\mathcal{L}(\mathbf{w}, b, \mathbf{a}) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{n=1}^N a_n \left\{ t_n (\mathbf{w}^T \mathbf{x}_n + b) - 1 \right\}.
$$  
## Stationarity Conditions  
Set the gradients with respect to $\mathbf{w}$ and $b$ to zero:  

- For $\mathbf{w}$:  
  $$
  \nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{n=1}^N a_n t_n \mathbf{x}_n = 0 \Rightarrow \mathbf{w} = \sum_{n=1}^N a_n t_n \mathbf{x}_n.
  $$  
- For $b$:  
  $$
  \frac{\partial \mathcal{L}}{\partial b} = -\sum_{n=1}^N a_n t_n = 0 \Rightarrow \sum_{n=1}^N a_n t_n = 0.
  $$  
## The Dual Problem  

We now substitute $\mathbf{w}$ into each term of $\mathcal{L}$.

ðŸ”¹ First term: $\frac{1}{2} |\mathbf{w}|^2$

$$
\frac{1}{2} \|\mathbf{w}\|^2 
= \frac{1}{2} \left( \sum_{n=1}^N a_n t_n \mathbf{x}_n \right)^T \left( \sum_{m=1}^N a_m t_m \mathbf{x}_m \right) 
= \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m \mathbf{x}_n^T \mathbf{x}_m
$$

This becomes the second term of the dual.

ðŸ”¹ Second term: $-\sum_n a_n t_n (\mathbf{w}^T \mathbf{x}_n)$

Since $\mathbf{w}^T \mathbf{x}_n = \left( \sum_m a_m t_m \mathbf{x}_m^T \right) \mathbf{x}_n$, we get:

$$
-\sum_n a_n t_n \mathbf{w}^T \mathbf{x}_n 
= -\sum_n a_n t_n \sum_m a_m t_m \mathbf{x}_m^T \mathbf{x}_n 
= -\sum_{n,m} a_n a_m t_n t_m \mathbf{x}_n^T \mathbf{x}_m
$$

Notice this exactly cancels the previous quadratic term. But wait â€” it doesnâ€™t cancel. Itâ€™s part of the full quadratic expression. See next.

ðŸ”¹ Third term: $-\sum_n a_n t_n b$

Since $\sum_n a_n t_n = 0$, this term vanishes:

$$
-\sum_n a_n t_n b = -b \sum_n a_n t_n = 0
$$

ðŸ”¹ Fourth term: $+\sum_n a_n$

This comes from distributing the $-1$ inside the constraint expression:

$$
-\sum_n a_n (-1) = \sum_n a_n
$$


$$
\max_{\mathbf{a}} \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m (\mathbf{x}_n^T \mathbf{x}_m) \\
\text{ subject to } \sum_{n=1}^N a_n t_n = 0, \quad a_n \geq 0, \quad n=1,\dots,N.
$$  
This quadratic programming problem is convex and has a unique solution. Notably, only those training points for which $a_n > 0$ (the support vectors) contribute to $\mathbf{w}$ and the prediction function.  

## Prediction  

Once the optimal multipliers $\{a_n^*\}$ are found, the classifier is defined by:  

$$
y(\mathbf{x}) = \sum_{n \in S} a_n^* t_n (\mathbf{x}_n^T \mathbf{x}) + b,
$$  
where $S = \{n : a_n^* > 0\}$ is the set of support vectors. The bias $b$ can be determined using any support vector (typically those with $0 < a_n < C$) by enforcing the condition:  

$$
t_n y(\mathbf{x}_n) = 1,
$$  

and numerical stability is usually improved by averaging over all such conditions.  

# 6. Soft Margin SVM and Slack Variables  

## Motivation  

In practice, data may not be perfectly linearly separable, or even if separable, a very narrow margin might lead to poor generalization due to noise or model overfitting. To counter these issues, slack variables $\xi_n \geq 0$ are introduced to allow for some training errors or margin violations.  

## Modified Constraints and Objective  

The new constraints become:  

$$
t_n (\mathbf{w}^T \mathbf{x}_n + b) \geq 1 - \xi_n, \quad \xi_n \geq 0, \quad n=1,\dots,N.
$$  

**Interpretation of $\xi_n$:**  

- $\xi_n = 0$: The point is correctly classified and lies outside or exactly on the margin.  
- $0 < \xi_n \leq 1$: The point is inside the margin but still correctly classified.  
- $\xi_n > 1$: The point is misclassified.  

The optimization objective is modified to penalize violations:  

$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{n=1}^N \xi_n,
$$  
where $C > 0$ is a regularization parameter that controls the trade-off:  

- **Small $C$:** Emphasizes a larger margin even if it means more training errors (soft constraints).  
- **Large $C$:** Forces the solution to prioritize fewer errors, even if the margin is smaller (hard constraints).  

## Dual Formulation for the Soft Margin  

Introducing Lagrange multipliers $a_n \geq 0$ for the margin constraints and $\mu_n \geq 0$ for the slack variables, and following similar steps as in the hard-margin case, one obtains the dual problem:  

$$
\max_{\mathbf{a}} \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m (\mathbf{x}_n^T \mathbf{x}_m) \\
\text{subject to } \sum_{n=1}^N a_n t_n = 0, \quad 0 \leq a_n \leq C, \quad n=1,\dots,N.
$$  

The additional constraint $a_n \leq C$ arises from the interaction between the Lagrange multipliers and the slack variables. The prediction function remains the same:  

$$
y(\mathbf{x}) = \sum_{n \in S} a_n t_n (\mathbf{x}_n^T \mathbf{x}) + b.
$$  

## Practical Considerations  

**Choice of $C$:**  
In practice, the optimal value of $C$ is chosen via validation techniques; one tries a range of values and selects the one that yields the best performance on a held-out dataset.  

**Computational Cost:**  
Solving the quadratic programming problem typically requires computational costs ranging between $O(N^2)$ and $O(N^3)$. Therefore, SVMs are most effective when $N$ (the number of training points) is not excessively large (e.g., up to around 50,000).  

# 7. Summary and Key Insights  

- **Binary Classification:**  
  The fundamental task is to separate two classes by designing a function $y(\mathbf{x})$ whose sign indicates the predicted class.  

- **Linear Classifiers:**  
  When using a linear function $y(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$, the problem often reduces to finding the proper hyperplane.  

- **Maximum Margin Principle:**  
  Among many possible hyperplanes that classify the training data perfectly (in the linearly separable case), the one that maximizes the margin (the distance to the closest data points) is preferred for its theoretical advantages in generalization.  

- **Optimization and Duality:**  
  By normalizing the classifier (setting the support vectors to $\pm 1$), the problem becomes minimizing $\frac{1}{2} \|\mathbf{w}\|^2$ subject to linear inequalities. Using Lagrange multipliers and the KKT conditions, we derive a dual formulation where only support vectors (points with nonzero multipliers) determine the solution.  

- **Soft Margin SVM:**  
  Introducing slack variables $\xi_n$ provides robustness in real-world scenarios with noisy or non-separable data. The regularization parameter $C$ balances between a wide margin and a low training error.  

- **Numerical Stability:**  
  In practice, the bias $b$ is computed by averaging the values derived from each support vector constraint to improve numerical stability.  

## Final Thoughts  

Support Vector Machines represent an elegant blend of geometric intuition and convex optimization. They demonstrate how theoretical insights (such as maximizing the margin) translate into practical algorithms that can achieve excellent generalization performance. By carefully deriving the dual formulation and introducing slack variables to handle non-ideal datasets, SVMs offer both robustness and interpretability. These foundational concepts not only serve as a basis for more complex models in deep learning but also provide a rich framework for understanding the role of optimization under constraints in machine learning.  

These notes have covered all the significant topics and ideas presented, with expanded explanations for nontrivial steps. They should serve as a solid foundation for an advanced masterclass on the subject, ensuring that you develop a deep intuition for SVMs and their theoretical underpinnings.