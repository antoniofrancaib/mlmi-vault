Lectures by Andrew Fitzgibbon

Topics: 

# 1. From Discriminative to Generative Modeling

**Discriminative Classifiers:**  
Traditionally, discriminative models focus solely on the decision boundary between classes. For example, a classifier might learn to separate "oranges" from "non-oranges" without modeling the full data distribution. This leads to the question:  

*"But how do you know what you don't know?"*  
In other words, if you don't model the underlying data distribution, you may have no clue about areas of low or ambiguous density. Since discriminative models **don’t model $p(\mathbf{x})$**, they **don’t know how typical or atypical** a data point is.

**Generative Approach:**  
By contrast, generative models aim to capture the full probability distribution $p(\mathbf{x})$ of the data. Even when the true distribution is complex (and not, for instance, a simple Gaussian as in 1809's models), we can attempt to do better by modeling the distribution itself. One common approach is to use a mixture of Gaussians, which we discuss next.

# 2. Mixture of Gaussians (GMMs)

**Single Gaussian Model:**  
A single Gaussian is given by  

$$p(\mathbf{x}\mid\boldsymbol{\mu},\boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right).$$


However, many real-world distributions are not well approximated by a single Gaussian.  

**Using $K$ Gaussians:**  
In a mixture model, we assume that  

$$p(\mathbf{x}) = \sum_{k=1}^K a_k\, p(\mathbf{x}\mid\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k),$$  

where $a_k$ are the mixing coefficients (with $\sum_k a_k = 1$).  
The twist is that we do not know which Gaussian generated which point; the assignment is latent (hidden).  

**Learning via Maximum Likelihood:**  
Given training data $\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^n$, we wish to find the parameters  

$$\boldsymbol{\theta} = \{a_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\}_{k=1}^K$$  
that maximize the likelihood $p(\mathcal{D}\mid\boldsymbol{\theta})$.

# 3. Maximum Likelihood Estimation (MLE) for Mixture Models

**Likelihood and Log-Likelihood:**  
The joint likelihood is  

$$p(\mathcal{D}\mid\boldsymbol{\theta}) = \prod_{i=1}^n \sum_{k=1}^K a_k\, p(\mathbf{x}_i\mid\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k).$$  
Taking the logarithm (which turns the product into a sum) gives:  

$$\mathcal{L}(\boldsymbol{\theta}) = \log p(\mathcal{D}\mid\boldsymbol{\theta}) = \sum_{i=1}^n \log\left(\sum_{k=1}^K a_k\, p(\mathbf{x}_i\mid\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\right).$$  

**Direct Optimization Challenges:**  

- *Differentiation:* One might try to differentiate with respect to parameters (e.g., $\boldsymbol{\mu}_k$) and set the derivative to zero. For instance, differentiating the log-likelihood with respect to a mean leads to a weighted average:  

$$\boldsymbol{\mu}_k = \frac{\sum_i w_{ik}\, \mathbf{x}_i}{\sum_i w_{ik}},$$  

where $w_{ik}$ are the weights (responsibilities) indicating how much Gaussian $k$ "explains" point $\mathbf{x}_i$.  

- *Gradient Descent:* Alternatively, one may attempt gradient descent; however, the sum inside the logarithm makes the gradients complicated.  

**Hard vs. Soft Assignment:**  
A naive strategy might be to "hard assign" each sample to the most likely Gaussian and then update the parameters. While this can work, it ignores uncertainty. The EM algorithm uses a soft assignment (or "responsibility") $w_{ik}$ that reflects the probability of point $i$ coming from Gaussian $k$.

# 4. The EM Algorithm: An Intuitive Derivation

The Expectation-Maximization (EM) algorithm provides a strategy for dealing with latent variables by iteratively improving a lower bound on the log-likelihood.  

## 4.1 The Challenge of the Log-Sum  
The log-likelihood has the form:  

$$\mathcal{L}(\boldsymbol{\theta}) = \sum_i \log\left(\sum_k a_k\, p(\mathbf{x}_i\mid\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\right).$$  

**Non-Linearity Issue:**  
Unlike the simpler case where $\log(a e^b) = \log a + b$, here the log does not "distribute" over the sum:  

$$\log(a e^b + c e^d) \neq \log(a e^b) + \log(c e^d).$$

This prevents a direct maximization.  

## 4.2 Using Jensen's Inequality  
**Jensen's Inequality:** For any concave function $f$ and weights $\{w_k\}$ with $\sum_k w_k = 1$,  

$$f\left(\sum_k w_k E_k\right) \geq \sum_k w_k\, f(E_k).$$  

**Auxiliary Function:**  
By "swapping" the log and sum using Jensen's inequality, we derive a lower bound on the log-likelihood:  

$$\mathcal{L}(\boldsymbol{\theta}) \geq Q(\boldsymbol{\theta}^{(k)}, \boldsymbol{\theta}),$$  

where the auxiliary function $Q(\boldsymbol{\theta}^{(k)}, \boldsymbol{\theta})$ is defined as  

$$Q(\boldsymbol{\theta}^{(k)}, \boldsymbol{\theta}) = \sum_i \sum_k p(k\mid\mathbf{x}_i,\boldsymbol{\theta}^{(k)}) \log\left(a_k\, p(\mathbf{x}_i\mid\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)\right).$$  
Maximizing $Q$ instead of $\mathcal{L}$ gives us a procedure that is guaranteed to improve (or at least not decrease) the true log-likelihood.  

## 4.3 Deriving $Q$
We start with the log-likelihood:

$$
L(\theta) = \sum_{i=1}^{n} \log p(x_i \mid \theta) = \sum_{i=1}^{n} \log \sum_{k} p(x_i, k \mid \theta)
$$

We want $L(\theta_{t+1}) > L(\theta_t)$ remember we know $\theta_t$ already. So we consider:

$$
L(\theta_{t+1}) - L(\theta_t) = \sum_{i=1}^{n} \log \frac{p(x_i \mid \theta_{t+1})}{p(x_i \mid \theta_t)} > 0
$$

Write this as:

$$
\begin{aligned}
L(\theta_{t+1}) - L(\theta_t) 
&= \sum_{i=1}^{n} \log \left( \sum_k \frac{p(x_i, k \mid \theta_{t+1})}{p(x_i \mid \theta_t)} \right) \\
&\geq \sum_{i=1}^{n} \sum_k p(k \mid x_i, \theta_t) \log \left( \frac{p(x_i, k \mid \theta_{t+1})}{p(k \mid x_i, \theta_t) \, p(x_i \mid \theta_t)} \right) \\
&= \sum_{i=1}^{n} \sum_k p(k \mid x_i, \theta_t) \log \left( \frac{p(x_i, k \mid \theta_{t+1})}{p(x_i, k \mid \theta_t)} \right) =  Q(\theta_t, \theta_{t+1}) - Q(\theta_t, \theta_t)
\end{aligned}
$$

where we applied Jensen's Inequality:

$$
\log \left( \sum_k w_k E_k \right) \geq \sum_k w_k \log(E_k)
$$

and defined:

$$
Q(\theta_t, \theta') = \sum_{i=1}^{n} \sum_k p(k \mid x_i, \theta_t) \log \left( p(x_i, k \mid \theta') \right)
$$

We call $\mathcal{Q}$ the **auxiliary function**. If we can ensure:

$$
Q(\theta_t, \theta_{t+1}) - Q(\theta_t, \theta_t) \geq 0
$$

then we will know that:

$$
L(\theta_{t+1}) \geq L(\theta_t)
$$

## 4.4 EM Iterative Steps  
**Initialization:**  
Begin with an initial guess $\boldsymbol{\theta}^{(0)}$ for the parameters.  

**E-step (Expectation):**  
Compute the "responsibilities" or soft assignments:  

$$w_{ik} = p(k\mid\mathbf{x}_i,\boldsymbol{\theta}^{(k)}) = \frac{\mathcal{N}(x_i ; \mu_k, \Sigma_k)}{\sum_{k'} \mathcal{N}(x_i ; \mu_{k'}, \Sigma_{k'})} \quad \text{in the case of Gaussian Mixture}.$$  
These represent the probability that data point $\mathbf{x}_i$ came from Gaussian $k$.  

**M-step (Maximization):**  
Update the parameters by maximizing the auxiliary function:  

$$\boldsymbol{\theta}^{(k+1)} = \arg\max_\boldsymbol{\theta} Q(\boldsymbol{\theta}^{(k)}, \boldsymbol{\theta}).$$  
For example, differentiating with respect to $\boldsymbol{\mu}_k$ leads to the weighted mean update shown earlier.  

**Iteration:**  
Repeat the E-step and M-step until convergence. Each iteration is guaranteed not to decrease the likelihood.

# 5. Latent Variable Generative Models and Bayesian Networks

![[latent-generative-models.png]]
Latent variable models are powerful because they explicitly introduce hidden (latent) variables that explain the observed data. There are several archetypal models:  

**Factor Analysis (FA):**  

- *Latent Variable:* Continuous ($\mathbf{z} \in \mathbb{R}^p$)  
- Factor Analysis models the observed variable $\mathbf{x} \in \mathbb{R}^D$ as: 

$$
\mathbf{x} = \mathbf{C} \mathbf{z} + \boldsymbol{\epsilon}, \quad \text{where } \mathbf{z} \sim \mathcal{N}(0, \mathbf{I}), \; \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{\Sigma}_{\text{diag}}) $$
	So:
	- $\mathbf{z}$ is a latent (hidden) variable in a low-dimensional space $\mathbb{R}^p$, where $p \ll D$.
	- $\mathbf{C} \in \mathbb{R}^{D \times p}$ maps latent factors to observations.
	- $\boldsymbol{\epsilon}$ is independent noise with diagonal covariance.

Then the marginal distribution over $\mathbf{x}$ is:

$$
p(\mathbf{x}) = \mathcal{N}(0, \mathbf{C} \mathbf{C}^\top + \boldsymbol{\Sigma}_{\text{diag}})
$$

- *Interpretation:*  FA can be seen as a method to find a **low-dimensional manifold** that underlies high-dimensional data or as a way to obtain a **compact representation of the covariance matrix**.  If $\boldsymbol{\Sigma}_{\text{diag}} = \sigma^2 \mathbf{I}$, then the model reduces to probabilistic PCA.  

	1. **Low-Dimensional Manifold Interpretation**
	
	The observed data $\mathbf{x}$ is generated from a lower-dimensional latent space $\mathbf{z}$ via a linear transformation $\mathbf{C}$. So $\mathbf{x}$ lies near a $p$-dimensional subspace of $\mathbb{R}^D$, corrupted by noise $\boldsymbol{\epsilon}$. This is very similar to PCA — but with explicit noise modeling, which leads us to...
	
	2. **Covariance Decomposition Interpretation**
	
	The covariance of $\mathbf{x}$ is: $\text{Cov}[\mathbf{x}] = \mathbf{C} \mathbf{C}^\top + \boldsymbol{\Sigma}_{\text{diag}}$
	
	So FA expresses the full covariance matrix as:
	
	- A **low-rank part** $\mathbf{C}\mathbf{C}^\top$, capturing the shared variance (correlated structure) across dimensions.
	- A **diagonal part** $\boldsymbol{\Sigma}_{\text{diag}}$, capturing independent noise in each dimension.
	
	It explains dependencies in high-dimensional data using a small number of latent factors + per-dimension noise.
	
	3. **Special Case: Probabilistic PCA**
	
	If: $\boldsymbol{\Sigma}_{\text{diag}} = \sigma^2 \mathbf{I}$
	
	Then the model becomes: $p(\mathbf{x} \mid \mathbf{z}) = \mathcal{N}(\mathbf{C} \mathbf{z}, \sigma^2 \mathbf{I})$
	
	This is exactly the probabilistic PCA model (Tipping & Bishop, 1999).


**Gaussian Mixture Models (GMMs):**  

- *Latent Variable:* Discrete indicator variable $c$ (often represented as a one-hot vector or simply an index)  
- *Generative Process:*  
$$p(\mathbf{x}) = \sum_{m=1}^M P(c_m)\, p(\mathbf{x}\mid c_m).$$  
- *Use:* Widely used for clustering, density estimation, and as a building block for more complex models.  

**Discrete Mixture Models:**  
Both the latent variable and the observation might be discrete. These models are common in applications such as document clustering.  

Each of these models can be represented as a Bayesian network. In these diagrams, arrows indicate the direction of dependency (from the latent variable $q$ to the observation $\mathbf{x}$).

# 6. Continuous Auxiliary Functions and Variational EM

## 6.1 Factor Analysis and the Auxiliary Function  
For models like FA, the continuous form of **auxiliary function** becomes
$$
\mathcal{Q}(\theta^{(k)}, \theta^{(k+1)}) = \int p(\mathbf{z} \mid \mathbf{x}, \theta^{(k)}) \log \left( p(\mathbf{x}, \mathbf{z} \mid \theta^{(k+1)}) \right) \, d\mathbf{z}
$$

The joint log-probability (of data and latent variable) is  

$$
\begin{aligned}
\log \left( p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta}) \right)
&= \log \left( p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\theta}) \right)
+ \log \left( p(\mathbf{z}) \right) \\
&= \log \left( \mathcal{N}(\mathbf{x} ; \mathbf{A} \mathbf{z}, \boldsymbol{\Sigma}_{\text{diag}}) \right)
+ \log \left( \mathcal{N}(\mathbf{z} ; \mathbf{0}, \mathbf{I}) \right) \\
&= -\frac{1}{2} \left(
\mathbf{z}^\top \mathbf{A}^\top \boldsymbol{\Sigma}_{\text{diag}}^{-1} \mathbf{A} \mathbf{z}
- 2 \mathbf{z}^\top \mathbf{A}^\top \boldsymbol{\Sigma}_{\text{diag}}^{-1} \mathbf{x}
+ \mathbf{x}^\top \boldsymbol{\Sigma}_{\text{diag}}^{-1} \mathbf{x}
\right) \\
&\quad - \frac{1}{2} \log \left( \left| \boldsymbol{\Sigma}_{\text{diag}} \right| \right) + C
\end{aligned}
$$  
Because all terms are Gaussian, the expectation over the latent variable $\mathbf{z}$ involves only the first and second moments. For example, terms such as  

$$\mathbf{z}^\top \mathbf{A}^\top \boldsymbol{\Sigma}_{\text{diag}}^{-1} \mathbf{A}\mathbf{z} = \text{trace}\left(\mathbf{A}^\top \boldsymbol{\Sigma}_{\text{diag}}^{-1} \mathbf{A}\, \mathbb{E}[\mathbf{z}\mathbf{z}^\top]\right)$$  
emerge naturally.  

## 6.2 Variational EM Framework  
**Basic Idea:**  Rather than maximizing the difficult log-likelihood directly, we maximize an auxiliary function (often called the variational lower bound) defined by:  

$$\mathcal{F}(q, \boldsymbol{\theta}) = \int q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) \log\left(\frac{p(\mathbf{x},\mathbf{z}\mid\boldsymbol{\theta})}{q(\mathbf{z}, \tilde{\boldsymbol{\theta}})}\right) d\mathbf{z}.$$  
Here, $q(\mathbf{z}, \tilde{\boldsymbol{\theta}})$ is an arbitrary distribution that approximates the true posterior $p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta})$.  

**EM Revisited:**  

- *E-step:*  Given current parameters $\boldsymbol{\theta}^{(k)}$, set the auxiliary distribution to the true posterior:  
$$q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}) = p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta}^{(k)}).$$  
- *M-step:*  Maximize the lower bound:  
$$\boldsymbol{\theta}^{(k+1)} = \arg\max_\boldsymbol{\theta}\; \mathcal{F}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}), \boldsymbol{\theta}\right).$$  
- *Guarantee:*  
This process guarantees that  

$$\mathcal{L}(\boldsymbol{\theta}^{(k)}) = \mathcal{F}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}), \boldsymbol{\theta}^{(k)}\right) \leq \mathcal{F}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}), \boldsymbol{\theta}^{(k+1)}\right) \leq \mathcal{L}(\boldsymbol{\theta}^{(k+1)}),$$  
so that the log-likelihood is non-decreasing with each iteration.  

**Generalization:**  In the generalized (or variational) EM framework, one may choose other forms for $q(\mathbf{z}, \tilde{\boldsymbol{\theta}})$. A common and powerful choice is the mean-field approximation, where the joint auxiliary distribution factorizes:  

$$q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) = \prod_{i=1}^n q_i(\mathbf{z}_i, \tilde{\boldsymbol{\theta}}).$$

# 7. Kullback-Leibler (KL) Divergence and Its Role

**Definition and Intuition:**  
The KL divergence between two probability distributions $p(\mathbf{x})$ and $q(\mathbf{x})$ is defined as  
$$\text{KL}(p \parallel q) = \int p(\mathbf{x}) \log\left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) d\mathbf{x}.$$  
It measures how "different" $p$ is from $q$ and is always non-negative, with equality if and only if $p(\mathbf{x}) = q(\mathbf{x})$ almost everywhere.  

**Derivation: positivity of KL** 
Consider two PDFs, $p(x)$ and $q(x)$.
$$
\mathcal{KL}(p(x) \| q(x)) = \int p(x) \log \left( \frac{p(x)}{q(x)} \right) dx
= - \int p(x) \log \left( \frac{q(x)}{p(x)} \right) dx
$$

Using $\log(y) \leq y - 1$, we can write:

$$
\int p(x) \log \left( \frac{q(x)}{p(x)} \right) dx
\leq \int p(x) \left( \frac{q(x)}{p(x)} - 1 \right) dx
= \int (q(x) - p(x)) dx = 0
$$

This gives the following inequality:
$$
\mathcal{KL}(p(x) \| q(x)) = \int p(x) \log \left( \frac{p(x)}{q(x)} \right) dx \geq 0
$$

**KL for Gaussians:**  For two Gaussian distributions  

$$p(\mathbf{x}) = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1),$$
$$q(\mathbf{x}) = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2),$$  
the divergence is given by:  

$$\text{KL}(p \parallel q) = \frac{1}{2} \left(\text{tr}\left(\boldsymbol{\Sigma}_2^{-1} \boldsymbol{\Sigma}_1\right) - d + (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^\top \boldsymbol{\Sigma}_2^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2) + \log\frac{|\boldsymbol{\Sigma}_2|}{|\boldsymbol{\Sigma}_1|}\right).$$  
As a simple example, if $p(\mathbf{x}) = \mathcal{N}(0,1)$ and $q(\mathbf{x}) = \mathcal{N}(\mu,1)$, varying $\mu$ will change the divergence, illustrating how "off" the approximation is.  

![[KL-plot.png]]

**In Variational Inference:**  
The KL divergence is used to measure the difference between the variational distribution $q(\mathbf{z}, \tilde{\boldsymbol{\theta}})$ and the true posterior $p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta})$. Minimizing this divergence is equivalent to tightening the lower bound on the log-likelihood.

# 8. Log-Likelihood, Lower Bounds, and Variational Approximations

**Decomposition:**  
For any valid distribution $q(\mathbf{z}, \tilde{\boldsymbol{\theta}})$, the log-likelihood can be decomposed as:  

$$\log p(\mathbf{x}\mid\boldsymbol{\theta}) = \mathcal{F}(q, \boldsymbol{\theta}) + \text{KL}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) \parallel p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta})\right),$$ 
where  
$$\mathcal{F}(q, \boldsymbol{\theta}) = \int q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) \log\left(\frac{p(\mathbf{x},\mathbf{z}\mid\boldsymbol{\theta})}{q(\mathbf{z}, \tilde{\boldsymbol{\theta}})}\right) d\mathbf{z}.$$  
Since the KL divergence is always non-negative, $\mathcal{F}(q, \boldsymbol{\theta})$ is a lower bound on $\log p(\mathbf{x}\mid\boldsymbol{\theta})$.  
Equality holds when $q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) = p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta})$.  

**Variational Approximation (Slide Example):**  
One can write:  

$$\log p(\mathbf{x}\mid\boldsymbol{\theta}) = \left\langle \log\left(\frac{p(\mathbf{x},\mathbf{z}\mid\boldsymbol{\theta})}{q(\mathbf{z}, \tilde{\boldsymbol{\theta}})}\right) \right\rangle_{q(\mathbf{z}, \tilde{\boldsymbol{\theta}})} + \text{KL}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) \parallel p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta})\right),$$  
which is the starting point for the variational EM algorithm.

# 9. EM Revisited: General Form and Convergence Guarantees

**Iterative Scheme Recap:**  
Start with an initial parameter estimate $\boldsymbol{\theta}^{(0)}$ and set  

$$q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(0)}) = p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta}^{(0)}).$$  
Then at each iteration $k$:  

- *M-step:*  Update $\boldsymbol{\theta}^{(k+1)}$ by maximizing the auxiliary function:  
$$\boldsymbol{\theta}^{(k+1)} = \arg\max_\boldsymbol{\theta}\; \left\langle \log \frac{p(\mathbf{x},\mathbf{z}\mid\boldsymbol{\theta})}{q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)})} \right\rangle_{q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)})}.$$  
- *E-step:*  Update the auxiliary distribution by setting  

$$q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k+1)}) = p(\mathbf{z}\mid\mathbf{x},\boldsymbol{\theta}^{(k+1)}).$$  

**Guarantee:**  Under this scheme, one obtains a chain of inequalities:  

$$\mathcal{L}(\boldsymbol{\theta}^{(k)}) = \mathcal{F}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}), \boldsymbol{\theta}^{(k)}\right) \leq \mathcal{F}\left(q(\mathbf{z}, \tilde{\boldsymbol{\theta}}^{(k)}), \boldsymbol{\theta}^{(k+1)}\right) \leq \mathcal{L}(\boldsymbol{\theta}^{(k+1)}),$$  
ensuring that the log-likelihood is non-decreasing and that the algorithm converges (to a local maximum).  

**Generalized EM:**  In practice, one might not be able to compute the exact posterior. The generalized form allows any $q(\mathbf{z}, \tilde{\boldsymbol{\theta}})$ (for instance, via a mean-field factorization):  

$$q(\mathbf{z}, \tilde{\boldsymbol{\theta}}) = \prod_{i=1}^n q_i(\mathbf{z}_i, \tilde{\boldsymbol{\theta}}),$$  
and one then minimizes the KL divergence to improve the bound.

# 10. Summary and Final Remarks

**From Boundaries to Distributions:**  While discriminative classifiers focus on decision boundaries, generative models—especially those with latent variables—provide a richer understanding of the data by modeling its full distribution.  

**Mixture Models and EM:**  Mixture models (like GMMs) illustrate both the power and challenges of generative modeling. The EM algorithm addresses these challenges by iteratively refining soft assignments (responsibilities) and updating model parameters to maximize a tractable lower bound.  

**Latent Variable Models:**  Models such as Factor Analysis, Gaussian Mixture Models, and discrete mixtures show how latent variables can be used to explain complex data. The variational EM framework generalizes these ideas and underpins many modern machine learning methods (e.g., variational autoencoders, hidden Markov models, Kalman filters).  

**Mathematical Foundations:**  The use of Jensen's inequality to derive a lower bound, the role of the KL divergence, and the iterative improvement guaranteed by the EM algorithm are central ideas that provide both theoretical rigor and practical guidance in model fitting.  

