Lectures by Andrew Fitzgibbon

Topics: 


# 1. Overview of Deep Learning with Structured Data

Deep learning models are functions that map an input vector to an output vector. In many applications—such as image classification or molecular property regression—the goal is to learn a function

$$f(\mathbf{x};\theta)$$

parameterized by $\theta$ that predicts a desired output $\mathbf{y}$ given an input $\mathbf{x}$. The design of a "good" function involves choosing both the architecture (how layers are composed) and the learning algorithm.

**Key ideas include:**

- **Input/Output Representations:** Even if inputs and outputs are high-dimensional (e.g., images or multi-dimensional signals), the model is essentially learning a mapping between vector spaces.

- **Family of Functions:** We define a family $f(\mathbf{x};\theta)$ (often as a deep network) and then use a training set

$$D=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^n$$

to select parameters $\theta$ that minimize a chosen loss function.

# 2. Loss Functions and Their Role

## 2.1 Regression Loss
For tasks such as 1D regression, a common choice is the least squares loss:

$$L(y,y^{\text{obs}})=(y-y^{\text{obs}})^2.$$

The training objective is then to minimize the total loss over the dataset:

$$L(\theta)=\sum_{i=1}^n L(f(\mathbf{x}_i;\theta),\,y_i).$$

## 2.2 Classification Loss
For classification problems where the label is one out of $K$ classes:

- **Softmax Transformation:** First, the network produces a vector $\mathbf{y}\in\mathbb{R}^K$. The softmax function converts these raw scores into probabilities:

$$z_k=\frac{\exp(y_k)}{\sum_{k'}\exp(y_{k'})}.$$

- **Cross-Entropy Loss:** The loss for a sample with true class $l$ is the negative log-likelihood:

$$L(\mathbf{y},\mathbf{y}^{\text{obs}})=-\log z_l=-\left(y_l-\log\sum_{k'}\exp(y_{k'})\right).$$

In both cases, the goal is to minimize the loss over the training set, adjusting $\theta$ accordingly.

# 3. Neural Network Architectures and Activations

## 3.1 The Building Blocks
- **Affine Transformation:** A typical layer performs an operation of the form

$$\mu(\mathbf{A},\mathbf{x},\mathbf{b})=\mathbf{A}\mathbf{x}+\mathbf{b}.$$

- **Activation Functions:** The nonlinearity $\phi$ is applied elementwise. A popular choice is the ReLU:

$$\phi(x)=\max(x,0),$$

which introduces piecewise linear behavior.

## 3.2 Layer Composition and Function Complexity
- **Piecewise Linear Functions:**
  - With a single layer (one affine transformation followed by ReLU), the network defines a piecewise linear function—its graph is made of linear segments.
  - With multiple layers, the composition of affine transformations and nonlinearities creates a function that partitions the input space into many regions, each with its own linear behavior. In essence, the more layers (and neurons per layer) you have, the richer the set of piecewise linear regions and the more complex the decision boundaries.

## 3.3 A Typical Deep Network
A simple deep network might be defined as:

$$
\begin{aligned}
\mathbf{h}_1&=\phi(\mathbf{A}_1\mathbf{x}+\mathbf{b}_1), \\
\mathbf{h}_2&=\phi(\mathbf{A}_2\mathbf{h}_1+\mathbf{b}_2), \\
\mathbf{h}_3&=\phi(\mathbf{A}_3\mathbf{h}_2+\mathbf{b}_3), \\
\mathbf{y}&=\phi'(\mathbf{A}_4\mathbf{h}_3+\mathbf{b}_4).
\end{aligned}
$$

Here, note that $\phi'$ may be a different (or even the identity) activation on the final layer. The sizes of $\mathbf{A}_i$ and $\mathbf{b}_i$ determine the number of neurons and parameters.

# 4. Gradient Descent and Optimization

## 4.1 Full-Batch Gradient Descent
The standard optimization procedure involves:

1. **Initialization:** Start with an initial guess $\theta_0$.
2. **Iterative Updates:** At each iteration $i$:
   - Compute the gradient of the loss:
   $$\mathbf{g}=\nabla_\theta L(\theta_i).$$
   
   - Update the parameters:
   $$\theta_{i+1}=\theta_i-\lambda\,\mathbf{g},$$
   
   where $\lambda$ is the learning rate.
3. **Repeat:** Continue until convergence.

## 4.2 Stochastic Gradient Descent (SGD)
Rather than computing the gradient over the entire dataset (full-batch), SGD computes the gradient using a mini-batch—a randomly sampled subset of the training data. Benefits include:

- **Escaping Local Minima:** The noise from using subsets can help the optimizer avoid getting stuck.
- **Computational Efficiency:** Each update is cheaper because fewer data points are used.
- **Practical Convergence:** Although SGD "does not converge" in the traditional sense (due to noise), it tends to find solutions that generalize well.

## 4.3 Momentum and Adam
- **Momentum:** Rather than using only the current gradient, momentum methods keep a running average of past gradients. This can be thought of as a "prediction" of the next update direction. In Nesterov accelerated momentum, one makes a preliminary move in the predicted direction before evaluating the gradient.
- **Adam (Adaptive Moment Estimation):** Adam computes both a running average of gradients and of squared gradients, effectively normalizing updates. This makes the learning rate adaptive for each parameter and often improves performance in practice.

# 5. Computing Gradients: The Chain Rule, Jacobians, and Reverse Derivatives
A cornerstone of training deep networks is the ability to compute gradients efficiently. The following ideas are central:

## 5.1 Direct Gradient Computation
For a given loss function that depends on the network output, the gradient with respect to parameters is computed by applying the chain rule. For a single training example:

$$\frac{\partial \ell(f(\mathbf{x};\theta),\mathbf{y}^{\text{obs}})}{\partial \theta}=\frac{\partial \ell}{\partial f}\cdot\frac{\partial f}{\partial \theta}.$$

This is applied recursively over each layer.

## 5.2 The Jacobian and Its Size
- **Definition:** For a function $f:\mathbb{R}^N\to\mathbb{R}^O$, the Jacobian matrix $\mathbf{J}_f$ is an $O\times N$ matrix whose $(i,j)$ entry is $\frac{\partial f_i}{\partial x_j}$.
- **Multidimensional Inputs/Outputs:** When functions operate on tensors (e.g., images or feature maps), the Jacobian becomes a higher-dimensional object. In practice, it is often too large to compute or store explicitly.

## 5.3 Reverse Derivative (Vector-Jacobian Product, VJP)
Instead of computing the full Jacobian, deep learning frameworks compute the vector-Jacobian product (VJP). For a scalar loss $L$, one needs only the product:

$$\frac{dL}{d\mathbf{x}}=\mathbf{v}\cdot\mathbf{J}_f(\mathbf{x}),$$

where $\mathbf{v}$ (often the gradient from later layers) is of a size that makes the computation tractable. This "reverse-mode" differentiation is the backbone of backpropagation.

### Detailed Examples
- **Addition:** For $\text{add}(\mathbf{a},\mathbf{b})=\mathbf{a}+\mathbf{b}$, the derivative with respect to each input is the identity, so the VJP simply passes back the incoming gradient.
- **Matrix Multiplication:** For $\text{mul}(\mathbf{A},\mathbf{B})=\mathbf{A}\mathbf{B}$, the reverse derivatives are:
  - With respect to $\mathbf{A}$: Multiply the incoming gradient by $\mathbf{B}^\top$.
  - With respect to $\mathbf{B}$: Multiply $\mathbf{A}^\top$ by the incoming gradient.

Using these rules, one can "propagate" gradients backwards through any network architecture without ever forming the full Jacobian.

## 5.4 Deriving Gradients in a Network
For a network defined by a series of operations (e.g., affine transformations and nonlinearities), we apply the chain rule line by line. In a typical setup:

- **Forward Pass:** Compute intermediate quantities (e.g., $\mathbf{p}_1=\mu(\mathbf{A}_1,\mathbf{x},\mathbf{b}_1)$, then $\mathbf{h}_1=\phi(\mathbf{p}_1)$, etc.).
- **Backward Pass:** Starting from the loss, compute the gradient with respect to the network output, then use reverse derivatives (or VJP) to backpropagate through each operation. This involves transposing matrices where necessary and ensuring that the dimensions match.

# 6. Weight Initialization and Input Scaling

## 6.1 The Role of Scaling
When training deep networks, the magnitudes of the intermediate activations are crucial. For instance, if the inputs have a standard deviation $\sigma_x$ and an affine layer is applied,

$$\mathbf{p}=\mathbf{A}\mathbf{x}+\mathbf{b},$$
then (assuming zero biases and independent entries) the variance of the output roughly scales as

$$\sigma_p^2\approx N\,\sigma_A^2\sigma_x^2,$$

where $N$ is the number of inputs to the layer.

## 6.2 Weight Initialization: The He Initialization
For layers using ReLU activations, note that the ReLU "kills" half of the input (by zeroing out negatives). Thus, if the pre-activation has variance $\sigma_x^2$, then the output variance becomes approximately $\frac{1}{2}\sigma_x^2$. To maintain the signal through many layers, weights are often initialized so that

$$\sigma_A^2\approx \frac{2}{N}.$$

This "He initialization" (named after its proposer) helps keep the scale of activations roughly constant from layer to layer.

## 6.3 Softmax and the Log-Sum-Exp Trick
For classification, the softmax is sensitive to the scale of its inputs. In practice, one uses the logsoftmax formulation:

$$\log\text{softmax}(y_k)=y_k-\log\sum_{k'}\exp(y_{k'}).$$

To improve numerical stability, it is common to subtract the maximum value from the logits before applying the exponential, thereby avoiding overflow or underflow.

# 7. Batching, Broadcasting, and Computational Efficiency
- **Mini-Batching:** Running computations on a batch of samples simultaneously is key for efficiency on GPUs. If a function $f$ is defined for a single sample ($\mathbb{R}^I\to\mathbb{R}^K$), it is extended to a batch of $B$ samples as

$$f:\mathbb{R}^{I\times B}\to\mathbb{R}^{K\times B}.$$

- **Broadcasting:** Many operations (such as adding a bias vector) are automatically extended across the batch dimension. Libraries like NumPy, PyTorch, and JAX provide built-in broadcasting rules.
- **vmap Transform:** In frameworks like JAX, the vmap transformation automates the vectorization of functions over the batch dimension.

# 8. Regularization and Generalization
Deep networks often have far more parameters than training examples. To prevent overfitting and to improve generalization to unseen data, regularization techniques are employed.

## 8.1 Explicit Regularization
- **L2 Regularization (Weight Decay):** An extra term is added to the loss function to penalize large weights:

$$L_{\text{reg}}(\theta)=L(\theta)+\gamma\|\theta\|^2.$$

This encourages smoother models and discourages oscillatory behavior.

## 8.2 Implicit Regularization
- **Gradient Descent Effects:** The optimization process itself can act as a form of regularization. For example, using a finite learning rate or injecting noise (as in SGD) can prevent the model from fitting noise in the data.
- **Early Stopping:** By monitoring the performance on a validation set and stopping training early, one prevents the model from overfitting the training data.

### Other Techniques:
- **Dropout:** Randomly "dropping" units during training forces the network to build redundancy and avoid over-reliance on any single pathway.
- **Ensembling:** Combining multiple models (with different initializations or subsets of data) can improve generalization.
- **Data Augmentation:** Increasing the effective size of the training set by applying transformations (rotation, scaling, etc.) improves robustness.

## 8.3 The Lottery Ticket Hypothesis
An interesting empirical observation is that among large networks there often exist "winning tickets" (subnetworks) that perform well when trained in isolation. This idea motivates both the use of overparameterized models and strategies to identify and use effective subnetworks.

# 9. The "Boxes and Arrows" Computation Model
Deep learning frameworks typically represent a network as a directed acyclic graph (DAG) where:

- **Boxes:** Represent functions (e.g., affine transformations, nonlinearities, softmax).
- **Arrows:** Represent the flow of data (and later gradients) through the network.

The computation is split into:

- **Forward Pass:** Computes intermediate values (activations) and the final output.
- **Backward Pass (Backpropagation):** Computes gradients by propagating derivatives backwards using the chain rule and VJP methods.

This view underpins modern automatic differentiation libraries.

# 10. Summary and Further Directions
- **Training Process:** Deep learning models are trained using variants of gradient descent. Backpropagation is made efficient by using reverse-mode differentiation (the vector-Jacobian product) rather than computing full Jacobians.
- **Architecture Design:** Layer composition (affine transformations plus nonlinear activations) yields powerful models capable of approximating complex functions.
- **Optimization Variants:** From full-batch gradient descent to SGD with momentum and adaptive methods like Adam, many strategies are used to improve convergence and escape local minima.
- **Initialization and Scaling:** Proper weight initialization (such as He initialization) and input scaling are crucial to maintain numerical stability and prevent issues like exploding or vanishing gradients.
- **Regularization:** Both explicit methods (L2 regularization, dropout) and implicit effects (noise from SGD, early stopping) are key to achieving models that generalize well.
- **Additional Topics:** Future lectures might expand on techniques such as normalization (e.g., batch norm), advanced architectures, dropout variants, and further discussions on the lottery ticket hypothesis.