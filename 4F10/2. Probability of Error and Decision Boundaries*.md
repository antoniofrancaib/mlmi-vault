Lectures by Jose Miguel Hernandez Lobato

Topics: 

# 1. Introduction & Motivation
In many classification tasks—such as vowel recognition using formant features or distinguishing between oranges and lemons—we face **uncertainty**. Even with well‐designed classifiers, *no decision is "perfect" because observations do not always clearly separate classes*. Our goal is to design decision processes that minimize error, and these notes explain how loss functions, probability estimates, and decision boundaries come together to yield optimal (or near-optimal) classification rules.

![[peterson-data.png]]
# 2. Supervised Training and Evaluation Data

**Dataset Structure:** We assume a supervised dataset

$$
D = \{ \{\mathbf{x}_1, y_1\}, \dots, \{\mathbf{x}_N, y_N\} \}
$$

where each $\mathbf{x}_i$ is a feature vector and $y_i$ is the corresponding class label drawn from the set $\{\omega_1, \dots, \omega_K\}$.

**Data Generation:** The samples are considered draws from the joint distribution $p(\omega, \mathbf{x})$. In practice, we first obtain features $\mathbf{x}$ (drawn from $p(\mathbf{x})$) and then labels $y$ according to $P(\omega \mid \mathbf{x})$.

**Training vs. Evaluation:** Since our primary interest is in performance on unseen data, the dataset is split into:

- **Training Data**: Used to estimate model parameters.
- **Evaluation (Held-Out) Data**: Used to assess the classifier's performance and approximate its true error on new samples.

# 3. Expected Loss and Empirical Loss
## Expected Loss ($L_{\text{act}}$)

**Definition**: For any decision $f(\mathbf{x}, \theta)$ made by the classifier, there is an associated loss. If we *integrate this loss over the feature space*, weighting by both the posterior probabilities $P(\omega_i \mid \mathbf{x})$ and the density $p(\mathbf{x})$, we obtain the expected (or "actual") loss:

$$
L_{\text{act}} = \int \sum_{i=1}^K L(f(\mathbf{x}, \theta), \omega_i) P(\omega_i \mid \mathbf{x}) \, p(\mathbf{x}) \, d\mathbf{x}.
$$

**Interpretation**: $L_{\text{act}}$ represents the average loss over the entire data distribution. In the common case of 0–1 loss (i.e., a loss of 1 for an error and 0 for a correct classification), minimizing $L_{\text{act}}$ is equivalent to minimizing the probability of error.

## Empirical Loss ($L_{\text{emp}}$)

**Definition**: On a finite dataset of $N$ samples, the empirical loss is defined as:

$$
L_{\text{emp}} = \frac{1}{N} \sum_{i=1}^N L(f(\mathbf{x}_i, \theta), y_i).
$$

**Convergence**: As $N \to \infty$, the empirical loss converges to the true expected loss, making held-out evaluation a *reliable proxy for performance*. In reality, the finite case yields $L_{\text{act}} \geq L_{\text{emp}}$. 

# 4. Bayes' Decision Rule
## The Rule Under 0–1 Loss

**Loss Function**: With a 0–1 loss function:

$$
L(\hat{\omega}, \omega_i) = \begin{cases} 
0, & \hat{\omega} = \omega_i \\
1, & \text{otherwise}
\end{cases}
$$

**Optimal Decision**: Bayes' decision rule aims to minimize the expected loss. For an observation $\mathbf{x}^\star$, the optimal decision is:

$$
\hat{\omega} = \arg \min_\omega \sum_{i=1}^K L(\omega, \omega_i) P(\omega_i \mid \mathbf{x}^\star).
$$

Because of the 0–1 loss, this simplifies to selecting the class with highest posterior:

$$
\hat{\omega} = \arg \max_\omega \, P(\omega \mid \mathbf{x}^\star).
$$

**Model Approximation**: In practice, the posterior $P(\omega \mid \mathbf{x}^\star)$ is not known exactly. Instead, we train a model with parameters $\theta$ to approximate the posterior:

$$
f(\mathbf{x}^\star, \theta) = \arg \max_\omega \, P(\omega \mid \mathbf{x}^\star, \theta).
$$

# 5. Binary Classification
## Two-Class Case

**Class Labels**: For binary classification, we often label classes as $w_1 = 1$ and $w_2 = -1$.

**Empirical Error Rate**: With the 0–1 loss, the probability of error on a held-out set can be estimated as:

$$
P(\text{error}) \approx L_{\text{eval}} = \frac{1}{N^\star} \sum_{i=1}^{N^\star} L(f(\mathbf{x}_i^\star, \theta), y_i^\star)
$$

or, equivalently, if we encode the loss in terms of absolute differences:

$$
P(\text{error}) \approx \frac{1}{2N^\star} \sum_{i=1}^{N^\star} |f(\mathbf{x}_i^\star, \theta) - y_i^\star|.
$$

**True Probability of Error**: If we know the true data distribution, the error can be expressed by integrating over the decision regions. Denote by $\Omega_1$ the region where we predict $\omega_1$ and by $\Omega_2$ the region for $\omega_2$. Then:

$$
P(\text{error}) = P(\omega_1, \mathbf{x} \in \Omega_2) + P(\omega_2, \mathbf{x} \in \Omega_1)
$$

which can also be written as:

$$
P(\text{error}) = \int_{\Omega_2} p(\mathbf{x} \mid \omega_1) P(\omega_1) \, d\mathbf{x} + \int_{\Omega_1} p(\mathbf{x} \mid \omega_2) P(\omega_2) \, d\mathbf{x}.
$$

![[prob-error.png]]
Note that, by Bayes’ theorem:

$$P(\omega_i \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \omega_i) P(\omega_i)}{p(\mathbf{x})}$$

Since $p(\mathbf{x}) > 0$, this is equivalent to comparing  (y axis):

$$
p(\mathbf{x} \mid \omega_1) P(\omega_1) \quad \text{vs} \quad p(\mathbf{x} \mid \omega_2) P(\omega_2)
$$

Choose the class with highest posterior probability:

Decide $\omega_1$ if $P(\omega_1 \mid \mathbf{x}) > P(\omega_2 \mid \mathbf{x})$, else $\omega_2$

# 6. Visualizing Error and Decision Boundaries

## Decision Boundaries
![[oranges-lemons.png]]

**Definition**: The decision boundary is the (hyper)surface in feature space where the classifier is indifferent between classes—that is, where the estimated posterior probabilities are equal.

**For Binary Classification with Equal Class Priors**:
If the classes have equal probabilities and the *same cost of error*, the decision boundary is the set of points $\mathbf{x}$ where:

$$
\log P(\omega_1 \mid \mathbf{x}, \theta) = \log P(\omega_2 \mid \mathbf{x}, \theta).
$$

**Multivariate Gaussian Case**:
For generative classifiers assuming multivariate Gaussian class-conditional densities, the decision boundary is given by an equation of the form:

$$
\mathbf{x}^T A \mathbf{x} + \mathbf{b}^T \mathbf{x} + c = 0,
$$

where the matrices and vectors $A$, $\mathbf{b}$, and $c$ depend on the means, covariances, and class priors. In the special case where the covariance matrices for the two classes are identical ($\Sigma_1 = \Sigma_2 = \Sigma$), the quadratic term cancels and the boundary becomes linear:

$$
\mathbf{b}^T \mathbf{x} + c = 0.
$$

# 7. Classifier Types: Generative vs. Discriminative
## Generative Models

**Approach**: These models estimate the joint probability distribution $p(\mathbf{x}, \omega \mid \theta)$. Once the joint distribution is known, the class posterior is computed via Bayes' rule:

$$
P(\omega_i \mid \mathbf{x}^\star, \theta) = \frac{p(\mathbf{x}^\star \mid \omega_i, \theta) P(\omega_i)}{\sum_{j=1}^K p(\mathbf{x}^\star \mid \omega_j, \theta) P(\omega_j)}.
$$

**Parameter Estimation**: For each class, parameters $\theta_i$ (such as the mean $\mu_i$ and covariance $\Sigma_i$ in a Gaussian model) are estimated from the subset of training data with $y = \omega_i$. Maximize the joint likelihood of the data and labels:
$$
\hat{\theta} = \arg\max_{\theta} \sum_{i=1}^{N} \log p(\mathbf{x}_i \mid y_i, \theta) + \log P(y_i) = \arg\max_{\theta} \sum_{i=1}^{N} \log p(y_i, \mathbf{x}_i \mid \theta)
$$

**Probability of error**: We model how the data is generated given each class: $p(\mathbf{x} \mid \omega)$, then combine it with the prior $P(\omega)$ wuch that $p(\omega, \mathbf{x}) = p(\mathbf{x} \mid \omega) P(\omega)$. Therefore

$$
P(\text{error}) = \int_{\Omega_2} p(\mathbf{x} \mid \omega_1) P(\omega_1) \, d\mathbf{x} + \int_{\Omega_1} p(\mathbf{x} \mid \omega_2) P(\omega_2) \, d\mathbf{x}
$$

## Discriminative Models

**Approach**: These models directly estimate the posterior probability $P(\omega \mid \mathbf{x}, \theta)$ without assuming a generative process for the features.

**Parameter Estimation**:: The model parameters are determined by maximizing the conditional likelihood of the training labels given the features:

$$
\hat{\theta} = \arg \max_\theta \sum_{i=1}^N \log P(y_i \mid \mathbf{x}_i, \theta).
$$
**Probability of error**: We directly model the posterior probability $P(\omega \mid \mathbf{x})$, and combine it with the input distribution $p(\mathbf{x})$ This time, factor the joint in the opposite direction: $p(\omega, \mathbf{x}) = P(\omega \mid \mathbf{x}) p(\mathbf{x})$. Thus we write:

$$
P(\text{error}) = \int_{\Omega_2} P(\omega_1 \mid \mathbf{x}) p(\mathbf{x}) \, d\mathbf{x} + \int_{\Omega_1} P(\omega_2 \mid \mathbf{x}) p(\mathbf{x}) \, d\mathbf{x}
$$

### Discriminant Functions:
Alternatively, one might train a function that maps $\mathbf{x}$ directly to a class label without explicitly estimating the posterior probability.

# 8. Unequal Loss Functions and ROC Analysis
## Unequal Loss Functions

**Motivation**: In many applications, the cost of misclassification is not symmetric. For example, in speaker verification for bank access, falsely granting access (false positive) can be much more costly than a false rejection.

**Modified Decision Rule**: With unequal loss, suppose the losses are defined as:

$$
L(f(\mathbf{x}^\star, \theta), \omega_1) = \begin{cases} 
0, & \text{if } f(\mathbf{x}^\star, \theta) = \omega_1 \\
C_{21}, & \text{if } f(\mathbf{x}^\star, \theta) = \omega_2
\end{cases}
$$

and similarly for class $\omega_2$. Then Bayes' rule adapts to:

$$
\hat{\omega} = \arg \min \{ C_{12} P(\omega_2 \mid \mathbf{x}^\star, \theta), \; C_{21} P(\omega_1 \mid \mathbf{x}^\star, \theta) \}.
$$

The ratio $C_{12} / C_{21}$ acts as an operating threshold.

## Receiver Operating Characteristic (ROC) Curve

This decision rule can be expressed as classifying $\mathbf{x}$ using:

$$
C_{21} P(\omega_1 \mid \mathbf{x}, \theta) > C_{12} P(\omega_2 \mid \mathbf{x}, \theta)
$$

or equivalently:
$$
\omega_1 \quad \text{if} \quad \frac{P(\omega_1 \mid \mathbf{x}, \theta)}{P(\omega_2 \mid \mathbf{x}, \theta)} > \frac{C_{12}}{C_{21}}, \quad \text{else} \quad \omega_2
$$

- The “cost” ratio $C_{12}/C_{21}$ is effectively an operating threshold  
- Possible to produce curves by changing this threshold  
- $\omega_1 = \text{positive}$, $\omega_2 = \text{negative}$, equal priors $P(\omega_1) = P(\omega_2)$

**Definition**: A ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 − specificity) for different threshold settings.

**Interpretation**: By adjusting the operating threshold (the cost ratio), one can trace out a curve that visualizes the trade-off between detecting true positives and avoiding false positives.

**Application**: ROC analysis is a powerful tool for comparing classifiers, especially when costs are unequal.

![[roc.png]]
# 9. Training the Decision Process
## Learning the Model Parameters
### Supervised Training:
Given the training set $D = \{ (\mathbf{x}_i, y_i) \}_{i=1}^N$, we estimate the parameters $\theta$ of our chosen model.

### Discriminative Training:
The parameters are estimated by maximizing the conditional likelihood:

$$
\hat{\theta} = \arg \max_\theta \log P(\mathbf{y} \mid \mathbf{X}, \theta). = \arg \max_\theta \sum_{i=1}^N \log P(y_i \mid \mathbf{x}_i, \theta).
$$

A key observation is that on the decision boundary (for binary tasks), the log-posteriors for both classes are equal.

### Generative Training:
For generative models, we estimate the class-conditional densities $p(\mathbf{x} \mid \omega_i, \theta_i)$ for each class and the class priors $P(\omega_i)$.  We aim to maximize this posterior 
$$
P(\omega_i \mid \mathbf{x}) \approx \frac{p(\mathbf{x}, \omega_i \mid \theta)}{\sum_{j=1}^{K} p(\mathbf{x}, \omega_j \mid \theta)} = \frac{p(\mathbf{x} \mid \omega_i, \theta) P(\omega_i)}{\sum_{j=1}^{K} p(\mathbf{x} \mid \omega_j, \theta) P(\omega_j)}
$$

- $P(\omega_i)$ is the prior for class $\omega_i$  
- $p(\mathbf{x} \mid \omega_i, \theta)$ is the likelihood of the observation given class $\omega_i$  
- Use **maximum likelihood (ML)** training to estimate $\theta$  
- A separate model is trained for each class $\omega_i$, $\theta_i$

$$
\theta_i = \arg\max_{\theta} \sum_{j \,:\, y_j = \omega_i} \log p(\mathbf{x}_j \mid \omega_i, \theta)
$$
For example, when using multivariate Gaussians, the mean is estimated as:

$$
\hat{\mu}_i = \frac{\sum_{j: y_j = \omega_i} \mathbf{x}_j}{\sum_{j: y_j = \omega_i} 1}.
$$

The covariance matrix is estimated as:

$$
\hat{\Sigma}_i = \frac{\sum_{j: y_j = \omega_i} (\mathbf{x}_j - \hat{\mu}_i) (\mathbf{x}_j - \hat{\mu}_i)^T}{\sum_{j: y_j = \omega_i} 1}.
$$

## Class Prior Estimation
### Counting:
The simplest approach is to estimate the prior by counting the frequency of each class in the training data:

$$
P(\omega_i) \approx \frac{1}{N} \sum_{j: y_j = \omega_i} 1.
$$

### Smoothing:
For small $N$ or to avoid zero counts, add-one smoothing (or other techniques) can be used:

$$
P(\omega_i) \approx \frac{1 + \sum_{j: y_j = \omega_i} 1}{N + K}.
$$

# 10. Multivariate Gaussian Class Conditional PDFs and Decision Boundaries
## Multivariate Gaussian PDFs

Formulation: When modeling the feature distributions for each class, a common choice is the multivariate Gaussian:

$$
p(\mathbf{x} \mid \omega_i, \theta_i) = \mathcal{N}(\mathbf{x}; \mu_i, \Sigma_i) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i) \right),
$$

where $d$ is the dimensionality of the observation vector.

## Deriving the Decision Boundary

General Form: Using Bayes' rule for a generative classifier, the decision boundary (for a two-class problem) is found by setting:

$$\log(P(\omega_1 \mid \mathbf{x}, \theta)) = log(P(\omega_2 \mid \mathbf{x}, \theta))$$
The denominators cancel out and thus

$$
\log(P(\omega_1) p(\mathbf{x} \mid \omega_1, \theta_1)) = \log(P(\omega_2) p(\mathbf{x} \mid \omega_2, \theta_2)).
$$

After substituting the Gaussian densities and simplifying, this typically leads to a quadratic form:

$$
\mathbf{x}^T A \mathbf{x} + \mathbf{b}^T \mathbf{x} + c = 0,
$$

where:

- $A = \Sigma_1^{-1} - \Sigma_2^{-1}$,
- $\mathbf{b} = 2(\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1)$,
- $c$ comprises terms involving $\mu_i$, $\Sigma_i$, and the priors $P(\omega_i)$.

**Linear Case**: When the covariance matrices are assumed equal ($\Sigma_1 = \Sigma_2 = \Sigma$), the quadratic term cancels (i.e., $A = 0$) and the boundary becomes linear:

$$
\mathbf{b}^T \mathbf{x} + c = 0.
$$

This is a hyperplane in the feature space.

![[dhs-boundary.png]]

# 11. Evaluating Classifier Performance
## Optimality and Practical Considerations

**Bayes' Optimality**: Bayes' decision rule is optimal in the sense that it minimizes the expected loss—provided that the class posterior probabilities are accurately estimated.

**Real-World Challenges**: In practice, several factors can lead to deviations from optimal performance:

- **Limited Training Data**: Poor estimates of the true distribution.
- **Model Mismatch**: The assumed model (e.g., Gaussian) may not accurately capture the true underlying distribution.
- **Optimization Issues**: The training process may not always find the global optimum.

### Engineering Trade-Offs:
The design of the classifier involves many decisions, including the choice between generative and discriminative models, the form of the loss function, and techniques for parameter estimation. Each choice affects the probability of error and overall robustness.

# 12. Summary
These notes have taken you through the foundational concepts necessary for understanding and designing classifiers with a focus on probability of error and decision boundaries:

1. **Data and Loss**: We began by detailing how supervised data is collected and how expected and empirical losses are defined.
2. **Bayes' Rule**: We explained how the optimal decision rule minimizes the error by selecting the class with the highest posterior probability.
3. **Binary and Multi-Class Cases**: Special attention was given to the binary case, including practical ways to estimate error rates.
4. **Classifier Types**: The distinctions between generative and discriminative models were discussed, highlighting their respective estimation procedures.
5. **Gaussian Models and Boundaries**: A common approach using multivariate Gaussian distributions was detailed, leading to a derivation of quadratic (or linear, with equal covariances) decision boundaries.
6. **Practical Evaluation**: Finally, we examined how real-world challenges can affect classifier performance and why careful design and validation are critical.
