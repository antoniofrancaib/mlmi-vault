# 1. Introduction and Motivation
Many traditional kernel methods—such as those based on linear, polynomial, or radial basis functions—assume that input data are fixed-length vectors. However, in numerous real-world applications, data naturally come in structured forms such as biological sequences, molecular graphs, text documents, and parse trees. The central challenge addressed here is: How can we apply kernel methods when the input objects are not naturally represented as fixed-dimensional vectors?

The general solution is twofold:

1. **Feature Extraction**: Map the structured data to a feature space by extracting relevant characteristics from each object.
2. **Kernel Computation**: Compute inner products (dot products) in this new feature space using the extracted features.

This two-step process allows one to apply the familiar machinery of kernel-based machine learning techniques even when dealing with non-vectorial, structured data.

# 2. Kernels for Strings
Structured data in the form of strings (sequences of symbols or characters) appear in many contexts—ranging from DNA sequences to natural language text. Two main ideas are illustrated below:

## 2.1 The General String Kernel
**Feature Mapping**:

Every data point $\mathbf{x}$ is a string over an alphabet $A$, so $\mathbf{x} \in A^*$ (where $A^*$ denotes the set of all finite-length strings).

One defines a potentially very high-dimensional feature vector

$$
\mathbf{\phi}(\mathbf{x}) = (\phi_{s_1}(\mathbf{x}), \phi_{s_2}(\mathbf{x}), \phi_{s_3}(\mathbf{x}), \dots)^T,
$$

where each coordinate $\phi_s(\mathbf{x})$ corresponds to a substring $s$ in some (possibly infinite) list of substrings.

**Feature Values**:

$\phi_s(\mathbf{x})$ is defined such that it is zero if the substring $s$ does not occur in $\mathbf{x}$, and positive (possibly greater than one) if it does occur. In many formulations, it is simply the count of how many times $s$ appears in $\mathbf{x}$.

**Kernel Definition**:

With the above representation, the kernel between two strings $\mathbf{x}$ and $\mathbf{x'}$ is defined as their dot product:

$$
k(\mathbf{x}, \mathbf{x'}) = \mathbf{\phi}(\mathbf{x})^T \mathbf{\phi}(\mathbf{x'}) = \sum_s \phi_s(\mathbf{x}) \phi_s(\mathbf{x'}).
$$

This formulation makes it possible to quantify similarity by comparing the count or weighted occurrence of substrings.

## 2.2 Variations of the String Kernel
### Gap-Weighted Kernel
**Concept**:
In many practical settings, it is useful to consider substrings that are not necessarily contiguous. For example, one may wish to allow gaps in the substring occurrences in the string $\mathbf{x}$.

**Weighting by Gaps**:
If a substring occurs in $\mathbf{x}$ with $n$ gaps (i.e., the characters of the substring are not contiguous), then the feature value can be taken as $\lambda^n$ where $0 < \lambda < 1$. This weighting penalizes occurrences with more gaps, giving higher importance to more contiguous (and often semantically tighter) occurrences.

**Efficient Computation**:
Although the space of possible substrings is huge, dynamic programming techniques can be used to compute these features (and hence the kernel) efficiently.

### k-Spectrum Kernel
**Idea**:
Instead of considering all substrings, one can focus on those substrings of a fixed length $k$. The "k-spectrum" kernel considers all subsequences (or substrings) of length $k$.

**Feature Representation**:
Here, $\phi_s(\mathbf{x})$ is defined as the count of how many times the substring $s$ of length $k$ appears exactly in $\mathbf{x}$.

**Efficient Counting**:
Tools such as suffix trees can be exploited to store and rapidly count all the length-$k$ substrings in a string. This is particularly useful in domains like DNA sequence classification, where the choice of $k$ must be balanced:

- If $k$ is large, the co-occurrence of long substrings provides more specific information.
- If $k$ is too large, these occurrences become too sparse.

**Special Case – Bag-of-Words**:
When $k=1$, the kernel essentially becomes a bag-of-words kernel, often used in document classification.

# 3. Kernels for Graphs
Many structured data sets naturally have a graph structure. For example, molecules can be represented as graphs with nodes (atoms) and edges (bonds).

## 3.1 Representing Graphs
**Graph $G$**:
A graph $G = (V, E)$ is given by a set of nodes $V$ and a set of edges $E \subseteq V \times V$.

**Adjacency Matrix**:
One common representation is the adjacency matrix $\mathbf{A}$, where $A_{ij} = 1$ if there is an edge from node $i$ to node $j$, and $0$ otherwise.

## 3.2 Graph Walks and Counting
**Walks in Graphs**:
A k-length walk is defined as a sequence of nodes $\mathbf{w} = \{v_1, v_2, \dots, v_{k+1}\}$ such that there is an edge from $v_i$ to $v_{i+1}$ for $i=1,\dots,k$.

**Matrix Powers and Walks**:
The number of k-length walks from node $i$ to node $j$ can be computed using the $(i,j)$ entry of the matrix $\mathbf{A}^k$. For example,

$$
[\mathbf{A}^2]_{ij} = \sum_{s=1}^{|V|} A_{i,s} A_{s,j},
$$

and similarly for higher powers.

## 3.3 Random-Walk Graph Kernel
**Feature Mapping with Walks**:
One way to define a kernel for graphs is by mapping a graph $G$ into a feature vector $\mathbf{\phi}(G)$ where the $i$-th component counts the number of walks of length $i$ in $G$.

**Kernel Computation**:
The random-walk graph kernel between two graphs $G$ and $G'$ is obtained by counting common walks. One method to achieve this is to construct the direct product graph $G_\times$, defined as follows:

- **Node Set**: $V_\times = \{(a, b) : a \in V, b \in V'\}$.
- **Edge Set**: $E_\times = \{((a, a'), (b, b')) : (a, b) \in E \text{ and } (a', b') \in E'\}$.

**Kronecker Product and Convergence**:
The adjacency matrix $\mathbf{A}_\times$ for $G_\times$ can be written as the Kronecker product $\mathbf{A} \otimes \mathbf{A'}$. With a small positive scalar $\lambda$ (which ensures convergence), one defines the kernel as:

$$
k(G, G') = \mathbf{1}^T \left( \sum_{n=0}^\infty \lambda^n \mathbf{A}_\times^n \right) \mathbf{1}.
$$

This series converges under appropriate conditions and can be thought of as summing the contributions from walks of all lengths weighted by $\lambda^n$.

**Efficient Computation**:
In practice, this infinite series can be computed using an iterative method:

$$
\mathbf{x}^{(t+1)} = \mathbf{1} + \lambda (\mathbf{A} \otimes \mathbf{A'}) \mathbf{x}^{(t)},
$$

where vectorization operations (using $\text{vec}$ and its inverse) are exploited.

**Limitations**:

- **Cycle Repetition**: Walks can repeatedly visit cycles, so even small structural similarities might lead to very large kernel values.
- **Computational Cost**: The process generally incurs a cost of $O(n^3)$ for $n \times n$ matrices, making it computationally expensive for large graphs.

## 3.4 Weisfeiler-Lehman Graph Kernel
The Weisfeiler-Lehman (WL) graph kernel addresses some of the limitations of the random-walk kernel. Its key features include:

**Incorporation of Node Labels**:
It explicitly takes into account node labels, which is particularly useful when the nodes have meaningful categorical attributes.

**Iterative Relabeling Process**:

1. **Iteration Process**: For each graph and for $M$ iterations:
   - **Neighborhood Collection**: For every node, create a set of labels consisting of the labels of its adjacent nodes.
   - **Label Enrichment**: Sort the collected set and append (or "prefix") the original node label to this set. This creates a combined label that reflects both the node and its immediate neighborhood.
   - **Compression**: Convert (compress) this enriched label into a unique value. This relabeling process effectively captures structural patterns.

**Final Kernel Computation**:
After performing the iterations, the kernel is computed by applying a bag-of-words approach to the collection of new vertex labels across all iterations (including the original labels).

**Advantages**:
The WL kernel typically has lower computational cost compared to random-walk kernels and often yields better performance, especially in large graph settings.

# 4. Fisher Kernel
The Fisher kernel takes a very different approach by leveraging a probabilistic generative model to represent complex structured data in a fixed-dimensional space.

## 4.1 Overview of the Fisher Kernel
**Generative Model Training**:
First, a generative model $p(\mathbf{x} \mid \mathbf{\theta})$ is trained on the data $\{\mathbf{x}_1, \dots, \mathbf{x}_N\}$ (usually by maximum likelihood estimation to obtain $\mathbf{\theta}_{\text{MLE}}$).

**Fisher Score Vector**:
For each data point $\mathbf{x}_n$, one computes the Fisher score vector:

$$
\mathbf{\phi}(\mathbf{x}_n) = \nabla_\mathbf{\theta} \log p(\mathbf{x}_n \mid \mathbf{\theta}) \big|_{\mathbf{\theta} = \mathbf{\theta}_{\text{MLE}}}.
$$

This gradient essentially captures the sensitivity of the log-likelihood with respect to the model parameters and quantifies how $\mathbf{x}_n$ "pulls" on the parameters.

**Fisher Kernel Definition**:
The kernel is then obtained as the dot product between these score vectors:

$$
k(\mathbf{x}_n, \mathbf{x}_m) = \mathbf{\phi}(\mathbf{x}_n)^T \mathbf{\phi}(\mathbf{x}_m).
$$

## 4.2 An Example with Mixture Models
Consider a mixture model defined as:

$$
p(\mathbf{x} \mid \mathbf{\theta}) = \sum_{k=1}^K \pi_k p(\mathbf{x} \mid \mathbf{\theta}_k),
$$

where $\pi_k$ is the weight and $p(\mathbf{x} \mid \mathbf{\theta}_k)$ is the component density.

**Component-Wise Fisher Score**:
For the mixture weight $\pi_k$, the corresponding component of the Fisher score for $\mathbf{x}$ is:

$$
[\mathbf{\phi}(\mathbf{x})]_{\pi_k} = \frac{\partial}{\partial \pi_k} \log p(\mathbf{x} \mid \mathbf{\theta}) \big|_{\mathbf{\theta} = \mathbf{\theta}_{\text{MLE}}}.
$$

For instance, one might obtain:

$$
[\mathbf{\phi}(\mathbf{x})]_{\pi_k} = \frac{p(\mathbf{x} \mid \mathbf{\theta}_k^{\text{MLE}})}{\sum_j \pi_j^{\text{MLE}} p(\mathbf{x} \mid \mathbf{\theta}_j^{\text{MLE}})},
$$

which directly measures the contribution of the $k$-th mixture component in generating $\mathbf{x}$.

**Intuition**:
In effect, the Fisher kernel partitions the input space into regions according to the relative contribution of each model component. Data points that invoke similar gradients (i.e., similar "pulls" on the parameters) are considered similar under this kernel.

# 5. Summary and Key Takeaways
**General Framework**:
Kernels for structured data generally follow a two-step procedure: first extract a feature representation from structured inputs (strings, graphs, molecules, etc.) and then compute similarities as dot products in this feature space.

**String Kernels**:

- **Gap-weighted kernel**: Accounts for non-contiguous occurrences with gap-dependent penalties.
- **k-spectrum kernel**: Focuses on exact occurrences of fixed-length substrings, with the bag-of-words kernel as a special case when $k=1$.

**Graph Kernels**:

- **Random-Walk Kernel**: Counts walks via the Kronecker product of adjacency matrices. While conceptually elegant, it can be computationally heavy and may overemphasize repeated cycles.
- **Weisfeiler-Lehman Kernel**: Uses an iterative relabeling process to capture local structure efficiently and is well suited for graphs with node labels.

**Fisher Kernel**:
Utilizes a generative probabilistic model to compute a fixed-length Fisher score vector for each data point. This method bridges generative and discriminative approaches by measuring how data points influence parameter estimates.

These methods provide powerful tools to handle a variety of structured data scenarios. Understanding the details behind dynamic programming for efficient substring counting, matrix-power approaches in graph kernels, or gradient-based representations in the Fisher kernel allows practitioners to make informed choices when designing learning systems for complex data.