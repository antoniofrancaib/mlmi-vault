Lectures by Andrew Fitzgibbon

Topics: 

# 1. Language Modelling and Sequence Tasks
## 1.1. Language Modelling Fundamentals
**Task Example:**  
For the sentence "The cat sat on the ____", a language model predicts the next word by assigning probabilities to tokens:

$$P(w_{i+1} \mid w_1, \ldots, w_i)$$

**Notational Setup:**
- Tokens $w_i \in V$ (the vocabulary)
- The observed sequence $w_1, \ldots, w_i$ forms the context
- The goal is to model the conditional probability over the vocabulary

## 1.2. Sequence Modelling Tasks
**General Formulation:**  
The task is often written as:

$$y = f(x_1, \ldots, x_T; \theta)$$

where:
- $x_t \in \mathbb{R}^D$ (the input at time $t$)
- $y$ may be a scalar, vector, or matrix (depending on the task)
- $T$ is the sequence length, and the function $f$ must gracefully handle arbitrary $T$.

**Task Variants:**
- **Sequence-to-Answer:**  
  For example, "predict the next word" or a binary decision such as "video is fake."
- **Sequence-to-Sequence:**  
  Examples include translation ("Je viens d'Irlande" → "I am from Ireland") or tagging parts of speech.

# 2. Representing Words: From Tokens to Vectors
## 2.1. Tokenization and One-Hot Encoding
**Key Point:**  
Words are not naturally vectors. In computer models, words are first tokenized into integers.

**One-Hot Encoding:**  
A word becomes a one-hot vector of size $|V|$ (vocabulary size). However, while one-hot encoding is conceptually simple, it is inefficient in practice.

## 2.2. Embedding Matrix
**Embedding Transformation:**  
Instead of using one-hot vectors directly, we use an embedding matrix $\mathbf{A}_0 \in \mathbb{R}^{|V| \times D}$ that maps each token to a $D$-dimensional dense vector.

An input sentence $\mathbf{W} = w_1, \ldots, w_T$ is first one-hot encoded to form a $|V| \times T$ matrix.

Multiplying this by $\mathbf{A}_0$ yields a $D \times T$ representation.

**Training Embeddings:**  
The embedding matrix is learned in the same way as other model parameters. Gradients propagate through the indexing operation so that representations improve during training.

# 3. Recurrent Neural Networks (RNNs)
## 3.1. Basic RNN for Sequence Modelling
**Recurrent Dynamics:**  
A simple recurrent network processes an input sequence by updating a hidden state $\mathbf{h}_t$ at every time step:

$$
\begin{aligned}
\mathbf{h}_1 &= \phi(\mathbf{A}\mathbf{x}_1 + \mathbf{b}) \\
\mathbf{h}_2 &= \phi(\mathbf{A}\mathbf{x}_2 + \mathbf{b} + \mathbf{C}\mathbf{h}_1) \\
&\vdots \\
\mathbf{h}_T &= \phi(\mathbf{A}\mathbf{x}_T + \mathbf{b} + \mathbf{C}\mathbf{h}_{T-1}) \\
\mathbf{y}_T &= \mathbf{D}\mathbf{h}_T + \mathbf{e}
\end{aligned}
$$

Here, $\mathbf{A}$, $\mathbf{b}$, $\mathbf{C}$, $\mathbf{D}$, and $\mathbf{e}$ are parameters, and $\phi$ is a nonlinear activation.

**Interpretation:**  
Each hidden state $\mathbf{h}_t$ captures both the current input $\mathbf{x}_t$ and information from the past via $\mathbf{C}\mathbf{h}_{t-1}$. For sequence-to-sequence tasks, an output $\mathbf{y}_t$ is produced at each step:

$$\mathbf{y}_t = f_D(\mathbf{h}_t) = \mathbf{D}\mathbf{h}_t + \mathbf{e}$$

The function $f$ now produces a sequence $\mathbf{y}_1, \ldots, \mathbf{y}_T$.

## 3.2. Residual Connections in RNNs
**Motivation:**  
To ease the learning of deep networks, residual (or skip) connections add the previous hidden state (or other signals) to the computation:

$$\mathbf{h}_t = \mathbf{Q}\mathbf{h}_{t-1} + \phi(\mathbf{A}\mathbf{x}_t + \mathbf{b} + \mathbf{C}\mathbf{h}_{t-1})$$

This "shortcut" helps with gradient flow and mitigates issues like vanishing gradients.

**Variations:**  
One can even use multiple previous states (or the previous output $\mathbf{y}_t$) as input to the recurrent unit.

## 3.3. Recurrent Unit Abstraction
**General Unit:**  
The core idea is to encapsulate the recurrence in a function $R$:

$$(\mathbf{h}_{t+1}, \mathbf{y}_{t+1}) = R(\mathbf{h}_t, \mathbf{x}_{t+1}; \text{parameters})$$

This abstraction allows various parameterizations, such as different weight matrices (which may have special structures like diagonal or scaled identity).

# 4. Advanced RNN Architectures: Gating and Memory
## 4.1. Gated Recurrent Units (GRU)
**Gating Mechanism:**  
GRUs introduce gates that control the information flow. For example:

$$
\begin{aligned}
\mathbf{a}_f &= \sigma(\mathbf{W}_{xf}\mathbf{x} + \mathbf{b}_{hf} + \mathbf{W}_{hf}\mathbf{h}_{\text{prev}}) \\
\tilde{\mathbf{h}} &= \mathbf{a}_f * \mathbf{h}_{\text{prev}} \\
\tilde{\mathbf{h}}' &= \phi(\mathbf{W}_x\mathbf{x} + \mathbf{W}_h\tilde{\mathbf{h}} + \mathbf{b}_h) \\
\mathbf{a}_o &= \sigma(\mathbf{W}_{xo}\mathbf{x} + \mathbf{b}_{ho} + \mathbf{W}_{ho}\mathbf{h}_{\text{prev}}) \\
\mathbf{h}' &= \text{gate}(\mathbf{a}_o, \mathbf{h}_{\text{prev}}, \tilde{\mathbf{h}}')
\end{aligned}
$$

**Explanation:** The gates (using sigmoid $\sigma$) decide how much of the previous state to keep or update. The "gate" function blends information from $\mathbf{h}_{\text{prev}}$ and the new candidate $\tilde{\mathbf{h}}'$.

## 4.2. Long Short-Term Memory (LSTM)
**Additional Memory:**  
LSTMs maintain a cell state $\mathbf{c}$ that carries long-term information. They use multiple gates:
- **Forget Gate ($\mathbf{a}_f$)**: Decides what to discard from $\mathbf{c}$.
- **Input Gate ($\mathbf{a}_i$)**: Determines how much of the new candidate information to add.
- **Output Gate ($\mathbf{a}_o$)**: Controls what part of the cell state to output.

**Update Equations (Conceptually):**

$$
\begin{aligned}
\mathbf{c}' &= \mathbf{a}_f * \mathbf{c} + \mathbf{a}_i * \text{candidate} \\
\mathbf{h}' &= \mathbf{a}_o * \phi(\mathbf{c}')
\end{aligned}
$$

**Intuition:**  
The cell state $\mathbf{c}$ acts like a conveyor belt for information. The gates modulate this flow to capture long-term dependencies, addressing the vanishing gradient problem common in standard RNNs.

## 4.3. Highway Connections
**Purpose:**  
Similar to residual connections, highway networks allow the model to choose between transforming the input or passing it along unchanged.

**Operation:**  
A highway cell computes a candidate $\tilde{\mathbf{h}}$ and then gates between $\tilde{\mathbf{h}}$ and the original input $\mathbf{x}$:

$$\mathbf{h} = \text{gate}(\mathbf{a}, \tilde{\mathbf{h}}, \mathbf{x})$$

This controlled blend helps training very deep architectures.

# 5. Attention Mechanisms and Self-Attention
## 5.1. Motivation for Attention
**Problem with Pure Recurrence:**  
RNNs are inherently sequential and can be slow on parallel hardware. Moreover, they tend to bias recent inputs—making it hard to capture long-range dependencies.

**Breaking the Sequential Chain:**  
The idea behind attention is to compute interactions between all input tokens in parallel. Rather than propagating information solely through recurrent steps, each output can "attend" to all positions in the input sequence.

## 5.2. Dot Product Attention
**Basic Idea:**  
Given a sequence of hidden states $\mathbf{h}_1, \ldots, \mathbf{h}_T$ computed as:

$$\mathbf{h}_t = \phi(\mathbf{A}\mathbf{x}_t + \mathbf{b}_A)$$

one computes attention scores by the dot product:

$$a_{t,k} = \mathbf{h}_t \cdot \mathbf{h}_k$$

**Softmax Normalization:**  
To make these scores comparable, they are normalized via a softmax:

$$\mathbf{a}_t = \text{softmax}(a_{t,1}, \ldots, a_{t,T})$$

**Output Computation:**  
The output at time $t$ is a weighted sum:

$$\mathbf{y}_t = \sum_{k=1}^T a_{t,k} \mathbf{h}_k$$

**Intuition:**  
Inputs with similar representations (i.e. parallel vectors) yield high dot products, so the model learns to focus on related tokens.

## 5.3. Enhancements: Query, Key, and Multi-Head Attention
**Learned Projections:**  
Instead of using raw hidden states, the model can project $\mathbf{h}_t$ into a query $\mathbf{Q}\mathbf{h}_t$ and a key $\mathbf{K}\mathbf{h}_t$ (with additional biases). The attention score becomes:

$$a_{t,k} = (\mathbf{Q}\mathbf{h}_t + \mathbf{b}_Q) \cdot (\mathbf{K}\mathbf{h}_k + \mathbf{b}_K)$$

**Multi-Head Attention:**  
Multiple pairs $(\mathbf{Q}_m, \mathbf{K}_m)$ are learned, where each "head" can capture a different aspect of the input (for example, one head might focus on nouns while another attends to verbs). The outputs from each head are then combined to form the final result.

**Self-Attention:**  
When the attention is computed among elements of the same input sequence, it is called self-attention. This mechanism is central to the transformer architecture.

# 6. Positional Encoding
## 6.1. The Need for Position Information
**Issue in Attention Models:**  
Since self-attention computes pairwise interactions without any recurrence, it is inherently permutation-invariant. In other words, the order of the inputs is lost.

## 6.2. Encoding Strategies
**Sinusoidal Encoding:**  
A common approach is to add a position-dependent signal to the input embeddings:

$$\mathbf{x}_t' = \mathbf{x}_t + \sin(t * \text{range}(D))$$

Here, each dimension of the embedding gets a sinusoidal component that varies with position $t$.

**Effect:**  
When later projected via $\mathbf{Q}$ and $\mathbf{K}$, the dot product will implicitly capture relative position information.

**Alternative Views:**  
Position encoding can also be based on relative or absolute positional weights. The key idea is to let the model distinguish between tokens based solely on their position in the sequence.

# 7. The Transformer Architecture
## 7.1. Overview
**Parallelism:**  
Unlike RNNs, transformers process all tokens in parallel. This is achieved by replacing sequential recurrence with layers of attention.

**Decoder Model:**  
In the context of language modelling, the decoder predicts the next word given all previous tokens. This is done using masked self-attention—ensuring that at time $t$ the model only attends to positions $< t$.

## 7.2. Layered Transformer Model
**Input Embedding with Position Encoding:**

$$\mathbf{x}_t^0 = \mathbf{x}_t + \sin(t * \text{range}(D))$$

**Layer-by-Layer Processing:**  
For each layer $l \in \{1, \ldots, L\}$:
1. Compute a transformed hidden state:

$$\mathbf{h}_t^l = \phi(\mathbf{A}^l \mathbf{x}_t^{l-1} + \mathbf{b}_A^l)$$

2. Normalize (or "scale") the hidden state to produce $\hat{\mathbf{h}}_t^l$.
3. Compute attention scores using learned queries and keys:

$$a_{t,k}^{l,m} = (\mathbf{Q}^{l,m} \hat{\mathbf{h}}_t^l + \mathbf{b}_Q^{l,m}) \cdot (\mathbf{K}^{l,m} \hat{\mathbf{h}}_k^l + \mathbf{b}_K^{l,m})$$

where $m$ indexes the attention heads.
4. Apply softmax and sum over all positions:

$$\mathbf{y}_t^l = \sum_{k=1}^T \text{softmax}(a_{t,k}^{l,m}) \hat{\mathbf{h}}_k^l$$

5. Combine outputs across heads and pass through further transformations (using matrices $\mathbf{D}_1$ and $\mathbf{D}_2$) to produce the next layer's input:

$$\mathbf{x}_t^l = \mathbf{x}_t^{l-1} + \mathbf{y}_t^{l,''}$$

**Note:** The residual (or "skip") connections in each layer help gradients flow and stabilize training.

## 7.3. Training and Decoding
**Self-Supervised Training:**  
The model is trained to predict the next token:

$$\mathcal{L}(\mathbf{x}; \theta) = \text{loss}(f(\mathbf{x}_1, \ldots, \mathbf{x}_{T-1}; \theta), \mathbf{x}_T)$$

Here, the training label comes from the data itself.

**Large-Scale Datasets:**  
Models are trained on hundreds of billions of words (e.g., Wikipedia, "The Pile" dataset) to learn rich representations.

**Decoding Strategies:**
- **Sampling:**  
  Draw from $p(w_T \mid w_1, \ldots, w_{T-1})$ using a "temperature" $\tau$ that flattens or sharpens the distribution.
- **Beam Search:**  
  A Viterbi-style decoding where the most probable sequences are explored.
- **Other Techniques:**  
  Reinforcement learning from human feedback (RLHF) can further refine outputs, especially for large language models.

# 8. Summary and Key Intuitions
**Evolution of Architectures:**
- RNNs were designed to handle sequential data, where hidden states recursively capture past information. However, they struggle with long-range dependencies and are difficult to parallelize.
- Gated architectures (GRU, LSTM) introduce mechanisms that control the flow of information, helping to mitigate vanishing gradients and retain long-term context.
- Attention mechanisms allow models to compute dependencies between all tokens simultaneously. Dot-product attention (and its normalized softmax form) provides a way to focus on the most relevant parts of the sequence.
- Transformers combine self-attention with position encodings and layered processing to achieve both parallelism and effective modelling of long-range dependencies.

**Key Intuitions:**
- **Embeddings and Gradients:**  
  Transforming discrete tokens into continuous vectors (via an embedding matrix) enables the model to learn rich representations.
- **Recurrent vs. Parallel Processing:**  
  Recurrent networks process data sequentially, inherently biasing recent inputs, whereas attention-based models process all tokens in parallel and learn to weight information based on "relatedness."
- **Gating and Memory:**  
  Gated units and LSTMs use learned gates to decide what information to carry forward, making it easier to learn dependencies over long sequences.
- **Attention's Role:**  
  By computing interactions (dot products) between projections of hidden states, attention mechanisms let the network learn to emphasize important relationships (e.g., matching "surgeon" with "her") regardless of distance.
- **Positional Encoding:**  
  Since attention ignores order, positional encodings inject necessary information about the position of each token into the model.

**Modern Practice:**  
Today's state-of-the-art language models (e.g., GPT-3) use transformer architectures with dozens of layers, tens of thousands of parameters, and training on billions of words to achieve remarkable performance in diverse language tasks.