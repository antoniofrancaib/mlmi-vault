Lectures by Andrew Fitzgibbon

Topics: 

# 1. Transformer Architectures
## 1.1. Overview and Notation
The transformer model is defined as a function that maps an input sequence to an output sequence using a series of learned linear transformations, attention mechanisms, and residual connections. In the provided notation, we see the following key points:

**Input with Positional Encoding:**
The input at time-step $t$ is given by

$$x_t^0 = \mathbf{x}_t + \sin(t) \times \text{range}(D)$$

This formula indicates that each input vector is augmented with a sinusoidal positional encoding. The idea is to inject information about the token's position directly into the input embedding so that the model can differentiate positions.

**Layer-wise Transformation:**
For each layer $l \in \{1, \ldots, L\}$, the model computes:

A linear projection:

$$\mathbf{v}_t^l = \mathbf{A}^l \mathbf{x}_t^{l-1} + \mathbf{b}_A^l$$

Here, the notation has been modified relative to some earlier versions:

- The hidden state is renamed from $\mathbf{h}$ to $\mathbf{v}$ and its extra nonlinearity is removed.
- The input $\mathbf{x}$ is normalized rather than the intermediate hidden state.

**Attention Computation:**
Instead of computing attention on $\mathbf{h}$, the model now uses $\mathbf{x}$. Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and subsequent attention scores are computed as:

$$\text{at}^{l,m} = \text{softmax}\left(\tau (\mathbf{Q}^{l,m} \mathbf{x}_t^\circ + \mathbf{b}_Q^{l,m}) \cdot (\mathbf{K}^{l,m} \mathbf{x}_t^\circ + \mathbf{b}_K^{l,m})\right)$$

The parameter $\tau$ (typically set to $\frac{1}{\sqrt{D}}$) scales the dot-product to help stabilize gradients.

**Aggregation and Residual Connection:**
The attention outputs from multiple heads are aggregated (using a summation over heads after applying a linear transformation $\mathbf{T}$) and then passed through further linear transformations with non-linearities denoted by $\phi$ (which could represent activation functions or even feed-forward layers). Finally, a residual connection is applied:

$$x_t^l = x_t^{l-1} + y''_t^l$$

This residual formulation allows gradients to flow more easily through deep networks and is a key ingredient in training very deep models.

## 1.2. Key Design Choices
**Renaming and Nonlinearity:**
The removal of an extra nonlinearity (changing $\mathbf{h}$ to $\mathbf{v}$) simplifies the computation while still preserving the ability to learn complex mappings.

**Normalization on $\mathbf{x}$ rather than $\mathbf{h}$:**
By normalizing the inputs $\mathbf{x}$ directly, the model ensures that the attention mechanism works on a well-scaled set of features.

**Attention on $\mathbf{x}$:**
Using $\mathbf{x}$ for attention calculation emphasizes that the model's reasoning about relationships between tokens is based directly on the normalized input embeddings rather than a transformed hidden state.

**Scaling with $\tau$:**
Adding $\tau$ (usually $1/\sqrt{D}$) is essential to maintain numerical stability, especially when the dot products could become very large.

# 2. Transformer Training Loss
## 2.1. Input Representation
**Token Embeddings:**
The input consists of a sequence of tokens $w_1, w_2, \ldots, w_T$. Each token is mapped to a continuous vector using an embedding matrix $\mathbf{A}^0 \in \mathbb{R}^{V \times D}$. In formula:

$$\mathbf{x}_t = \mathbf{A}^0[w_t, :]$$

## 2.2. Sequence-to-Sequence Transformation
**Forward Pass:**
The transformer processes an input sequence $\mathbf{X} \in \mathbb{R}^{D \times T}$ and outputs a sequence $\mathbf{Y} \in \mathbb{R}^{D \times T}$.

**Prediction and Weight Tying:**
Predictions are computed using:

$$\mathbf{P} = \mathbf{A}^0 \times \mathbf{Y}$$

where $\mathbf{P} \in \mathbb{R}^{V \times T}$. Each column of $\mathbf{P}$ represents the logits for the vocabulary at a particular time-step. The concept of weight tying comes into play when the output projection matrix is set equal to $\mathbf{A}^0$ (i.e. $\mathbf{A}_{\text{out}} = \mathbf{A}^0$), which not only reduces the number of parameters but also can improve performance.

## 2.3. Loss Function
**Next-Token Prediction:**
The training loss is calculated by comparing the softmax-normalized predictions to the actual next token. For time-step $t$, the model predicts token $w_{t+1}$. The loss is a sum (or average) of the cross-entropy losses over the sequence:

$$L = \sum_{t=1}^{T-1} \text{CE}(\text{softmax}(\mathbf{P}[:, t]), w_{t+1})$$

This formulation encourages the model to capture the sequential structure of language by predicting the next token in the sequence.

# 3. Transformer Applications
## 3.1. Text Classification
**Fine-Tuning Pre-trained Models:**
For a task like sentence-level classification, a transformer pre-trained on a large corpus is fine-tuned on a dataset of (sentence, Boolean) pairs. Key steps include:

1. Introducing a special classification token (often denoted as `<cls>`) whose embedding is learned during fine-tuning.
2. Adding and training an additional multi-layer perceptron (MLP) on top of the transformer's output.
3. Fine-tuning all transformer weights along with the new components for the task at hand.

## 3.2. Natural Language Translation
**Encoder-Decoder Structure:**
Translation systems typically rely on a transformer that consists of an encoder and a decoder:

- **Encoder:** Processes the source language and produces a sequence of hidden states.
- **Decoder:** Uses self-attention to generate translations and cross-attention to integrate information from the encoder outputs.

**Attention Mechanisms in Translation:**

- **Self-Attention (Decoder):** Computes attention within the decoder's own input to capture dependencies.
- **Cross-Attention:** Combines the decoder's queries with the encoder's keys and values:

$$A_{st} = \mathbf{Q}[:, s] \cdot \mathbf{K}[:, t]$$

where $A_{st}$ represents the attention between the $s^{th}$ token in the decoder and the $t^{th}$ token in the encoder. The decoder then multiplies the softmax-scaled attention scores with the value vectors $\mathbf{V}$ to obtain context-aware representations.

# 4. Vision Transformers and Beyond
## 4.1. The Vision Transformer (ViT)
**Concept:**
The vision transformer adapts the transformer architecture—originally developed for language—to image data. The image is split into patches, each of which is linearly embedded, and these patch embeddings serve as the "tokens" for the transformer.

**Integration with CNNs:**
There is discussion on whether to add attention mechanisms to architectures like ResNet (e.g., processing a $2048 \times 49$ feature map with attention). The idea is to incorporate the strengths of transformers (global context via attention) into existing convolutional networks.

## 4.2. Regularization and Data Augmentation When Pretraining Is Unavailable
**Regularization Techniques:**
When large-scale pretraining is not feasible, several regularization methods can help improve performance:

- **Weight Decay:** Penalizes large weights to prevent overfitting.
- **Stochastic Depth:** Randomly drops entire layers during training to encourage robustness.
- **Dropout (in FFN layers):** Randomly zeroes a subset of activations to promote generalization.

**Data Augmentation:**
Techniques like MixUp and RandAugment are used to artificially enlarge the training dataset by blending images or applying random transformations, respectively. These methods have been shown to yield significant improvements in performance.

**Key Insight:**
Regularization and data augmentation together often yield improvements that can partly make up for the lack of pretraining, as the model is forced to learn more robust and invariant features.

## 4.3. Improving Vision Transformers: Distillation
**Distillation:**
This technique involves training a smaller "student" model to mimic the outputs of a larger, pre-trained "teacher" model. Distillation can help improve the performance of vision transformers by transferring knowledge from more complex models.

# 5. Generative Models
## 5.1. Latent Variable Models and Variational Autoencoders (VAEs)
**Modeling Image Distributions:**
The goal is to learn a probability density $p_\theta(\mathbf{x})$ for images $\mathbf{x}$. We parameterize this density with parameters $\theta$ and maximize the log-likelihood over the dataset.

**Latent Variable Decomposition:**
The joint density is factorized as:

$$p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z}) d\mathbf{z}$$

where $p(\mathbf{z})$ is typically a simple prior such as a unit Gaussian $\mathcal{N}(0, \mathbf{I})$ in a low-dimensional space (e.g., $\mathbb{R}^{20}$).

**Encoder-Decoder Framework:**

- **Decoder $p_\theta(\mathbf{x} \mid \mathbf{z})$:**
  Often modeled as a Gaussian $\mathcal{N}(\mathbf{x} \mid \mu_{\mathbf{x} \mid \mathbf{z}}, \Sigma_{\mathbf{x} \mid \mathbf{z}})$, where the parameters $\mu_{\mathbf{x} \mid \mathbf{z}}$ and $\Sigma_{\mathbf{x} \mid \mathbf{z}}$ are produced by a neural network.

- **Encoder $q_\phi(\mathbf{z} \mid \mathbf{x})$:**
  Since the true posterior $p_\theta(\mathbf{z} \mid \mathbf{x})$ is intractable, an approximate posterior $q_\phi(\mathbf{z} \mid \mathbf{x})$ is introduced (also modeled as a Gaussian with parameters $\mu_{\mathbf{x}; \phi}$ and $\Sigma_{\mathbf{x}; \phi}$).

**Evidence Lower Bound (ELBo):**
The training objective maximizes the ELBo:

$$\text{ELBo}(\mathbf{x}; \theta, \phi) = \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x})} [\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{\text{KL}}(q_\phi(\mathbf{z} \mid \mathbf{x}) \parallel p(\mathbf{z}))$$

The steps for evaluating the ELBo for a given $\mathbf{x}$ include:

1. Computing $\mu_{\mathbf{z}}, \Sigma_{\mathbf{z}}$ using the encoder.
2. Sampling one or more $\mathbf{z}$ values from $\mathcal{N}(\mu_{\mathbf{z}}, \Sigma_{\mathbf{z}})$.
3. Passing these samples through the decoder to compute $\mu_{\mathbf{x}}$ and $\Sigma_{\mathbf{x}}$.
4. Evaluating the log-likelihood $\log \mathcal{N}(\mathbf{x} \mid \mu_{\mathbf{x}}, \Sigma_{\mathbf{x}})$ and averaging over samples.
5. Subtracting the KL divergence between $q_\phi(\mathbf{z} \mid \mathbf{x})$ and the prior $p(\mathbf{z})$.

## 5.2. Generative Adversarial Networks (GANs)
**Basic Idea:**
GANs learn to generate data by pitting a generator network against a discriminator network. Variants include:

- **DCGAN:** A convolutional GAN designed for image generation.
- **Progressive Growing of GANs:** The generator progressively creates images of increasing resolutions while the discriminator improves correspondingly.
- **Conditional GANs:** The generator is conditioned on category labels, so that the output belongs to a desired class.

## 5.3. Diffusion Models
**State-of-the-Art Approaches:**
Diffusion models have recently emerged as some of the best generative models (used in systems like Sora and even in molecular modeling such as AlphaFold). Their key aspects include:

**Training Process:**
The model learns to reverse a diffusion process. At each step, the model predicts $\mathbf{z}_{t-1}$ from $\mathbf{z}_t$ and the current time-step $t$:

$$\mathbf{z}_{t-1} = f(\mathbf{z}_t, t; \theta)$$

**U-Net Architectures:**
A U-Net is often employed for the reverse process. The U-Net architecture helps capture multi-scale information, which is crucial for generating high-quality images.

**Practical Output Examples:**
The notes reference sample outputs (e.g., "a golden retriever dog wearing a blue checkered beret and red dotted turtleneck") to illustrate the impressive capabilities of diffusion models.

# 6. Summary: Fundamentals, Architectures, and Applications
## 6.1. Deep Learning Fundamentals
**Activation Functions and ReLU Networks:**
Understanding how ReLU activations partition the input space into linear regions is crucial for appreciating the expressive power of deep networks.

**Training Dynamics:**
Topics include gradient descent, numerical stability (e.g., scaling issues addressed by $\tau$), initialization strategies, and normalization techniques.

**Regularization and Ensembling:**
Techniques such as weight decay, dropout, early stopping, and model ensembling help prevent overfitting and improve generalization.

**Invariance and Inductive Bias:**
Data augmentation and convolutional architectures incorporate invariances (e.g., translation invariance) into models.

## 6.2. Architectures
**Fully Connected Networks:**
Simple multi-layer perceptrons that serve as the building blocks of more complex architectures.

**Convolutional Networks:**
Designed for grid-structured data (such as images), exploiting spatial locality.

**Sequence Models and Transformers:**
These models capture long-range dependencies in sequential data using self-attention mechanisms. Transformers have not only revolutionized natural language processing but are also being successfully applied to vision tasks.

## 6.3. Applications
**Classification and Translation:**
Transformers have proven effective in tasks ranging from text classification to language translation, thanks to their ability to model complex dependencies.

**Image Generation:**
From VAEs and GANs to state-of-the-art diffusion models, deep generative models enable the creation of high-fidelity images and other data types.