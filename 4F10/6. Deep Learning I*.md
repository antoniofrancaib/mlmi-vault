Lectures by Andrew Fitzgibbon

Topics: 

Discussion: 
- Shallow networks
- Composing two networks  
- Combining the two networks into one  
- **Hyperparameters**  (depth - width)
- Notation change and general case  
- Shallow vs. deep networks

![[deep-overview.png]]


# 1. Regression, Loss Functions, and Training
## 1.1. Setting Up the Problem
**Training Data:**  We assume a dataset  
$$D = \{(\mathbf{x}^{(i)}, y^{(i)}) \mid i = 1, \dots, n\}$$  
where each input $\mathbf{x}^{(i)}$ is paired with a corresponding observation $y^{(i)}$.  

**Objective:**  Our goal is to learn a function $f(\mathbf{x}; \boldsymbol{\theta})$ parameterized by $\boldsymbol{\theta}$ that maps inputs $\mathbf{x}$ to outputs, such that the predictions are as close as possible to the observations.  

## 1.2. Loss Function and Minimization  
**Loss Function:**  To measure the difference between the predicted output and the observed value, we define a loss function. For example, a simple loss function might be: 

$$L(y, \hat{y}) = y - \hat{y}$$  
More generally, the loss function could be any measure (e.g., mean squared error, cross-entropy) that quantifies prediction error.  

**Total Loss:**  The overall loss over the training set is given by:  

$$L(\boldsymbol{\theta}; D) = \sum_{(\mathbf{x}^{(i)}, y^{(i)}) \in D} L(f(\mathbf{x}^{(i)}; \boldsymbol{\theta}), y^{(i)})$$ 
Our aim is to find parameters $\boldsymbol{\theta}$ that minimize this total loss.  

**Intuition:**  By minimizing $L$, we adjust the parameters so that the function $f(\mathbf{x}; \boldsymbol{\theta})$ best fits the data. This is the *essence of training in deep learning*.  

# 2. Choosing the Function $f(\mathbf{x}; \boldsymbol{\theta})$  
## 2.1. Simple Function Families  
**Linear Function:**  A simple choice is a linear function:  
$$f(\mathbf{x}; \boldsymbol{\theta}) = \theta_0 + \theta_1 \mathbf{x}$$  
which corresponds to fitting a straight line.  

**Quadratic Function:**  To capture curvature, one might choose:  

$$f(\mathbf{x}; \boldsymbol{\theta}) = \theta_0 + \theta_1 \mathbf{x} + \theta_2 \mathbf{x}^2$$  
**Nonlinear Components:**  Adding periodic functions, for instance:  

$$f(\mathbf{x}; \boldsymbol{\theta}) = \theta_0 + \theta_1 \mathbf{x} + \theta_2 \sin \mathbf{x}$$  
helps model data with oscillatory behavior.  

## 2.2. Basis Functions  
Rather than handcrafting the form of $f$, we often express it as a linear combination of basis functions:  

$$f(\mathbf{x}; \boldsymbol{\theta}) = \sum_k \theta_k \phi_k(\mathbf{x})$$  
**Common Choices:**  
- Polynomials: $\phi_k(\mathbf{x}) = \mathbf{x}^k$  
- Sinusoids: $\phi_k(\mathbf{x}) = \sin(k \mathbf{x})$  
- Exponentials: $\phi_k(\mathbf{x}) = \exp\left(-\frac{(\mathbf{x} - \mu_k)^2}{\sigma^2}\right)$ (Gaussian basis functions)  
- Sigmoids and ReLUs: Nonlinear functions that introduce thresholds and piecewise linearity.  

**Translated Basis Functions:**  Often it is beneficial to allow shifts in the basis functions:  

$$\phi(\mathbf{x} - \theta_k)$$  
For example, translated Gaussians or sigmoids can capture localized behavior in the input space.  

# 3. Neural Networks as Compositions of Basis Functions  
## 3.1. The Shallow Network  
**Single Hidden Layer:**  In a shallow network, we compute several hidden units (each a transformed version of the input) and then combine them to produce the output:  

$$h_k = \phi(a_k \mathbf{x} + b_k) \quad \text{for each hidden unit } k,$$  
and then  
$$y = \sum_k c_k h_k + c_0.$$  
Here, $a_k$ and $b_k$ are parameters for each hidden unit (affecting the pre-activation), $\phi$ is a nonlinearity (such as ReLU, sigmoid, or Gaussian), and $c_k$ are the output weights.  

**ReLU Networks and Piecewise Linearity:**  When using ReLU (Rectified Linear Unit) activation:  
$$\phi(t) = \max(0, t),$$  
each hidden unit outputs a piecewise linear function. The network as a whole then computes a piecewise linear function, where the "activation pattern" (which hidden units are active for a given $\mathbf{x}$) determines the linear region.  

## 3.2. Universal Approximation and Expressivity  
**Universal Approximation Theorem:**  With a sufficient number of hidden units, a shallow network can approximate any continuous function on a compact domain to arbitrary precision. This means that even a single hidden layer is, in principle, enough to represent complex functions.  

**Regions and Convex Polytopes:**  
- For one-dimensional inputs, each hidden unit's ReLU creates a "kink" or joint, **dividing the input into regions**.  
- For higher-dimensional inputs, the network partitions the space into convex polytopes. The number of such regions increases with the number of hidden units.  

**Parameter-Region Relationship:**  
There is an interplay between the number of parameters and the number of distinct linear regions. For example, with 500 hidden units (or approximately 51,001 parameters in a certain configuration), the number of regions can be dramatically larger than with a shallower, less parameterized network.  

**Zaslavsky's Result:**  
The maximum number of regions created by $D_i$ hyperplanes in $D$ dimensions is bounded between $2^{D_i}$ and $2^D$ (more precisely, it involves binomial coefficients). This gives insight into the network's capacity to partition the input space.  

Number of regions created by $D > D_i$ planes in $D_i$ dimensions was proved by Zaslavsky (1975) to be:

$$
\sum_{j=0}^{D_i} \binom{D}{j}
$$

How big is this? It’s greater than $2^{D_i}$ but less than $2^D$.

# 4. Notation and Terminology in Neural Networks  
Understanding the precise terminology is key:  

**Weights and Biases:**  
- Weights: Often called "slopes," these parameters determine the influence of each input feature or hidden unit.  
- Biases (Y-offsets): These are constant terms added to the weighted sums before applying the activation function.  

**Activations vs. Pre-activations:**  
- Pre-activations: The raw output of a linear combination (e.g., $a_k \mathbf{x} + b_k$) before the nonlinearity.  
- Activations: The output after applying the activation function (e.g., $\phi(a_k \mathbf{x} + b_k)$).  

**Network Architecture:**  
- Fully Connected: Every node in one layer connects to every node in the next.  
- Feedforward: Information moves in one direction, with no loops.  

**Shallow vs. Deep:**  
- Shallow Networks: Typically consist of one hidden layer.  
- Deep Networks: Contain multiple hidden layers, increasing the network's capacity and *expressiveness*.  

# 5. Deep Networks: Composition and Hierarchical Representation  
## 5.1. Composition of Functions  
**Why Compose?**  
While a single linear transformation (or even a single layer of nonlinearity) can only represent limited classes of functions, composing multiple layers enables the network to build hierarchical features.  

**Function Composition Example:**  
A deep network might be written as:  

```python
def f(x, theta):
    h1 = F(x; theta1)
    h2 = F(h1; theta2)
    y  = F(h2; theta3)
    return y
```

Each $F(\cdot; \theta_i)$ represents a layer with its own parameters. The key is that **nonlinearity must be introduced at each stage**; note that composing linear functions without a nonlinearity would simply yield another linear function:

$$
y = A_3 \left( A_2 \left( A_1 x + b_1 \right) + b_2 \right) + b_3 = A' x + b'
$$

Thus, the activation functions (e.g., ReLU, sigmoid) are **essential**.

### 5.2. Combining Two Networks

**From Two Networks to One**:  
When you compose two networks, you are essentially creating a new network where the hidden representations of the first become the inputs to the second. This composition can sometimes be “collapsed” into a single equivalent network; however, the explicit layered structure often leads to more efficient representations and easier learning.

**Intuitive View**:  
Think of each layer as extracting features at a different level of abstraction. Early layers might capture simple patterns (edges, simple curves), while deeper layers capture high-level abstractions (object parts, complex shapes).

# 6. Hyperparameters and Network Capacity
## 6.1. Hyperparameters Defined
**Depth:**  The number of layers in the network. More layers can model more complex functions.

**Width:**  The number of hidden units per layer. Increasing width can increase the number of linear regions the network can represent.

**Other Hyperparameters:**  Learning rates, regularization parameters, and the specific choice of activation functions are also critical and are usually chosen via hyperparameter search.

## 6.2. Capacity and Expressiveness
**Hidden Units and Capacity:** The number of hidden units is often taken as a rough proxy for the network's capacity. For example, a shallow network with six hidden units may have 20 parameters and create around 9 regions, while another configuration might have 19 parameters and at most 7 regions. Increasing the number of hidden units typically increases the network's ability to partition the input space into many distinct regions.

**Trade-Offs:**  A key challenge in deep learning is balancing the model's capacity (which must be **high enough to capture complex patterns**) with the risk of **overfitting**. *Hyperparameter optimization is therefore a crucial part of model design*.

# 7. Vector and Matrix Representations for Multiple Inputs/Outputs
## 7.1. Moving Beyond Scalar Functions
**From Scalars to Vectors:**  When inputs and outputs are vectors, the operations are represented in matrix form:

$$\mathbf{y} = C \, \phi(A \, \mathbf{x} + \mathbf{b})$$

where $A$ and $C$ are weight matrices, and $\mathbf{b}$ is a bias vector.

**Fully Connected Layers:**  In a fully connected (feedforward) network, every element of one layer is connected to every element of the next. The operations in each layer are vectorized for computational efficiency.

## 7.2. Multiple Outputs and Hidden Layers
**Example with Two Outputs:**  For a network with one input, four hidden units, and two outputs, the computation in the hidden layer is:

$$h_k = \phi(a_k \mathbf{x} + b_k) \quad \text{for } k = 1, \dots, 4,$$

and then the outputs are computed as:

$$y_1 = c_{11} h_1 + c_{12} h_2 + c_{13} h_3 + c_{14} h_4 + c_{10},$$  
$$y_2 = c_{21} h_1 + c_{22} h_2 + c_{23} h_3 + c_{24} h_4 + c_{20}.$$

This illustrates how the network's parameters (weights and biases) scale with the number of inputs, hidden units, and outputs.

# 8. Summary and Key Takeaways
**Fundamental Approach:**  Deep learning can be understood as learning a parameterized function $f(\mathbf{x}; \boldsymbol{\theta})$ that minimizes a loss over training data. The choice of $f$ is critical and is typically built from basis functions and their translations.

**Shallow vs. Deep Networks:**  
- Shallow Networks: A single hidden layer can, with enough units, approximate any continuous function (universal approximation). However, they might require an impractical number of units to capture complex relationships.  
- Deep Networks: By composing multiple layers, deep networks can represent complex, hierarchical functions more efficiently. Each layer refines the representation learned by the previous one.

**Piecewise Linear Functions and Regions:**  
Networks using ReLU activations partition the input space into convex polytopes. The number of such regions (and thus the expressivity) grows with the number of hidden units and layers.

**Hyperparameters:**  
The depth and width of a network are chosen before training and have a direct impact on the network's capacity. Hyperparameter optimization is essential for balancing model expressiveness with the risk of overfitting.

**Function Composition and Nonlinearity:**  
The power of deep learning lies in the composition of nonlinear transformations. Without nonlinearity, multiple layers would collapse into a single linear transformation.

