Lectures by Jose Miguel Hernandez Lobato

Topics: 

Graphical models provide a framework for representing high-dimensional probability distributions in a compact, structured way. By exploiting conditional independencies, these models reduce complexity in both representation and computation. The following sections break down the key ideas, starting with fundamental probability rules and building up to advanced topics such as Bayesian networks and Markov networks.

# 1. Basics of Probability

## Sum and Product Rules

**Sum Rule:**  
The marginal probability of a variable $\mathbf{X}$ is obtained by summing (or integrating) out the other variable(s):

$$p(\mathbf{X}) = \sum_{\mathbf{Y}} p(\mathbf{X}, \mathbf{Y})$$

**Product Rule:**  
The joint probability of two variables can be expressed as a product of a conditional probability and a marginal probability:

$$p(\mathbf{X}, \mathbf{Y}) = p(\mathbf{Y} \mid \mathbf{X}) p(\mathbf{X}) = p(\mathbf{X} \mid \mathbf{Y}) p(\mathbf{Y})$$

## Bayes' Rule
By combining the sum and product rules, Bayes' rule is derived as:

$$p(\mathbf{Y} \mid \mathbf{X}) = \frac{p(\mathbf{X} \mid \mathbf{Y}) p(\mathbf{Y})}{p(\mathbf{X})} = \frac{p(\mathbf{X} \mid \mathbf{Y}) p(\mathbf{Y})}{\sum_{\mathbf{Y}} p(\mathbf{X}, \mathbf{Y})}$$

This fundamental rule underpins many machine learning algorithms by allowing us to "flip" conditional probabilities.

## Independence and Conditional Independence

**Independence ($\mathbf{X} \perp \mathbf{Y}$):**  
Two variables are independent if the joint distribution factorizes into the product of their marginals:

$$p(\mathbf{X}, \mathbf{Y}) = p(\mathbf{X}) p(\mathbf{Y})$$

**Conditional Independence ($\mathbf{X} \perp \mathbf{Y} \mid \mathbf{Z}$):**  
When conditioned on a third variable $\mathbf{Z}$, $\mathbf{X}$ and $\mathbf{Y}$ are independent if:

$$p(\mathbf{X}, \mathbf{Y} \mid \mathbf{Z}) = p(\mathbf{X} \mid \mathbf{Z}) p(\mathbf{Y} \mid \mathbf{Z})$$

This property is crucial for reducing the complexity of joint distributions.

# 2. Independence and Conditional Independence: Examples and Intuition

## Independence Example
Consider two random variables $\mathbf{X}$ and $\mathbf{Y}$ defined on $[0,1]$.

![[independence-example.png]]
**Independent Case:**  
If $p(\mathbf{X}, \mathbf{Y}) = 1$ (a uniform distribution over the unit square), then $\mathbf{X}$ and $\mathbf{Y}$ are independent because the joint density factorizes into $p(\mathbf{X}) = 1$ and $p(\mathbf{Y}) = 1$.

**Non-Independent Case:**  
A more complex distribution such as

$$p(\mathbf{X}, \mathbf{Y}) = 2 \, I\left[\mathbf{X} - \frac{1}{2} + \mathbf{Y} - \frac{1}{2} < \frac{1}{2}\right]$$

does not factorize into the product of two independent marginal densities. Here, the indicator function $I[\cdot]$ imposes a constraint that couples $\mathbf{X}$ and $\mathbf{Y}$.

## Conditional Independence Example
For variables $\mathbf{X}$ and $\mathbf{Y}$ conditioned on a discrete variable $\mathbf{Z} \in \{0, 1\}$:

![[cond-independence-example.png]]
**When $\mathbf{X} \perp \mathbf{Y} \mid \mathbf{Z}$ holds:**  
The joint distribution for a given $\mathbf{Z}$ factorizes as

$$p(\mathbf{X}, \mathbf{Y} \mid \mathbf{Z}) = p(\mathbf{X} \mid \mathbf{Z}) p(\mathbf{Y} \mid \mathbf{Z})$$

Different plots (or tables) for $\mathbf{Z} = 1$ and $\mathbf{Z} = 0$ may each show a factorized structure, simplifying the representation. Distribution is simple  


$$
p(X, Y, Z) = \frac{Z}{2} \, \mathcal{N}(X \mid 2, 1) \, \mathcal{N}(Y \mid 2, 1)
+ \frac{1 - Z}{2} \, \mathcal{N}(X \mid -2, 1) \, \mathcal{N}(Y \mid -2, 1)
$$


**When conditional independence does not hold:**  
The joint distribution involves extra dependencies that prevent such a neat factorization, often resulting in more complicated interaction terms. Distribution is more complicated

$$
\begin{aligned}
p(X, Y, Z) =\ & \frac{Z}{2} \left[ \frac{1}{2} \, \mathcal{N}(X \mid 2, 1) \, \mathcal{N}(Y \mid 2, 1)
+ \frac{1}{2} \, \mathcal{N}(X \mid -2, 1) \, \mathcal{N}(Y \mid -2, 1) \right] \\
& + \frac{1 - Z}{2} \left[ \frac{1}{2} \, \mathcal{N}(X \mid 2, 1) \, \mathcal{N}(Y \mid 2, 1)
+ \frac{1}{2} \, \mathcal{N}(X \mid -2, 1) \, \mathcal{N}(Y \mid -2, 1) \right]
\end{aligned}
$$

# 3. Motivation for Conditional Independencies

## Simplification Through Factorization
A key advantage of assuming conditional independencies is that it allows the representation of a high-dimensional joint distribution as a product of lower-dimensional factors. For example, consider the distribution:

$$p(\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}) = p(\mathbf{A} \mid \mathbf{C}) \, p(\mathbf{B} \mid \mathbf{C}) \, p(\mathbf{C}) \, p(\mathbf{D})$$

![[motivation-cond-independence.png]]

**Compact Representation:**  
Instead of a single table with $2^4 = 16$ entries, we now have smaller tables:

- $p(\mathbf{A} \mid \mathbf{C})$: size 4  
- $p(\mathbf{B} \mid \mathbf{C})$: size 4  
- $p(\mathbf{C})$: size 2  
- $p(\mathbf{D})$: size 2  

**Independencies:**  
From this factorization, one can read off several conditional independence properties:

- $\mathbf{C} \perp \mathbf{D}$ (no direct dependence between $\mathbf{C}$ and $\mathbf{D}$)  
- $\mathbf{D} \perp \mathbf{A}$ and $\mathbf{D} \perp \mathbf{B}$ (since $\mathbf{D}$ is independent of $\mathbf{A}$ and $\mathbf{B}$ once $\mathbf{C}$ is specified)  
- $\mathbf{A} \perp \mathbf{B} \mid \mathbf{C}$  

## Application to Language Models
In language modeling, the goal is to model the probability of a sequence of words:

$$p(\mathbf{W}_1, \mathbf{W}_2, \ldots, \mathbf{W}_T) = p(\mathbf{W}_1) \, p(\mathbf{W}_2 \mid \mathbf{W}_1) \, p(\mathbf{W}_3 \mid \mathbf{W}_1, \mathbf{W}_2) \cdots p(\mathbf{W}_T \mid \mathbf{W}_1, \ldots, \mathbf{W}_{T-1})$$

**Challenge:**  
Directly estimating these probabilities becomes intractable because most long sub-sequences rarely appear in training data.

**Solution via Conditional Independence:**  
By assuming that a word depends only on the preceding one (first-order Markov) or the preceding two words (second-order Markov), the model dramatically reduces the complexity. This leads to more robust estimates using frequency counts of short n-grams.

# 4. Markov Models and the Big Picture

## Markov Models
**Definition:**  
A Markov model leverages the product rule along with a conditional independence assumption that the future is independent of the past given the present.

**First Order Markov:**

$$p(\mathbf{W}_1, \ldots, \mathbf{W}_T) \approx p(\mathbf{W}_1) \prod_{t=2}^T p(\mathbf{W}_t \mid \mathbf{W}_{t-1})$$
![[first-order-markov.png]]

**Higher-Order Markov:**  
Extend the conditional independence to include more past words.

![[second-order-markov.png]]
## The Big Picture: Structured Distributions
**Intractability of Fully Flexible Distributions:**  
Modeling a joint distribution $p(\mathbf{X}_1, \ldots, \mathbf{X}_d)$ without assumptions requires an exponential number of parameters. Working with these fully flexible joint distributions is **intractable**

**Structured Factorizations:**  
By breaking down the joint distribution into products of simpler factors (each involving only a few variables), we:

- Compactly represent the overall distribution.  
- Simplify parameter estimation (learning) and inference.  
- Enable efficient marginalization, which is critical for many tasks.  

# 5. Graphical Models: A Visual Language for Factorization

Graphical models provide a natural way to represent the factorization of a joint distribution. There are two main types:

## Directed Graphical Models (Bayesian Networks)

**Definition:**  A Bayesian network is a directed acyclic graph (DAG) where each node represents a random variable. For each variable $\mathbf{X}_i$ with parents $\text{Pa}(\mathbf{X}_i)$, the joint distribution factorizes as:

$$p(\mathbf{X}_1, \ldots, \mathbf{X}_d) = \prod_{i=1}^d p(\mathbf{X}_i \mid \text{Pa}(\mathbf{X}_i))$$

**Conditional Independencies:**  The network encodes that each variable is conditionally independent of its non-descendants $\text{ND}(\mathbf{X}_i)$ given its parents:

$$\mathbf{X}_i \perp \text{ND}(\mathbf{X}_i) \mid \text{Pa}(\mathbf{X}_i)$$

**Example:**  
Consider a network with the following structure:

![[bayesian-gnn.png]]

**Factorization:**  
$$p(S, F, H, C, M) = p(S) \, p(F \mid S) \, p(H \mid S) \, p(C \mid F, H) \, p(M \mid F)$$

**Key Independencies:**  
- $F \perp H \mid S$  
- $C \perp S \mid F, H$  
- $M \perp H \mid F$ and $M \perp C \mid F$  

## Efficient Marginalization in Bayesian Networks
When computing marginals (e.g., $p(d)$ in a chain $A \rightarrow B \rightarrow C \rightarrow D$), naive summation can lead to a table with $n^4$ entries (if each variable takes $n$ values). However, by reordering the summations:

$$p(d) = \sum_c p(d \mid c) \left( \sum_b p(c \mid b) \left( \sum_a p(b \mid a) p(a) \right) \right)$$

the intermediate tables remain small (at most $n^2$ entries), significantly reducing computation. This approach is a form of variable elimination.

# 6. Undirected Graphical Models (Markov Networks)

## Motivation for Undirected Models
**Symmetry:** In some problems (e.g., image segmentation or multivariate Gaussians with sparse precision matrices), the natural interactions between variables are **symmetric**. Assigning a direction (as in Bayesian networks) can be awkward.

**Gaussian Example:** A multivariate Gaussian with covariance $\Sigma$ and sparse precision matrix $\Lambda = \Sigma^{-1}$ can be written as:

$$p(\mathbf{X}_1, \ldots, \mathbf{X}_d) \propto \exp\left( -\frac{1}{2} \sum_{(i,j) \in E} \lambda_{i,j} \mathbf{X}_i \mathbf{X}_j \right) = \prod_{(i,j) \in E}\exp\left( -\frac{1}{2}  \lambda_{i,j} \mathbf{X}_i \mathbf{X}_j \right)$$

The symmetry in the interaction (i.e., $\mathbf{X}_i$ with $\mathbf{X}_j$) is better captured by an undirected graph.

## Markov Networks
**Definition:**  A Markov network is an undirected graph where each node represents a random variable. The model is specified via positive potential functions $\phi_i(D_i)$ defined on **cliques** (fully connected subsets of nodes). The joint distribution is:

$$p(\mathbf{X}_1, \ldots, \mathbf{X}_d) = \frac{1}{Z} \prod_{i=1}^k \phi_i(D_i)$$

where $Z$ is the partition function ensuring normalization.

![[clique-examples.png]]

**Conditional Independencies:**  
The graph encodes that any two sets of nodes $\mathbf{A}$ and $\mathbf{B}$ are conditionally independent given a set $\mathbf{C}$ -i.e. $\mathbf{A} \perp \mathbf{B} \mid \mathbf{C}$- if $\mathbf{C}$ separates $\mathbf{A}$ and $\mathbf{B}$ in the graph ($\mathbf{C}$ blocks all paths in $\mathcal{G}$ between $\mathbf{A}$ and $\mathbf{B}$).

**Example: Simple Markov Network**  
**Graph:**  Consider a graph over nodes $\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}$ with potentials:

$$p(\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}) = \frac{1}{Z} \phi_1(\mathbf{A}, \mathbf{B}) \, \phi_2(\mathbf{B}, \mathbf{C}) \, \phi_3(\mathbf{C}, \mathbf{D}) \, \phi_4(\mathbf{A}, \mathbf{D})$$

![[markov-network.png]]
**Independencies:**  
This factorization implies conditional independencies such as $\mathbf{A} \perp \mathbf{C} \mid \{\mathbf{B}, \mathbf{D}\}$ and $\mathbf{B} \perp \mathbf{D} \mid \{\mathbf{A}, \mathbf{C}\}$.

# 7. The Potts Model: A Markov Network for Image Segmentation

![[segmentation-example.png]]

**Model Definition**: The Potts model is a type of Markov network particularly useful for image segmentation. Suppose we have $n$ pixels (or regions), each assigned a label $x_i \in \{1, \ldots, C\}$. The joint distribution is given by:

$$p(x_1, \ldots, x_n) = \frac{1}{Z} \prod_{(i,j) \in \text{Edges}} \phi_{ij}(x_i, x_j)$$

where the pairwise potential is defined as:

$$\log \phi_{ij}(x_i, x_j) = \begin{cases} 
\beta, & \text{if } x_i = x_j, \\
0, & \text{otherwise},
\end{cases}$$

with $\beta > 0$ encouraging neighboring pixels to have the same label.

![[potts-model.png]]
## Intuitive Understanding
**Segmentation Goal:**  In image segmentation, we aim to partition an image into regions where pixels within the same region are similar.

**Role of the Potential:**  The potential $\phi_{ij}$ rewards assignments where adjacent pixels share the same label, leading to smooth, coherent segments.

**Normalization:**  The partition function $Z$ sums over all possible label assignments to ensure the distribution is properly normalized, though it is often computationally challenging to compute directly.

# 8. Summary and Key Takeaways

**Fundamental Tools:**  The sum and product rules, along with Bayes' rule, form the basis for all probability-based modeling.

**Conditional Independence:**  This property is central to reducing the complexity of joint distributions, allowing high-dimensional problems to be broken down into simpler parts.

**Structured Distributions:**  By factorizing a joint distribution into smaller factors, we achieve a compact representation that facilitates efficient learning and inference.

**Graphical Models:**  
- **Bayesian Networks (Directed):** Use a DAG to represent conditional dependencies via a factorization $p(\mathbf{X}_1, \ldots, \mathbf{X}_d) = \prod_i p(\mathbf{X}_i \mid \text{Pa}(\mathbf{X}_i))$.  
- **Markov Networks (Undirected):** Represent symmetric relationships via potential functions on cliques.  

**Efficient Marginalization:**  The choice of summation order (or variable elimination) can drastically reduce computational costs.

**Practical Examples:**  Applications such as language models and image segmentation (via the Potts model) illustrate the real-world importance of these structured probabilistic frameworks.
