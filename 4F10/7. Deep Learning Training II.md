Lectures by Andrew Fitzgibbon

Topics: 

- Maths overview  
- Gradient descent algorithm  
- Linear regression example  
- Gabor model example  
- Stochastic gradient descent  
- Momentum  
- **Adam**


# 2. Loss Functions and Their Role in Learning  

## **Regression Loss**  
For tasks like 1D regression, a common loss is the least squares loss. Given a prediction $f(\mathbf{x}_i;\mathbf{\theta})$ and an observation $y_i$, the loss might be defined as:  

$$L(y,y_i)=(f(\mathbf{x}_i;\mathbf{\theta})-y_i)^2.$$  
The overall training loss is then:  

$$L(\mathbf{\theta})=\sum_{i=1}^n L(f(\mathbf{x}_i;\mathbf{\theta}),y_i).$$ 
This function measures the discrepancy between the predictions and the actual observations.  

## **Classification Loss**  
For classification problems, where $y_i$ is a label in $\{1,\ldots,K\}$, the procedure is slightly more involved:  

- **Output Vector:** $\mathbf{y}\in\mathbb{R}^K$ (a vector of raw scores or logits).  
- **Normalization:** Use the softmax function to transform these logits into probabilities: 

$$z_k=\frac{\exp(y_k)}{\sum_{k'}\exp(y_{k'})},$$  
ensuring that $0\leq z_k\leq1$ and $\sum_k z_k=1$.  

- **Cross-Entropy Loss:** Define the loss as the negative log likelihood of the true class $l$

$$L(\mathbf{y},y_i)=-\log(z_l)=-\left(y_l-\log\sum_{k'}\exp(y_{k'})\right).$$  
The training objective is again to minimize the total loss over the training set.  

# 3. Neural Network Architectures and Composition  
## **Basic Architecture**  
A deep neural network is built by composing many layers of simple functions. A typical network might be written as:  

$$
\begin{aligned}
\mathbf{h}_1&=\phi(\mathbf{A}_1\mathbf{x}+\mathbf{b}_1), \\
\mathbf{h}_2&=\phi(\mathbf{A}_2\mathbf{h}_1+\mathbf{b}_2), \\
\mathbf{h}_3&=\phi(\mathbf{A}_3\mathbf{h}_2+\mathbf{b}_3), \\
\mathbf{y}&=\phi'(\mathbf{A}_4\mathbf{h}_3+\mathbf{b}_4).
\end{aligned}
$$  
Here:  

- $\mathbf{A}_i$ and $\mathbf{b}_i$: These are weight matrices and bias vectors with specific dimensionalities.  
- $\phi$ and $\phi'$: Non-linear activation functions (often ReLU for hidden layers and possibly a different one—or none—on the final layer).  

## **Piecewise Linear Regions**  
ReLU Activation: When using ReLU, each layer computes a piecewise linear transformation. With a single input, you get a piecewise linear curve. With multiple layers, the composition yields many more linear regions, enabling the network to approximate very complex functions.  

# 4. Training via Gradient Descent  
## **The Optimization Problem**  
Training a neural network means finding the parameter set $\mathbf{\theta}$ that minimizes the total loss:  

$$L(\mathbf{\theta})=\sum_{i=1}^n L(f(\mathbf{x}_i;\mathbf{\theta}),y_i).$$ 
## **Basic Gradient Descent Algorithm**  
- **Initialization:** Start with an initial guess $\mathbf{\theta}_0$. (How you choose this initial guess is an important topic in its own right.)  
- **Iterative Update:** At each iteration $i+1$:  
  - Compute the gradient of the loss with respect to $\mathbf{\theta}$:  
  $$\mathbf{g}=\left.\frac{\partial L}{\partial \mathbf{\theta}}\right|_{\mathbf{\theta}_i}.$$  
  - Update the parameters using:  
  $$\mathbf{\theta}_{i+1}=\mathbf{\theta}_i-\lambda\mathbf{g},$$  
  where $\lambda$ is the learning rate.  
- **Repeat:** Continue iterating until a stopping criterion is met (or manually interrupted).  

## **Example: 1D Linear Regression**  
For a simple linear regression model:  

$$L(\mathbf{\theta})=\sum_{i=1}^n (\theta_0+\theta_1 x_i-y_i)^2,$$  
the gradients with respect to $\theta_0$ and $\theta_1$ can be derived by differentiating the loss function. The update rule remains the same: subtract a multiple of the gradient from the current parameter estimate.  

# 5. Computing Gradients: Chain Rule and Reverse Mode Differentiation  
## **The Challenge of Complex Models**  
For deep networks, $f(\mathbf{x};\mathbf{\theta})$ is a composite function made of several layers. The loss $L(\mathbf{\theta})$ is thus a function of the output of these layers. Computing the gradient with respect to $\mathbf{\theta}$ involves the chain rule.  

## **Chain Rule Breakdown**  
For a composite function like:  

$$f(\mathbf{x};\mathbf{\theta})=\phi'(\mathbf{A}_4\mathbf{h}_3+\mathbf{b}_4), \quad \mathbf{h}_3=\phi(\mathbf{A}_3\mathbf{h}_2+\mathbf{b}_3), \quad \text{etc.},$$  
the chain rule tells us that:  

$$\frac{\partial L}{\partial \mathbf{\theta}}=\frac{\partial L}{\partial \mathbf{y}}\cdot\frac{\partial \mathbf{y}}{\partial \mathbf{h}_3}\cdot\frac{\partial \mathbf{h}_3}{\partial \mathbf{\theta}}.$$  
Each of these factors may involve further chain rule applications.  

**Reverse-Mode Differentiation (Backpropagation)**  
- **Jacobians and Their Challenges:** A function $f:\mathbb{R}^{M\times N\times P}\to\mathbb{R}^{R\times S\times T}$ has a Jacobian with a very large number of entries. In deep learning, computing or storing the full Jacobian is impractical.  
- **Vector-Jacobian Product (VJP):** Instead of computing the full Jacobian, we compute products of the form:  
  $$f'(\mathbf{x})^\top \mathbf{v},$$  
  where $\mathbf{v}$ is a vector of sensitivities. This "reverse derivative" approach allows the efficient propagation of gradients from the output back to the input.  
- **Efficiency:** Since our loss function is scalar, the chain rule in reverse mode lets us multiply on the left by vectors whose first dimension is 1. This avoids forming huge intermediate matrices and is the foundation for backpropagation in frameworks like PyTorch or JAX.  

**Intuitive Explanation**  
Think of the network as a series of "transformers" that pass information forward. During training, we need to know how a small change in any parameter affects the final loss. Rather than computing an enormous sensitivity matrix, we work backward—from the loss all the way to the earliest parameters—using the chain rule to efficiently "push" gradients back through each layer.  

# 6. Advanced Training Strategies: Stochastic Optimization and Beyond  
## **Stochastic Gradient Descent (SGD)**  
- **Mini-Batching:** Instead of computing the gradient over the entire dataset, one computes it over a subset (mini-batch). One pass through the entire dataset is called an epoch.  
- **Benefits of SGD:**  
  - **Escaping Local Minima:** The inherent noise in using mini-batches can help the optimization process escape shallow local minima or saddle points.  
  - **Computational Efficiency:** Mini-batch updates reduce computational cost per update.  
  - **Better Generalization:** Empirically, the noise often helps find better solutions in practice.  
- **Learning Rate Schedule:** Often, the learning rate is decreased over time to allow the optimizer to "settle" into a minimum.  

## **Momentum**  
- **Idea:** Instead of relying solely on the current gradient, momentum incorporates information from previous gradients:  
  $$\mathbf{v}_{i+1}=\beta\mathbf{v}_i+(1-\beta)\mathbf{g}_i, \quad \mathbf{\theta}_{i+1}=\mathbf{\theta}_i-\lambda\mathbf{v}_{i+1}.$$

- **Intuition:** Think of momentum as a "prediction" of where the parameters are headed. It smooths the updates and can help accelerate convergence, especially in directions of persistent descent.  
- **Nesterov Accelerated Momentum:** A variant where the update "looks ahead" by moving in the direction of the momentum first, then evaluating the gradient.  

## **Adaptive Methods: Adam**  
- **Adaptive Moment Estimation (Adam)** combines ideas from momentum and adaptive learning rates:  
  - **Normalized Gradients:** Adam maintains a moving average of both the gradients and the squared gradients.  
  - **Parameter Update:** The parameters are updated by normalizing the current gradient by an estimate of its variance. This makes the step size adaptive to the scale of the gradients.  
- The overall effect is a robust method that often converges faster and with less parameter tuning than plain SGD.  

# 7. Practical Considerations and Further Topics  
## **Fitting Complex Models**  
The notes briefly mention fitting models such as the **Gabor model**, which serves as a practical example where gradient descent must navigate a landscape that may have multiple minima or saddle points. The underlying principles remain the same, though the complexity of the loss surface increases.  

**Remaining Topics (To Be Covered Later)**  
- **Regularization:** Techniques like dropout help prevent overfitting.  
- **Normalization:** Addressing issues such as exploding and vanishing gradients.  
- **Parameter Initialization:** How to choose initial values for $\mathbf{\theta}$ to ensure effective training.  
- **Alternative Architectures:** Exploring different network structures for various tasks.  

# 8. Summary and Key Takeaways  
- **Deep Learning Fundamentals:** At its core, deep learning uses a parameterized function $f(\mathbf{x};\mathbf{\theta})$ to map inputs to outputs, trained by minimizing a loss function over a dataset.  
- **Loss Functions:** Whether using least squares for regression or cross-entropy for classification, the loss quantifies how far off predictions are from actual targets.  
- **Gradient Descent and Backpropagation:** Training involves iterative updates based on gradients. Reverse mode differentiation (the vector-Jacobian product) efficiently computes these gradients without forming large Jacobians.  
- **Optimization Enhancements:** Variants like SGD, momentum, and Adam improve convergence properties, help navigate complex loss landscapes, and adapt to different scales of the gradient.  
- **Future Directions:** Regularization, normalization, advanced initialization methods, and novel architectures are crucial topics for mastering deep learning.  

