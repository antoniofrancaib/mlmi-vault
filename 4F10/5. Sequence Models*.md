Lectures by Andrew Fitzgibbon

Topics: 

## 1. Introduction: Sequences in Deep Learning  
Sequences are everywhere.  
In many domains, the data naturally comes in the form of sequences. Examples include:  
- **Financial Data**: Time-series such as the FTSE 100 stock index.  
- **Biological Data**: RNA sequences.  
- **Audio**: Speech waveforms and their spectrogram representations.  
- **Text**: Sentences or paragraphs (e.g., a Turing quote).  
- **Video**: Sequences of video frames.  
- **Handwriting and Trajectories**: Handwritten strokes or map trajectories.  

At the heart of sequence modeling is the task of prediction—for example, predicting the next word in a sentence. This idea underpins language modeling and many other sequence generation tasks.  

## 2. Language Modeling and Autoregressive Data Generation  
### 2.1 Language Modeling Basics  
In language modeling, we treat each word in a sentence as a random variable. Consider a sentence:  

$$\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_i, \mathbf{w}_{i+1}$$  
- **Vocabulary ($V$)**: Each word $\mathbf{w}_i$ is drawn from a large but finite vocabulary.  
- **Observed vs. Random Variables**: In a training set, we observe $\hat{\mathbf{w}}_i$ (the actual words) but model the sequence with random variables $\mathbf{w}_i$.  

The goal is to compute the conditional probability:  

$$P(\mathbf{w}_{i+1} \mid \hat{\mathbf{w}}_1, \hat{\mathbf{w}}_2, \ldots, \hat{\mathbf{w}}_i)$$  
This probability is then used for predicting the next word, as seen in examples such as "The cat sat on the ___."  

### 2.2 Autoregressive Generation  
Autoregressive models generate data one step at a time:  

**Process Overview**:  
1. **Initialize**: Start with a beginning-of-sentence symbol, e.g., $\mathbf{w}_0 = \langle s \rangle$.  
2. **Predict Next Word**: Sample  
   $$\mathbf{w}_1 \sim P(\mathbf{w}_1 \mid \langle s \rangle)$$  
3. **Iterate**: Use the generated word as part of the history to predict $\mathbf{w}_2$, and so on:  
   $$\mathbf{w}_{i+1} \sim P(\mathbf{w}_{i+1} \mid \mathbf{w}_1, \ldots, \mathbf{w}_i)$$  
This recursive process is straightforward but hides a key challenge: the combinatorial explosion of possible histories.  

### 2.3 Model Complexity: Exponential Growth in Histories  
Consider a vocabulary of size  
$$|V| = 2^{16} (\approx 65,536)$$  
and a sentence length of $L = 8$. The number of possible histories (i.e., sequences of $L-1$ words) is  
$$(|V|)^{L-1} = 2^{16 \times 7} = 2^{112}.$$  
Modeling every possible history explicitly would require parameters on the order of  
$$(|V|)^L = 2^{128} (\approx 3.4 \times 10^{38}),$$  
which is clearly infeasible.  

**Key Intuition**:  
The challenge is to find a compact representation of history so that we do not have to model an exponential number of possibilities.  

## 3. Compact History Representations and N-gram Models  
### 3.1 Truncation: The N-gram Approach  
A straightforward method is to truncate the history to the most recent $N-1$ words. That is, approximate:  
$$P(\mathbf{w}_i \mid \mathbf{w}_1, \ldots, \mathbf{w}_{i-1}) \approx P(\mathbf{w}_i \mid \mathbf{w}_{i-N+1}, \ldots, \mathbf{w}_{i-1}).$$  
- **$N = 1$**: Unigram model (no history)  
- **$N = 2$**: Bigram model (last word only)  
- **$N = 3$**: Trigram model, etc.  

### 3.2 Discrete Models with Count-Based Estimation  
For n-gram models, one estimates probabilities from data by counting:  
$$P(\mathbf{w}_i \mid \mathbf{w}_{i-N+1}, \ldots, \mathbf{w}_{i-1}) \approx \frac{C(\mathbf{w}_{i-N+1}, \ldots, \mathbf{w}_i)}{C(\mathbf{w}_{i-N+1}, \ldots, \mathbf{w}_{i-1})},$$  
where $C(\cdot)$ denotes the count of occurrences in the training corpus.  
Smoothing techniques (e.g., Laplace smoothing) are often necessary to handle zero counts.  

## 4. Latent Variable Models and Hidden Markov Models (HMMs)  
### 4.1 The Role of Latent Variables  
Rather than directly modeling an exponentially large history, one can introduce a latent variable that summarizes the past. This latent representation may be:  
- **Discrete**: As in HMMs.  
- **Continuous**: Leading into models like the Kalman filter.  

### 4.2 Hidden Markov Models (HMMs)  
HMMs are generative models that introduce a sequence of hidden (latent) states $\mathbf{q}_t$ to capture the underlying structure:  

- **State Transition Assumption**:  
  $$P(\mathbf{q}_t \mid \mathbf{q}_0, \ldots, \mathbf{q}_{t-1}) = P(\mathbf{q}_t \mid \mathbf{q}_{t-1}).$$  
- **Observation Independence**:  
  $$P(\mathbf{x}_t \mid \mathbf{x}_1, \ldots, \mathbf{x}_{t-1}, \mathbf{q}_0, \ldots, \mathbf{q}_t) = P(\mathbf{x}_t \mid \mathbf{q}_t).$$  

The overall likelihood of a sequence $\mathbf{x}_1, \ldots, \mathbf{x}_T$ is given by summing over all possible state sequences:  
$$p(\mathbf{x}_1, \ldots, \mathbf{x}_T) = \sum_{\mathbf{q} \in Q^T} P(\mathbf{q}_0) \prod_{t=1}^T P(\mathbf{q}_t \mid \mathbf{q}_{t-1}) P(\mathbf{x}_t \mid \mathbf{q}_t),$$  
where $Q^T$ represents the set of all state sequences of length $T$.  

**Emitting vs. Non-emitting States**:  
- **Emitting States**: Directly produce observations.  
- **Non-emitting States**: Often serve as start and end markers, ensuring a well-defined sequence.  

HMMs are usually trained with the Expectation-Maximization (EM) algorithm.  

![[hmm.png]]
### 4.3 Discrete Kalman Filters  
For continuous latent variables, the model often assumes a linear dynamical system:  

- **State Transition**:  
  $$\mathbf{z}_t = A\mathbf{z}_{t-1} + \boldsymbol{\nu}_t, \quad \boldsymbol{\nu}_t \sim \mathcal{N}(0, \Sigma_\nu)$$  
- **Observation Model**:  
  $$\mathbf{x}_t = C\mathbf{z}_t + \boldsymbol{\epsilon}_t, \quad \boldsymbol{\epsilon}_t \sim \mathcal{N}(0, \Sigma_\epsilon)$$  
This model is analogous to factor analysis but extended dynamically to handle sequences.  
![[kalman-filter.png]]

## 5. Inference in HMMs: The Viterbi and Forward–Backward Algorithms  
### 5.1 Viterbi Algorithm  
The Viterbi algorithm is a dynamic programming method used to find the most likely state sequence (the "best path") given an observation sequence. It works as follows:  

- **Initialization**:  
  $$\phi_1(0) = 0, \quad \phi_j(0) = \text{LZERO} \quad (1 < j < N)$$  
- **Recursion**: For each time step $t = 1, \ldots, T$ and for each state $j$,  
  $$\phi_j(t) = \max_{1 \leq k < N} \{\phi_k(t-1) + \log(a_{kj})\} + \log(b_j(\mathbf{x}_t)).$$  
  Here, $a_{kj}$ is the transition probability from state $k$ to $j$, and $b_j(\mathbf{x}_t)$ is the probability of observation $\mathbf{x}_t$ given state $j$.  
- **Termination**:  
  $$\log(p(\mathbf{x}_{1:T}, \hat{\mathbf{q}})) = \max_{1 \leq k < N} \{\phi_k(T) + \log(a_{kN})\},$$  
  where the best state sequence $\hat{\mathbf{q}}$ is recovered by backtracking through the stored partial paths.  

An important computational tool in these log-domain computations is the LAdd function:  
$$\text{LAdd}(a, b) = \log(\exp(a) + \exp(b)),$$  
which ensures numerical stability when summing probabilities in the log domain.  

### 5.2 Forward–Backward Algorithm  
The forward–backward algorithm is used both for computing likelihoods and for training HMMs (e.g., via EM).  

- **Forward Probabilities ($\alpha$)**:  
  $$\alpha_j(t) = \log(p(\mathbf{x}_1, \ldots, \mathbf{x}_t, \mathbf{q}_t = s_j))$$  
  with the recursion:  
  $$\alpha_j(t) = \log\left(\sum_{k=1}^N \exp(\alpha_k(t-1) + \log(a_{kj}))\right) + \log(b_j(\mathbf{x}_t)).$$  
- **Backward Probabilities ($\beta$)**:  
  $$\beta_j(t) = \log(p(\mathbf{x}_{t+1}, \ldots, \mathbf{x}_T \mid \mathbf{q}_t = s_j))$$  
  with a similar recursion using the transition probabilities and emission probabilities for future observations.  

These two sets of probabilities allow computation of the posterior:  
$$P(\mathbf{q}_t = s_j \mid \mathbf{x}_{1:T}) = \frac{\exp(\alpha_j(t) + \beta_j(t))}{Z},$$  
where $Z$ is a normalization term ensuring the posterior sums to 1.  

## 6. From Generative to Discriminative Models: Conditional Random Fields (CRFs)  

![[discriminative-seq-models.png]]
### 6.1 Discriminative Sequence Models  
Unlike generative models (such as HMMs) that model the joint probability $P(\mathbf{y}, \mathbf{x})$, discriminative models directly model the conditional probability $P(\mathbf{y} \mid \mathbf{x})$. For instance, in multinomial logistic regression (MaxEnt), one writes:  
$$P(\omega_j \mid \mathbf{x}) = \frac{\exp(\mathbf{a}_j^T \mathbf{x} + b_j)}{\sum_{k=1}^K \exp(\mathbf{a}_k^T \mathbf{x} + b_k)}.$$  
For sequences, the class may itself be a sequence (e.g., a part-of-speech (PoS) tagging sequence).  

### 6.2 Maximum Entropy Markov Models and CRFs  
A Maximum Entropy Markov Model (MEMM) extends the idea of logistic regression to sequences. For a given time $t$, the state posterior is:  
$$P(\mathbf{q}_t \mid \mathbf{q}_{t-1}, \mathbf{x}_t) = \frac{1}{Z_t} \exp\left(\sum_{i=1}^D \lambda_i f_i(\mathbf{q}_t, \mathbf{q}_{t-1}, \mathbf{x}_t)\right),$$  
where $f_i$ are features and $\lambda_i$ are parameters.  

A Conditional Random Field (CRF) generalizes this by defining a globally normalized model over the entire sequence:  
$$P(\mathbf{q}_{0:T} \mid \mathbf{x}_{1:T}) = \frac{1}{Z} \exp\left(\sum_{t=1}^T \sum_{i=1}^D \lambda_i f_i(\mathbf{q}_t, \mathbf{q}_{t-1}, \mathbf{x}_t)\right).$$  
- **Global Normalization**: Unlike HMMs where each time slice is normalized locally, CRFs normalize over all state sequences simultaneously. This allows the model to capture longer-range dependencies and to consider the entire sequence when making decisions.  
- **Feature Engineering**: The features $f_i$ can be designed to capture transitions (similar to HMM transition probabilities) and associations between the observations and the states (similar to HMM emissions).  

### 6.3 Training CRFs  
CRFs are usually trained by maximizing the conditional likelihood:  
$$\hat{\boldsymbol{\lambda}} = \arg \max_{\boldsymbol{\lambda}} P(\mathbf{y}_{1:T} \mid \mathbf{x}_{1:T}; \boldsymbol{\lambda})$$  
with  
$$P(\mathbf{y}_{1:T} \mid \mathbf{x}_{1:T}; \boldsymbol{\lambda}) = \frac{1}{Z} \exp\left(\sum_{i=1}^D \lambda_i f_i(\mathbf{x}_{1:T}, \mathbf{y}_{1:T})\right).$$  
Since the sequences are fully observed during training, there is no need for iterative methods like EM; standard gradient-based optimization methods can be used. Efficient computation of the normalizing constant $Z$ is achieved by a forward–backward type algorithm adapted to the CRF structure.  


## 7. Summary and Key Takeaways  
- **Modeling Sequences**:  Whether dealing with text, speech, or time-series data, the challenge is to capture dependencies over variable-length histories.  
- **Autoregressive Generation**:  Models generate data step-by-step, but the explosion of possible histories necessitates compact representations.  
- **N-gram Models**:  Provide a simple truncation method but quickly become limited as $N$ increases.  
- **Latent Variable Models & HMMs**:  Introduce hidden states to capture long-range dependencies in a more compact form. Inference is performed using dynamic programming (Viterbi, forward–backward).  
- **Discriminative Models (CRFs)**:  Directly model the posterior $P(\mathbf{y} \mid \mathbf{x})$, allowing for richer feature representations and global normalization across the sequence.  

